= Machine Learning Notes

:toc:
:stem:

== Linear regression

From https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf

asciimath:[y=X beta+epsilon]

=== Least squares solution

asciimath:[epsilon'epsilon=y'y-2beta'X'y+beta'X'X beta]

asciimath:[(del epsilon'epsilon)/(del beta)=-2X'(y-X hat beta)=0]

asciimath:[X'X hat beta=X' y]

asciimath:[hat beta=(X'X)^-1 X'y]

=== Properties of the least squares solution

Properties which do not depend on any assumption, but just follow from the LS solution.

* Observed values X uncorrelated with residuals
asciimath:[X'epsilon=0]
* If X includes a bias term 1, then also:
* Mean und sum of residuals is zero asciimath:[sum epsilon_i=0]
* Regression hyperplane through means asciimath:[bar y=bar x hat{beta}]
* Predicted values of y uncorrelated with residuals asciimath:[hat y epsilon=0]
* Mean of predicted y same as mean of observed asciimath:[hat bar y=bar y]

All of this is just because the coefficients were chosen to minimize the squared error.

=== Gauss-Markov theorem

==== Assumptions
* there is a linear relation asciimath:[y=X beta+epsilon]
* X has no perfect multicollinearity (full rank)
* asciimath:[E\[epsilon|X\]=0]. Implies that we get mean right asciimath:[E\[y\]=X beta]
* asciimath:[E\[epsilon epsilon'|X\]=sigma^2 I] homoskedasticity. Variance independent of X and no autocorrelation
* X generated by a mechanism unrelated to asciimath:[epsilon]

==== Theorem

The OLS is the Best Linear, Unbiased and Efficient estimator (BLUE). No other linear and unbiased estimator of asciimath:[beta] has smaller variance.

=== Covariance Matrix of beta

asciimath:[E\[(hat beta-beta)(hat beta-beta)'\]=sigma^2(X'X)^-1]

asciimath:[sf"cov"(beta_i,beta_j)]

Estimate noise from

asciimath:[hat sigma^2=(epsilon'epsilon)/(n-k)]

=== Hypothesis testing

For hypothesis testing it is also often additionally assumed that asciimath:[epsilon|X ~ N(0,sigma^2 I)]

By assuming that we have a multi-variate normal, we can conclude that

asciimath:[hat beta ~ N(beta, sigma^2(X'X)^-1)]

=== Heteroskedasticity

Without heteroskedasticity you can estimate parameter means, but not standard errors.

To compensate, you could

* use weighted least squares (if know something proportional to standard errors)
* or use robust standard errors (White 1980)

== Covariance

asciimath:[sf"cov"(X,Y)=E\[(X-E\[X\])(Y-E\[Y\])\]]

asciimath:[=E\[XY\]-E\[X\]E\[Y\]]

asciimath:[=E\[bb"X"bb"Y"^T\]-E\[bb"X"\]E\[bb"Y"\]^T]

but last equation is not numerically stable.

asciimath:[sf"cov"(X,a)=0]

asciimath:[sf"cov"(aX)=a cdot sf"cov"(X)]

asciimath:[sigma^2(sum a_i X_i)=sum_{i,j} sf"cov"(X_i, X_j)]

asciimath:[Sigma(AX)=A Sigma(X) A^T]

asciimath:[Sigma=E\[XX^T\]-mu mu^T]

Must be positive-semidefinite matrix and any psm can be a covariance matrix.

=== Independence

Independent variables have zero covariance. But zero covariance does not imply independence (e.g. for an X where asciimath:[E\[X\]=0] and asciimath:[E\[X^3\]=0] you have asciimath:[ss"cov"(X,X^2)=0])

=== Covariance matrix
For a vector X
asciimath:[Sigma(X)=sf"cov"(X,X)]

== Spatial data analysis

* Thiessen polygon: polygon where a point is the nearest

=== Aggregation to metrics

* 1-step would be find the ratio between supply and demand in a catchment region of radius (or travel time) around each point
* 2-Step-Floating-Catchment-Analysis:
** Find measure (e.g. ratio supply to sum demand) in radius around each supply point
** Sum measures (of supply point) in radius around each demand point to get final metric
* gravity model with inverse power weights, usually causes more trouble than this simple method

=== Spatial autocorrelation

* standard: Moran's I (basically spatial autocorrelation with weights)
* known form of expectation and variance can be used to set up a z-score for hypothesis testing

=== Interpolation / Kriging

* Kriging method is Best Linear Unbiased Estimator (BLUE) and recommended (but need correct variogram, other non-linear or bias methods might be better)
* other methods not as good: Trend Surface Analysis (just fit a [polynomial] function); Inverse Distance Weighting (inverse distance power)
* Interpolation is weighted mean of surrounding points; weights have to be determined
* first step is to calculate a variogram (relation between variance and distance): mean of asciimath:[(Z_i-Z_j)^2] within given radius (?)
* need to fit one of a certain class of functional forms to the variogram (this choice requires expertise); spherical, gaussian, linear, exponential
* version:
** ordinary kriging: mean is constant (this is the same as Gaussian Processes[?])
** universal kriging: mean is position dependent (usually polynomial trend; then identical to GLS polynomial curve fitting)
** co-kriging: dependence on additional features
** block kriging: made to blocks of areas (instead of points)
* can be used to estimate error of estimations
* "nugget": y-intercept of variogram
* honors observed values (there are matched exactly)

=== Hotspot analysis

* can be polygon or point based
* most popular method: Getis-Ord Gl* (simple weighted sum?)

=== Location coding

* GeoHash: uses z-scores (interleaved bits), Base32 encoded
+ C-Squares

=== Map matching

* matching objects to objects on match; e.g. coordinates of GPS to road (i.e. line) where you are on

== Confidence Intervals

* quite some explanation in "The fallacy of placing confidence in confidence intervals" (Morey et. al.)
* Definition: An X% confidence interval for a parameter theta is an interval (L, U) generated by a procedure (!) that in repeated sampling has an X% probability of containing the true value of theta, for all possible values of theta
* confidence procedure is a random process; confidence interval if observed and fixed
* frequentist CI theory says nothing about the probability of the value being in the interval
* frequentist evaluation: based on "power" of procedures, which is a frequency with which false values of a parameter are excluded
* confidence procedures closely related to hypothesis testing (control rate of including true value; more power if exluding false values)
* intervals based on Uniformly Most-Powerful test are optimal for the goal of CIs
* many different CI procedures
* UMP may still lose information (i.e. beyond 1D summary)
* (!) UMP based CI better than Bayesian at excluding false values
* only Bayesian credible intervals actually contain the true value X%
* frequentist pre-data; bayesian post-data
* when estimating mean of Gaussian, frequentist and bayesian coincide
* (!) always include procedure and statistic used when reporting CI
* CI width means nothing
* for normal data, for each CI procedure there is an equivalent Bayesian with a certain prior (Jeffrey, Lindley)
* CI have difference shape in result parameter space (even 100% CI may be nested in some 50% CI)
* checking whether a parameter is included in credible interval is wrong

== Imbalanced data

=== Oversampling

* Naive
* SMOTE (generate new)
** no relation to kNN results, no specific to whether in/out-lier
** 3 variants in imblearn: generate new at border (kind=borderline1, borderline2, svm)
* AdaSYN (generate new):
** generate new next to original which are "wrong" by kNN
** focusses on outliers only
* SMOTE and AdaSYN use same algorithm to generate http://contrib.scikit-learn.org/imbalanced-learn/stable/over_sampling.html#mathematical-formulation[Ref]
** generate new X on line to one of nearest neighbours
* effect: http://contrib.scikit-learn.org/imbalanced-learn/stable/over_sampling.html[imb_learn]
** SMOTE generates peculiar streaks  between existing samples
** AdaSYN seems to fill within "simplex"
* can do multiclass; one-vs-rest if need neighbourhood

=== Undersampling

http://contrib.scikit-learn.org/imbalanced-learn/stable/under_sampling.html#mathematical-formulation[Imblearn Explanation]

* controlled: specifying desired sample numbers
** RandomUnderSampler
** NearMiss: adds heuristics to select sample; 3 different types with parameter `version` (http://contrib.scikit-learn.org/imbalanced-learn/stable/auto_examples/under-sampling/plot_nearmiss.html#sphx-glr-auto-examples-under-sampling-plot-nearmiss-py [Ref])
* cleaning: automatic determination of samples to clean
** Tomek: two different class samples nearest neighbors from each other -> remove one (?)
** OneSidedSelection: use TomekLinks to remove noise (?)
** EditedNearestNeighbours: Remove what does not agree enough with neighbours
** RepeatEditedNearestNeighbours: apply ENN multiple times
** AllKNN: like R-ENN, but increase number or neighbours
** CondensedNearestNeighbour: Iterative consider 1 nearest neighbour, sensitive to noise
** NeighbourhoodCleaningRule: Clean data before condensing
** !InstanceHardnessThreshold(estimator): Remove samples which are misclassified

=== Over- with Under-sampling

* SMOTE
* SMOTEENN
* SMOTETomek

=== Ensemble

http://contrib.scikit-learn.org/imbalanced-learn/stable/ensemble.html[Imblearn Ensemble]

* EasyEnsemble(n_subsets): ensemble of randomly undersampled
* BalanceCascade(estimator)
* BalancedBaggingClassifier: to allow balance of subset (unlike plain sklearn)

== Clustering

* Cluster measure:
* modularity: number of edges in cluster / (number of edge you'd expected from a random graph with same total number of edges)

=== Comparing clustering algorithms

https://github.com/lmcinnes/hdbscan/blob/master/notebooks/Comparing%20Clustering%20Algorithms.ipynb[Rules]:

* Be conservative; Show rather no clustering than wrong clustering
* Better if intuitive parameters
* Stable to seed/sampling
* Fast enough on large data

Algorithms:

* k-Means: too eager, need number of cluster, fast
* Affinity Propagation: too eager, globular, stable, slow
* Mean Shift: conservative ok, globular, intuitive parameters, not quite stable, slow
* Spectral clustering: too eager, not globular, need to know number of clusters, not very stable, slowish
* Agglomerative clustering: too eager, not globular, need number of clusters, stable, fast
* DBSCAN: first which is reasonably conservative, parameter not too intuitive, stable, fast
* HDBSCAN: even better than DBSCAN - varying density compensated, `min_samples` not intuitive but not too sensitive, stable, can be fast

-> HDBSCAN recommended

=== Performance

* http://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation[Sklearn Userguide Clustering performance]
* http://scikit-learn.org/stable/modules/classes.html#clustering-metrics[Sklearn Clustering metrics]

=== Order clustering results

For example use http://scikit-learn.org/stable/auto_examples/bicluster/plot_spectral_biclustering.html[Spectral Biclustering] to get checkerboard structure (not diagonal yet).

More functions in https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html[Scipy Hierarchical Clustering].

== Multiclass

* http://scikit-learn.org/stable/modules/multiclass.html#multiclass[Sklearn Userguide Multiclass]

== Modelling tips

* scale any distance dependent algorithm
* also linear models with regularization
* SVM probabilities from CV?
* GaussianNB like LogReg+L2?

== Model comparison

* LogReg more robust to outliers than LDA

== Cross decomposition

* find best latent variable linear relation between two matrices X and Y
* http://scikit-learn.org/stable/modules/cross_decomposition.html[sklearn cross decomposition]: PLSRegression, PLSCanonical, CCA, PLSSVD
* PLS esp. when more variables than observations

== Factorization machines, Polynomial networks

* https://github.com/scikit-learn-contrib/polylearn[Polylearn]
* Capture feature interaction through polynomial terms
* Low rank
* uni

== Fitting binary observations

A conversion rate may depend on a variable (e.g. price). Instead of fitting on aggregated values (e.g. average conversion rate per price bin), one can also fit on 0/1 values per each un-/successful application.
A linear fit will reproduce the correct coefficients/slope if P(1|x) was linear. Careful: A logistic regression (even though range 0...1 seems nice) will give incorrect results (predict_proba curve) if the ground truth is linear.
The stddev given by statsmodels.OLS corresponds roughly to what the deviation from the real (toy data) slope would be.

== Structured prediction
* predict vector of output variables y=(y1,..,yn)
* e.g. multilabel classification, sequence tagging, image segmentation (want locality)
* labels correlated
* pairwise structured models:
** argmax_alllabels weight*scoring(x,y) -> need to approx alllabels
** make assumptions about correlation structure; e.g. say where you want to model correlations (e.g. neighbor pixels)
** Estimator =
    Learner (learns weights w)
    + Model (how does correlation graph look like)
    + Inference (computes argmax; hard)
* model=ChainCRF(inference="max_product")
  ssvm=OneSlackSSVM(..)
  ssvm.fit(X, y)
* have transition parameters
* for chain model inference easy by dynamic programming in linear time
* for loops hard; can do in pystruct
* classes:
** exact algorithms: "max-product" for chain or tree; {"ad3":{"branch_and_bound":True}} (slow, but always work)
** relaxed: "lp" linear programming (slow); "ad3" dual decomposition
** approximate/heuristics (fast): "max-product" loopy message passing; "qpbo"
* OpenGM good C++ lib for many inference algorithms for PyStruct

== Regression metrics

=== AIC

* https://en.wikipedia.org/wiki/Akaike_information_criterion[Wikipedia AIC]
* Model selection for one data set (relative model quality) -> smaller is better
* AIC = 2k-2ln(L); k:number estimated param, L:max likelihood
* estimates (differences) in information lost from some model to the real process
* exp((AIC_min - AIC_k)/2)~P(Model k closest to reality)
* -> only differences matter: dAIC=6 means other model is ~5% likely better (but in test an irrelevant column caused only dAIC=2)
* -> omit all, but the best models and look at these probabilities now -> weighted mean of those (or say it's inconclusive, or get more data)
* works for non-nested models (unlike likelihood test)
* may need correction when few data points (otherwise it selects too many parameters [overfit])
* -> use AICc (which has special equations depending on model, k and sample size; usually included k^2 term)
* if same k and AICc equations -> can use AIC just as well
* for linear the parameter count is number of coef (incl. bias) plus 1 for the variance of the gaussian errors
* need to use same distribution of target -> otherwise https://en.wikipedia.org/wiki/Akaike_information_criterion#Transforming_data[transform data] (multiply by derivative)
* some software may drop constant terms from the likelihood
* AIC ~ LOO CVs
* for LMSE: AIC=2k+n*ln(RSS)+const -> for same k: AIC same as RSS (residual sum of squares)
* same as Mallow Cp for Gaussian linear regression

=== BIC

* BIC=-2 ln(likelihood)+ #freeparam * ln(#datapoints)
* asymptotic result if data distr is from exponential family
* model selection -> smaller is better
* BIC=ln(n)*k-2*ln(L)
* derived assuming data is for exponential family
* approx minimum description length
* can be used to choose clusters
* need n>>k
* not good for variable selection in high-dim
* difference in BIC of 5 would be good (2 is negligible)

=== AIC vs BIC vs ...

* https://en.wikipedia.org/wiki/Akaike_information_criterion#Comparison_with_BIC[Wikipedia AIC vs BIC]
* BIC assumes constant prior prob over all models -> not sensible? (models should be unequal)
* if true model in in candidates, BIC will (asymptically) always select it [but true model is never in candidates?] -> AIC won't for sure (even for infinite data)
* AIC might yet select even better model, which is not the real (??)
* however, BIC has a higher probably of selecting a very bad model
* AIC could select even better model than true model(?)
* AIC selects model closest to true model by information loss
* AIC optimal for LMSE when true model is not in candidate set
* BIC penalized free parameters stronger than AIC
* For F-test and likelihood test, models need to be nested
* Adjusted R^2 only for nested models (?)


==== Simulated data test

* distinguish at 50/50 model linear/cubic (small high order terms) -> guess correct model from noisy data points
* AIC best performance (~85% precision on both classes)
* LOO-CV (Predicted R^2, PRESS) asymptotically like AIC, but can be worse and is much slower
* CV seems worse than LOO-CV (?)
* BIC prefered simple model too often

=== Other

* https://en.wikipedia.org/wiki/Deviance_information_criterion[Deviance Information Criterion]: Generalization for hierarchical modelling; e.g. MCMC

== Logistic regression

P(y=1|x)=1/(1+exp(-(b_1*x+b_0)))

== PCA
* first component explains most variance; others orthogonal and also explain most of remaining variance
* alternatively minimize variance from line (line through multidimensional mean); others minizmize variance after correlation with previous subtracted
* sensitive to variable scaling
* need mean centering first (for above interpretations of max. variance); otherwise first EV will look like mean
* eigenvalues and -vectors of XT*X

== MARS
* Multivariate adaptive regression splines
* Opensource "Earth"
* c_0+sum c_i*B(x)
* B=max(0,x-const) or B=max(0,const-x)
* B=product of hinges
* can be combined with link function for GLM

== LDA
* GDA = LDA + QDA(unequal cov.)
* find linear combination of features which characterize or separate classes; assumes normal distribution
* related to ANOVA (ANOVA: cat. indep + cont dep.; DA: cont. indep. + cat. dep.); LR and Probit Regr. more similar to DA (use these if no Gaussian)
* vs PCA: PCA does not model differences
* vs Factor analysis: factor analysis treats indep./dep. equal
* for cat. indep. -> Discriminant Correspondence Analysis

== Factor analysis
* explain variables by fewer underlying unobserved factors
* variables are linear combination of factors plus error term (equiv. to low-rank approximation)
* related to PCA but not equal

== Compressed sensing
* for reconstructing a signal
* theorem 2004 and fewer samples than sampling theorem needed
* solve underdetermined linear system
* optimization; constraint: sparsity/incoherence
* use L1 or L0 norm for sparsity
* for many problem L1 equiv to L0 (Candes)
* L1 linear program; but with noise basis pursuit denoising preferred since preserves sparsity despite noise
* multiply both sides by wide matrix Q; know alpha from (QD)a=(Qx); knowing QD and Qx only

== HMM
* Markov process with unobserved hidden states
* transition probabilities in hidden states; output/emission probabilities from hidden states
* "simplest dynamic Bayesian network"
* for temporal pattern recognition, speech, handwriting, ...
* generalization to pair/triple models for more complex data structures and nonstationary data
* forward algorithm; forward-backward algorithm; Viterbi algorithm

== word2vec
* 2013
* words to high dim space
* similar words near
* similar relations are parallel lines
* meaning from words around them
* shallow ANN
* multiple algorithms (CBOW, Skip-gram, ..)
* scales well

== Sparse modeling
* L0 norm on alpha NP-hard:
* L1 norm as relaxation -> basis pursuit
* greedy methods -> matching pursuit, orthogonal MP

== Convolutional neural network:
* https://www.youtube.com/watch?v=n6hpQwq7Inw
* edges most valueable; normalize image (increase training speed); contrast normalization (like edge detector)
* 32x32 image (need to rescale and shift)
* convolution: expresses amount of overlap; use Gabor filters (detects orientation; vertical, horizontal, +45, -45)
* 1. filters (not quite Gabor filters) which are effectively similar to edge detection
* 2. use Tanh, Abs
* 3. subsampling, Tanh layer
* 4. convolution map
* 5. linear classification

== Plots

Box-Plot (by Tukey): plot Median, Q25, Q75, Min, Max; but consider values  x < Q25-1.5*(Q75-Q25) and x>Q75+1.5*(Q75-Q25) outliers

== K-Means:

* Disadvantages
** worst case is superpolynomial
** results can be arbitrarity bad wrt objective function of optimal clustering
** fro small data initial grouping can determine clusters
** need to set K
** don't know attribute weighting

== Single Linkage Clustering

* canbe done in N^2 with SLINK

== Support Vector Machines
* Kernels:
** linear faster than RBF
** RBF (tuned) always as good or better than linear kernel
** linear good enough if a lot of features
** linear of number of features larger than number of observations
** RBF when more observations than features
** if number of observations larger than 50000 use linear instead of RBF due to speed
* SVM large margin = small VC dimension

== Time series
* hard to beat SARIMAX (e.g. Python https://github.com/tgsmith61591/pyramid)

=== Learned from Kaggle
* https://www.youtube.com/watch?v=9Zag7uhjdYo
* example competitions:
** automated graders; just as good as human grader (disagreement between humans the same as to machine) for large essays (worse for short answers to questions)
** toxicity prediction: many features
** yandex: personalize results
* computer vision, NLP harder; good results slower
* automated grading:
** lowercase, alphanum, porter, spelling correction
** 1-3 word grams
** many regex hand-engineered; predict regex probabilities
** Boruta, RF, GBM
** OR: alternatively character 4-6 grams, 200 LSI
* usually winning method already ensemble and best
* Merck: log transform, Multiclass of NN with dropout on all input; Gaussian process regression
* rapid iteration useful!
* thinking about problem deeply rarely useful
* RF feature importance (boruta in R)
* RF, GBM often useful
* NLP: porter stemmer (normalize words to stem)
* deep learning (caffe, theano, torch7)
* factorization machines!
* usually code not production quality
* usually no computationally efficient solution
* good to normalize single measure (many approaches)
* evaluate many approaches on one problem
* often finds data leakages (target variable info which won't be there in test set; e.g. future info, ids, ...)
* Merck: clusters in 2nd vs 3rd PCA but didnt help
http://blog.kaggle.com/2016/07/21/approaching-almost-any-machine-learning-problem-abhishek-thakur/
* for text do TFIDF and TruncatedSVD
* classification: RF, XGB, LR, NaiveBayes, SVM, kNN
* regression: RF, XGB, LR, RIdge, Lasso, SVR


=== Text mining
* need to find special sub-categories of data, before looking at top words (e.g. periodic, high/low, special attributes)
* -> do an (0,1,0,..) feature and count top words for each of the bins
* combine word into min(rank) list
* also show cumulative tail percent (to understand when to stop looking)
* show top secondary words for each primary word (don't show/count words which are already high primary words)
* this could be done with fully Spark in two groupbys?
* How many other words (Quartile of number)? How often alone?

=== Feature aggregation
* multiply by map (e.g. linear or hat function) and aggregate (e.g. sum/min/max) -> like convolution
* this way many features are done
* map may need to shift with time

== Feature selection

* greedy feature selection for AUC: https://github.com/abhishekkrthakur/greedyFeatureSelection
* for feature selection with RF keep n_estimators low and not much tuning (to avoid overfitting)
* feature selection possible with SelectKBest(chi2)
* wrapper method:
* filter method: look at some kind of correlation with target
* stability selection: which features most often selected by some base method which is run on different bootstrap input and different regularization parameter
** some implementation in https://github.com/skggm/skggm

== Causality

=== Summary

* to get general effect of treatment A on outcome Y
* need to control for all backdoor paths (arrow to treatment) -> find all confounders to control for (affect both treatment and outcome)
* backdoor path criterion: need to block all backdoor paths, not no descendants of treatments (i.e. effect of treatment)
* backdoor path means it is associated, but not as an effect from A
* do not open paths, but controlling on colliders
* once set of confounders is found, you can use
** matching (group instances which are similar in those confounders; exact, proximity with greedy/optimal)
** could do matching by value of propensity score (balancing score; can condition on it since asciimath:[P(X=x|pi(X)=p,A=1)=P(X=x|pi(X)=p,A=0)])
** or inverse probability of treatment weighting (use weighting which is reciprocal of probability determined from a model (receives treatment)~(confounders))
* after blocking backdoor paths by control on X you get ignorability: asciimath:[Y^0,Y^1 perp A|X]
* Causal mediation analysis: when want to quantify which share is on which front-door path

=== Orthogonalized ML approach

* https://youtu.be/eHOjmyoPCFU?t=5m30s
* "Double machine learning for treatment and
causal parameters" https://www.ifs.org.uk/uploads/cemmap/wps/cwp491616.pdf[Chernozhukov 2016]
* "double" ML, "orthogonalized" ML

asciimath:[Y=D theta_0+g_0(Z)+U]
asciimath:[E\[U|Z,D\]=0]

* D is treatment, asciimath:[theta_0] is treatment effect, Z is other features, asciimath:[g_0] is an arbitrary ML method
* Z may be confounders with asciimath:[D=m_0(Z)+V, E\[V|Z\]=0] where asciimath:[m_0 neq 0]
* Naive solution approach by iteratively fitting asciimath:[Z ~ g_0(Y-D hat theta_0)] and asciimath:[D ~ Y-hat g_
0(Z)] is *biased* and incorrect
* Correct approach:
** fit asciimath:[Y ~ m_1(Z)] and find residuals asciimath:[tilde Y=Y-hat m_1(Z)]
** fit asciimath:[D ~ m_2(Z)] and find residuals asciimath:[tilde D=D-hat m_2(Z)]
** fit asciimath:[tilde Y ~ theta tilde D] to get inbiased estimate of asciimath:[hat theta]
* Frisch-Waugh-Lovell 1930 style?
* unbiased estimate is asciimath:[sqrt n] consistent and roughly Gaussian around true value

=== Disjunctive cause criterion to control for

* control for (condition on) all causes of treatment or outcome (arrow inward into treatment or outcome)
* not always the smallest set
* works (if such a set exists in observed variables and all causes correctly identified)

=== Time-based
* paths not correlated but co-integrated: when any linear combination stationary (common stochastic drift -> Granger test)

X(t+dt)=Hx*X+Hy*Y+Rxy
compare error functionals
Ex=int dt [Rx,Ry]
Exy=int dt [Rxy, Rxy]
-> co-integrated uf Exy much smaller than Ex
"future of X can be better predicted with known X and Y than just Y alone"

-> form u(t)=X-beta*Y; apply Augmented Dickey Fuller test of residuals to see if stationary (test if rho<<1 in u(t+1)=rho*u(t)+eps -> then residuals not stationary and hence co-integrated)

P(X,Z|Y)=P(Z|Y)P(X|Y) (but conditional indep. neither necessary nor sufficient for normal independence)

P(X1..Xn)=prod P(Xi|par(Xi))

== Low rank matrix factorization
* https://www.youtube.com/watch?v=kfEWZA-b-YQ
* kMeans also just matrix approx if regarded as perfect solution (each point to only one cluster; min quadric)
* V \approx WH
* solve linear equations, transform data, compress data
* methods: kMean, SVD, NMF, Archetypal Analysis, Binary matrix factorization, CUR decomposition, ...
* Python Toolbox: PyMF (from Fraunhofer); support HDF, sparse matrices
* can also keep W fixed and find H only
* PCA:
  * not non-negative (not perfect for data analysis)
  * WT*W=1
  * for compression or filter out noise
  * always perfectly optimal for given k; always be orthogonal
  * finds average faces
* mdl=PCA(data, num_bases=..)
  mdl.factorize()
  V_approx=np.dot(mdl.W, mdh.H)
* NMF:
  * W,H>=0
  * NP hard for global solution (but known to exist)
  * often W converges to partial representation (reconstruct data by summing)
  * -> not complete faces but rather parts which have to be summed (ear, ..)
* Archetypal analysis:
  * min_WH ||V-VWH||^2; W,H>=0 (convexity); sum columns W,H=1
  * probabilistic combination
  * archetypes are most extreme data points!
  * allow probabilistic interpretation of archetypes (due to constraints) -> good for data analysis (interpretable)
  * archetypes have highest pairwise distance between them
  * O(n^2) solver; but efficient for large data exist
* PCA: compressed; kMeans: groups; NMF: parts; AA: opposites
* simplex volume maximization fast


== Bayesian Approach
* main component is prior
* with prior you could calculate anything (even average hypothesis or error on this)
* for example:
** for perceptron use distribution over weights (e.g. uniform)
** but if uniform: you'd know average and stddev; however real unknown could be completely different!

When is Bayesian justified?
* when prior valid (since known for some reason)
* when prior is irrelevant (just a catalyst)

== Probability distributions

* sum of uniform distributions http://en.wikipedia.org/wiki/Irwin%E2%80%93Hall_distribution
* mean of uniform distributions http://en.wikipedia.org/wiki/Bates_distribution (convergence $1/\sqrt{n}$ by Kolmogorov-Smirnov distance)

=== Tracy-Widom
* normalized largest eigenvalue of random Hermitian matrix
* application: multivariate statistics, inferring population structure, longest increasing subsequence, longest common subsequence, ...
* analytics form complicating
  
=== Shifted Gompertz
* largest of two indep var (exponential[b] and Gumbel[eta,b])
* applications: modeling adoption/diffusion of innovation, growth of social networks (better than Bass or Weibull), ...
* CDF: (1-exp(-bx))exp(-eta*exp(-bx))
  
=== Rayleigh
* usually when overall magnitude of vector related to directional component
* PDF: x*exp(-x^2)
* CDF: 1-exp(-x^2)
* generalizations: Rice, Weibull
  
=== Log-logistic
* survival analysis where rate first increases and the decreases
* ln X ~ logistic
* heavier tails than log-normal
* non-monotonic hazard rate, unlike more commonly Weibull
* CDF closed form unlike log-normal
* application: stream flow rates, distribution of wealth
* PDF: x^b/(1+x^b)^2
* CDF: 1/(1+x^-b)
  
=== Levy
* stable, analytically expressible (like normal and Cauchy)
* application: frequency of geomagnetic reversals, length of path of photon, time of hitting a single point in Brownian motion, ...
* PDF: exp(-c/(x-m))/(x-m)^(3/2)
* CDF: erfc(sqrt(c/(x-m)))
  
=== Half-normal
* normal, but only positive values
* mean: sig*sqrt(2/pi)
* variance: sig^2*(1-2/pi)
  
=== Gompertz
* distribution of adult life-spans; survival
* application: failure rates of computer nodes, customer life-time, ..
* PDF: exp(bx)*exp(-eta*exp(bx))
* CDF: 1-exp(-eta*(exp(bx)-1))
* Gamma is conjugate prior
  
=== Gamma/Gompertz
* PDF: exp(bx)/(a+exp(bx))^s
* CDF: 1-a/(c+exp(bx))
  
=== Generalized Pareto
* model tails of other distribution; parameters localtion, scale, shape
* PDF: (1+xi*z)^-(1/xi+1)
* CDF: 1-(1+xi*z)^(-1/xi)
  
=== F-distribution
* analysis of variance null distribution
* PDF, CDF: need Beta and Incomplete Beta Function
  
=== Exponential
* time between events in poisson process (i.e. constant rate)
* memoryless: P same when time shifted
* PDF: exp(-ax)
* CDF: 1-exp(-ax)
* minimum of multiple expontial variables also exponential with a=sum a_i (but not for max!)
* estimation: a=1/<x> (MLE), (n-1)/n*1/<x> (unif.min.variance.unbias.est.)
  
=== Dagum
* income estimation
* CDF: (1+(x/b)^-a)^-p
  
=== Birnbaum-Saunders
* fatigue life distribution
* application: reliability, model failure times; failure due to cracks in repeated stress, ...
* transform Y=sqrt(X/b)-sqrt(b/X) to get normal
  
=== Kumaraswamy:
* like Beta but analytically simpler
* PDF: x^(a-1)*(1-x^a)^(b-1)
* CDF: 1-(1-x^a)^b
  
=== Irwin-Hall (uniform sum)
* sum of uniform variables
* (Bates: mean of uniform)
* PDF, CDF: sum similar to binomial expansion
* mean: n/2
* variance: n/12
  
=== Beta
* variables in finite interval
* applications: order statistics (k-th smallest of uniform is Beta), Bayesian priors, project management where events constraint to interval, ..
* PDF, CDF: needs Beta function
  
=== Wishart
* generalization of chi-square to multiple dimensions
  
=== Parabolic fractal
* discrete
* can fit better than power law; model King effect
* PMF: n^-b*exp(-c*(log n)^2)
  
=== Borel
* branching, queueing theory; number of off-spring Poisson -> will extinct -> number of descendants Borel
* application: distribution of typical busy period of queue, ...
* PMF: exp(-m*n)*(m*n)^(n-1)/n!
* generalization Borel-Tanner
  
=== chi
* square root of sum of squares of normal
* PDF: x^(k-1)*exp(x^2/2)
* Mode: sqrt(k-1)
* variance: sig^2=k-m^2
  
=== chi-squared
* sum of squares of normal
* PDF: x^(k/2-1)*exp(-x/2)
* mean: k
* mode: max(k-2,0)
* variance: 2k
* converges to normal (e.g. k>50); but slow
* sqrt(x), root[3](k) also normal (see transformations)
  
=== Exponential-logarithmis
* lifetime with descreasing failure rate (work hardening or immunity)
* PDF, CDF: analytic but quite nested
  
=== Fréchet (inverse Weibull)
* special case of extreme value distr
* application: extrems events such as annually max one-day rainfalls
* CDF: exp(-z^(-a))
  
=== Gamma
* applicatiions: inter-event intervals, amount aggregated insurance claims, ...
* PDF: x^(a-1)*exp(b*x)
* CDF: needs gamma func
  
=== Erlang
* sum of exponential variables
* special case of Gamma
* applications: number of connections at the same time in queue; waiting times between k events of Poisson process
* PDF: x^(k-1)*exp(-l*x)
* CDF: needs incomplete gamma function or sum of exp
  
=== Log-Cauchy
* applications: survival when some extreme outliers
* super-heavy log tail
* PDF: 1/x*sig/((ln(x)-m)^2+sig^2)
* CDF: 1/pi*arctan((ln(x)-m)/sig)+1/2
* only median exists: exp(m)
  
=== Lomax
* Pareto that shifts to begin at zero
* PDF: (1+x/b)^-(a+1)
* CDF: 1-(1+x/b)^-a
* mean: b/(a-1)
  
=== Pareto
* PDF: a*b^a/x^(a+1)
* CDF: 1-(b/x)^a
* mean: a*b/(a-1)
  
=== Weibull
* applications: failure proportional to power of time; survival analysis, delivery times, failure analysis, size of insurance claims, partical size
* PDF: x^(k-1)*exp(-(x/b)^k)
* CDF: 1-exp(-(x/b)^k)
* -> plot {ln(-ln(1-F))} = k*{ln(x)} - k*ln(b) for linear
* see Wikipedia for how to deal with empirical data
* b*(-ln(U))^(1/k) is Weibull if U~uniform[0,1]
  
=== Generalized extreme-value distribution
* Fisher-Tippett-Gnedenko theorem: for n iid vars; Mn=max(X1..Xn);i
* if lim(n->infty) (Mn-bn)/an exists, then it must be Gumbel, Frechet or Weibull
* CDF: exp(-t(x)); t(x)=(1-z*eta)^(-1/eta) [eta!=0]; t(x)=exp(-z) [eta=0]
* PDF: more nested
* location, scale, shape param; support half-infinite, direction depending on eta
* mean: m+sig*(Gamma(1-eta)-1)/eta
  
=== Generalized Pareto distribution
* tails for extreme value distribution
* PDF: (1+eta*z)^-(1/eta+1)
* CDF: 1-(1+eta*z)^(-1/eta)

=== Remaining
* looked through all continuous [0,infty] for interesting distributions

== Top 10 ML algorithms

* C4.5
* K-Mean
* SVM
* Apriori
* EM
* PageRank
* AdaBoost
* kNN
* Naive Bayes
* CART
* Kernel Density Estimation and Non-parametric Bayes Classifier
* K-Means
* Kernel Principal Components Analysis
* Linear Regression
* Neighbors (Nearest, Farthest, Range, k, Classification)
* Non-Negative Matrix Factorization
* Dimensionality Reduction
* Fast Singular Value Decomposition
* Decision Tree
* Bootstapped SVM
* SOM
* Ant Colony
* Genetic Algorithms
*Dirichlet process clustering
* Multiclass classification:
* Bonzaiboost (AdaBoost.MH on DT (200x 2level; 1000x 1 level))
* RotationForest
* LogReg+Sparse binary coding+NaiveBayes regularization

== Missing data

Possibilities:
* null values and meaning
* suspicous numbers (esp. 2^n numbers-1, 10^n, 10^n-1)
* not all data (categories are there)
* duplicate rows (or part of rows)
* spelling inconsistent (e.g. names)
* field part order inconsistent (e.g. first/last name)
* data format inconsistent (e.g. date format)
* number not uniform (poisson distance test?)
* monotone changes over time