== Tuning

== General

GBM Generlized Boosted Machines: loss function suitable for optimization
AdaBoost: GBT with exp loss
watchlist: show performance on these sets while training
native xgb and xgboost.sklearn wrapper

custom metric possible

train returns last model, not best!
bst.best_score, bst.best_iteration, bst.best_ntree_limit

xgb.cv(..) -> multiple CV, with stddev

missing data:
* xgb.DMatrix(.., missing=np.nan)
* in sklearn: np.nan


* CART: score at each leaf
* Winning competitions: Avito Context Ad Click, Crowdflower, WWW2015 Microsoft Malware
* use Taylor of loss with 1st and 2nd derivative
* add regularization = gamma * [number of leaves] + 1/2 * lambda * sum (leafweight_i)^2
* find decision tree along gradient
* combination of boosted trees with conditional random field 
* https://github.com/dmlc/xgboost/blob/master/demo/README.md

* test data set: https://archive.ics.uci.edu/ml/datasets/Covertype (7 classes)

parameters (https://xgboost.readthedocs.org/en/latest/parameter.html#parameters-for-tree-booster):
* sketch_eps: consider only the best splits
* shrinkage only effects score of leaf nodes, not tree shape

