== Tuning

== General

GBM Generlized Boosted Machines: loss function suitable for optimization
AdaBoost: GBT with exp loss
watchlist: show performance on these sets while training
native xgb and xgboost.sklearn wrapper

custom metric possible

train returns last model, not best!
bst.best_score, bst.best_iteration, bst.best_ntree_limit

xgb.cv(..) -> multiple CV, with stddev

missing data:
* xgb.DMatrix(.., missing=np.nan)
* in sklearn: np.nan

imbalanced:
* small min_child_weight
* weights to instances of DMatrix(..., weight=..)
* scale_pos_weight: param for weight ratio

Suggestions1:
around 100..500 trees; play with learning_rate: 0.01..0.3
max_depth: start with 6; or max_leaf_nodes instead
min_child_weight: start with 1/sqrt(eventrate)
colsample_bytree: 0.3-1.0
subsample: 1.0; 0.5-1.0
gamma: 0.0; min required loss reduction; regularization (use if train error >> test error; e.g. gamma 5); 20 would be very high (use if fast overfitting or dominating features); use gamma [complexity from loss] instead of min_child_weight [loss derivative]) when train CV skyrockets; https://medium.com/data-design/xgboost-hi-im-gamma-what-can-i-do-for-you-and-the-tuning-of-regularization-a42ea17e6ab6#.t243a3kp9
lambda: L2 reg on weight to prevent overfitting
alpha: L1 on weights; some feature selection (e.g. on high dim)
max_delta_step: usually not needed; can help with logreg and imbalanced
colsample_bylevel: usually subsample/colsample_bytree instead
scale_pos_weight: faster convergence if imbalance
too complex, overfitting (test error rises; large gap) -> learning_rate-, max_depth-, min_child_weight+
too simple (test and training error close and fall continuously) -> opposite
just right -> test error levels off

Example:
learning_rate = 0.01 (small shrinkage)
max_depth = 9 (default max_depth=6 which is not deep enough)
sub_sample = 0.9 (giving some randomness for preventing overfitting)
num_rounds = 3000 (because of slow shrinkage, many trees are needed)

Approach1:
* train with default
* if bad, eta=0.1 and tune n_trees
* tune rest (max_depth, gamma, subsample, colsample_bytree); for gbtree dont use gamma unless test error diff
* tune alpha, lambda if needed
* change eta and repeat
* CART: score at each leaf
* Winning competitions: Avito Context Ad Click, Crowdflower, WWW2015 Microsoft Malware
* use Taylor of loss with 1st and 2nd derivative
* add regularization = gamma * [number of leaves] + 1/2 * lambda * sum (leafweight_i)^2
* find decision tree along gradient
* combination of boosted trees with conditional random field
* https://github.com/dmlc/xgboost/blob/master/demo/README.md

* test data set: https://archive.ics.uci.edu/ml/datasets/Covertype (7 classes)

parameters (https://xgboost.readthedocs.org/en/latest/parameter.html#parameters-for-tree-booster):
* sketch_eps: consider only the best splits
* shrinkage only effects score of leaf nodes, not tree shape

special:
max_delta_step: for LogReg when unbalanced(?)
--------
https://medium.com/data-design/xgboost-hi-im-gamma-what-can-i-do-for-you-and-the-tuning-of-regularization-a42ea17e6ab6
gamma: depends on data and also other param!
regul -> high gamma (e.g. 10;20 already extremely high); changes -20% or +2
if trainCV>>testCV: gamma instead of min_child_weight (or max_depth)
