Cointegration
-------------
Two time series that have "dependence" as linear combination are. For tests see http://en.wikipedia.org/wiki/Cointegration (e.g. simple Engle-Granger test).

Independence
------------
* Subindependence (weaker the independence) if characteristic function factorizes

Central limit theorem
---------------------
* http://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem quantifies deviation from normal distribution


Pickands-Balkema-deHaan theorem:
* 2nd theorem of extreme values, which gives asymptotic tails (above threshold)
* -> usually generalized Pareto distribution:

Tests
-----
* Rayleigh:
  * test for periodicity of irregularly sampled data

Dvoretzky–Kiefer–Wolfowitz inequality
=====================================
* Probability that empirical distribution will deviate by eps
* P(diff eps in F)<=2*exp(-2*n*eps^2)

Variance of sum of binomials
============================
* Var(Z)=n*p*(1-p)-n*s^2
* p = avg p_i
* s^2 = Var(p_i)
* https://en.wikipedia.org/wiki/Binomial_sum_variance_inequality

Sum of Bernoulli is approx Poisson
==================================
* La Cam's theorem
* for multiple Bernoulli with prob p_i
* sum(P(k)-Pois(k))<2*sum p_i^2

Probability of overshooting of sum
==================================
* https://en.wikipedia.org/wiki/Lorden%27s_inequality

Relation between moments of collection of indep variables
=========================================================
* https://en.wikipedia.org/wiki/Marcinkiewicz–Zygmund_inequality

Mean delay in a queue
=====================
* https://en.wikipedia.org/wiki/Ross%27s_conjecture

Probability of observing atypcial sequence of samples
=====================================================
* https://en.wikipedia.org/wiki/Sanov%27s_theorem

Positive probability of being positive
======================================
* P(X>0)>=E(X)^2/E(X^2)

Bound on deviation from mean on unimodal
========================================
* https://en.wikipedia.org/wiki/Vysochanskij–Petunin_inequality
* P(|X-mu|>=lambda*o)<=4/(9*lambda^2)

Bound that sum deviates from mean
=================================
* https://en.wikipedia.org/wiki/Hoeffding%27s_inequality
* more general than Bernstein
* special cases of https://en.wikipedia.org/wiki/Doob_martingale#McDiarmid.27s_inequality

Upper bound for deviation of sum from expected
==============================================
* https://en.wikipedia.org/wiki/Bennett%27s_inequality
* max(X)=a (almost surely)
* P(S>t)<=exp(...)

Bound of deviation of sum from mean
===================================
* https://en.wikipedia.org/wiki/Bernstein_inequalities_(probability_theory)

Some strengthening of law of large numbers
==========================================
* https://en.wikipedia.org/wiki/Hsu–Robbins–Erdős_theorem

Prob of random walk in center
=============================
* https://en.wikipedia.org/wiki/Rademacher_distribution#Van_Zuijlen.27s_bound
* P(|sum X/sqrt(n)|<=1)>=0.5

Convergence to mean quantified
==============================
* https://en.wikipedia.org/wiki/Berry–Esseen_theorem

Exponential bound on tail distribution of sum
=============================================
* https://en.wikipedia.org/wiki/Chernoff_bound

Probability that partial sum exceeds a bound
============================================
* https://en.wikipedia.org/wiki/Kolmogorov%27s_inequality

Concentration inequalities for deviations
=========================================
* https://en.wikipedia.org/wiki/Concentration_inequality

Bound on probability that partial sum exceeds a bound
=====================================================
* https://en.wikipedia.org/wiki/Etemadi%27s_inequality

Correlation and independence
============================
* jointly normal and uncorrelated (cov=0) -> independent
* only marginally normal and uncorrelated -> not necessarily indep (https://en.wikipedia.org/wiki/Normally_distributed_and_uncorrelated_does_not_imply_independent)

Entropy power inequality
========================
* https://en.wikipedia.org/wiki/Entropy_power_inequality

Circular mean by complex numbers
================================
* https://en.wikipedia.org/wiki/Mean_of_circular_quantities
* max likehood
* minimizes distances 1-cos(a,b) [half the squared Euclidean distance of points]


== Poisson approximation

=== Le Cam's theorem

https://en.wikipedia.org/wiki/Le_Cam%27s_theorem

* sum of independent Bernoulli variables (not identical)
* asciimath:[lambda_n=p_1+...+p_n]

asciimath:[sum_(k=0)^oo abs(sf"Pr"(S_n=k)-(lambda_n^k e^(-lambda_n))/(k!))<2 sum_(i=1)^n p_i^2]

Maybe, improved version of Hollander with extra factor asciimath:[sf"min"(1,1/lambda_n)]?

== Exponential family

* https://www.quora.com/Why-are-exponential-families-so-awesome
* unify: normal, binomial, poisson, gamma, ...
* only for them: there is always a sufficient statistic of fixed dimensionality independent of data size
* minimal sufficient statistic is also complete sufficient statistic
* maximized entropy (under some conditions)
* conjugate distributions are also exponential family
* MLE estimates are simple: set the observed value of the natural sufficient statistic equal to its expected value

== Quadratic variance function

* only these distributions have a variance that is a quadratic function of the mean:
** normal
** poisson
** gamma
** binomial
** negative binomial
** hyperbolic secant function (https://www.tandfonline.com/doi/abs/10.1080/00031305.2013.867902[Ding'13])

++++++++++++
Durbin-Watson:

* Auto-correlation of residuals; 0..4
* d=2 none; d<1 considerable positive corr; 
* d<2 successive terms much different (-> possibly underestimation of statistical significance)
* complex derivations for thresholds (depending on significance and num. of samples)
* affects ability for statistical test:
  * positive corr -> F-stat may be inflated (since MSE underestimated); OLS std. errors of coef. underestimated -> incorrectly reject Null
  * for corr. -> use Cochrane-Orcutt procedure
  * D-W stat not applicable if lagged dependent variables -> rather use Durbin's h-test if large samples
  * D-W stat biased for autoregressive moving avg. models -> for large samples use normally distrib. h-statistic (which derives from d and estimated variance)

Jarque-Bera:

* test is skewness and kurtosis belong to normal
* JB=(n-k)/6(S^2+(K-3)^2/4)
* chi^2 with df=2 (for n>2000)
* tests S=0, K-3=0; for small n rejects Null too much
