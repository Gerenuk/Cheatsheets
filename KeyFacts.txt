Classification
--------------
* precision: TP/(TP+FP)
* recall/sensitivity: TP/(TP+FN)
* accuracy: (TP+TN)/(TP+FP+TN+FN)
* ROC curve: TPR [TP/(TP+FN)] - FPR [FP/(TN+FP)]
* multiclass from binary: error correcting codes, one-vs-all
* rare class -> cost sensitive learning

Generalized linear model
------------------------
* generalization of linear regression
* link function -> response variable other than normal distribution (e.g. only positive, or bounded [0;1] )
* E(Y)=g^-1(theta*X)  [from exponential family or even overdispersed exp. family]

SVM
---
* good:
  * good generalization, bounds on generalization error from theory (doesn't depend on dimension of space)
* bad:
  * unscaled features
  * imbalanced classes (rather DT)
  * multiple classes
  * huge feature number
  * batch learning
  * parallel training needed
  * parameters difficult to interpret
  * not for many classes or regression (even though possible; for regression at least Platt scaling)
  * memory intensive
* quadratic soft margin -> like linear with shifted kernel K <- K+lambda (?)
* kernel obeys Mercers theorem
* usually 25% support vector
* in practice always some kernel with SVM possible
* AvA best for multiclass?

Association analysis
--------------------
* support o(X+Y)/N
* confidence O(X+Y)/o(X)

Cluster analysis
----------------
* hierarchal (nested) / partitional (unnested)
* exclusive/overlapping/fuzzy
* complete/partial
* well-separated (objects closer to cluster than others)
* prototype-based
* density-based
* graph-based
* K-Means (cant do non-spherical or widely diff densities, outliers diff, need centroid calc), agglomerative (merging, dendrogram; no global function, time expensive), DBSCAN

AdaBoost
--------
* type of boosting
* adaptively change training set
* adaptive boosting
* combined with other methods (can be weak)
* sensitive to noise data and outliers

kNN
---
* good:
* bad:
  * sensitive to noise
* non-parametric classification
* can be used for regression too (average)
* number of neighbors and distance metric needed
* care of majority voting when classes skewed
* dimension reduction to avoid curse of dimensionality
* effective parameters: n/k
* improve: kernel, emphasize some vars, local regression, ...

Naive Bayes
-----------
* good:
  * good if independence holds
  * very simple (just counts)
  * high bias -> good for small training set
* P(X|Y=y)=prod P(Xi|Y=y)
* for continuous variable discretize

Matrix decomposition
--------------------
* eigendecomposition: A=ULU^(-1) (U is matrix of EV; A nondegenerate eigenvalues and lin.indep. eigenvectors; always possible if A and U square)
* SVD: A=UDV^H (A is any matrix; UU^H=VV^H=1 [unitary])

Linear regression
-----------------
* \hat{y}=X(XT*X)^-1*XT*y (hat matrix times y)
* shrinkage method:
  * ridge regression: L2, still closed form solution
  * lasso (basis pursuit): L1, quadratic programming, some coefficients exactly zero

Logistic regression
-------------------
* only Normal solvable in closed form; Newtons method otherwise
* no convergence for multicollinearity, sparseness, complete separation
* need about 10 events (hits) per feature
* to test: Deviance (compare likelihood to saturated model [too many parameters]), and more...
* low bias (may overfit -> sometime rather Naive Bayes)
* many ways to regularize
* online gradient descent possible (update; unlinke DT or SVM)
* multinomial logistic regression (for multiple classes)

Apriori algorithm
-----------------
* frequent item set mining (find sets which appear sufficiently often)
* sets successively built up

Decision tree
-------------
* good:
  * simple+white box (explanable)
  * no preparation
  * numerical/categorial
  * robust
  * large data
  * mixed data
  * missing values
  * robust to outliers
  * insensitive to monotone transform
  * comput scalable
  * deal with irrelevant inputs
* bad:
  * NP-complete
  * overcomplex trees (overfitting)
  * XOR/parity/multiplexer hard to learn
  * weaker predictive power than SVM
  * sometime hard to interpret
  * can overfit  
* aka classification trees, regression trees
* rectangular decision regions
* leaves are class labels, branches are deciding feature
* top down or bottom up; usually greedy
* ensemble methods: random forest, boosted tree, bagging
* algorithms: CART, C4.5, MARS
* impurity difference between parent and weighted mean of children
* impurity measure Gini: 1-sum p_i^2
* at the end tree pruning
* non-parametric
* algorithm ID3 (entropy based) -adv-> C4.5

Curse of dimensionality
-----------------------
* hypercube: in 10D to catch 1% or samples one needs 63% of feature range
* all samples are close to an edge
* samples sparse in space

EM algorithm
------------
* for maximum likelihood problems


Mistakes to avoid
-----------------
* lack of data (need about 5 data points per feature)
* only one technique
* ask wrong question
* leaks from future
* extrapolate
* answer every question
* believe in best model

Fraud detection
---------------
* anomaly detection
* interesting pattern mining
* clustering
* expert system encode fraud and search
* supervised neural networks, bayesian learning neural network
* change of behaviour

Regression
----------
* ANN
* CART (DT)
* MARS (Multivariate adaptive regression splines)
* kNN

Multiple classes
----------------
* One-vs-All
* All-vs-All (for all pairs)
* Max-Class

Covariance matrix
-----------------
* \Sigma_ij=E((X_i-mu_i)(X_j-mu_j))=E(X*X^T)-mu*mu^T

Dimensionality reduction
------------------------
* PCA (+ kernel trick)
* self-organizing maps

