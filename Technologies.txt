= Javascript transpilers
TypeScript won
Coffescript lose interest
Dart not real JavaScript
Elm good but not enough traction
Babel worse?

= Misc Deep Learning
* High volume: Caffe2, CNTK

= Tensorflow
* Tensorflow has optimisations for Intel Xeon
* Static
* Hard to implement something new if not existing (attention)

= Pytorch
* Dynamic
* Use python functions
* New fast.ai lib soon (switching from TF)

= Web
* jQuery obsolete

= XGBoost
* Throwing more cores is not the ideal for fast histogram xgboost, while exact xgboost likes getting more cores
* XGBoost is a really good stacker model (see Wolpert's stacked generalization, see Lecun's sequential modeling

= Arrow
* data format for (nested) tables
* common serialization format between technologies
* optimized for modern CPU usage (SIMD, cache local)
* mainly in-memory (disk would pay more attention to compression)

The trade-offs being for columnar data are different for in-memory. For data on disk, usually IO dominates latency, which can be addressed with aggressive compression, at the cost of CPU. In memory, access is much faster and we want to optimize for CPU throughput by paying attention to cache locality, pipelining, and SIMD instructions.

Interoperability between Parquet and Arrow has been a goal since day 1

= Apache Ignite
* memory centric data processing
* distributed execution and data
* hot data in memory or flash, cold data on disk
* SQL99 support, key-value, ACID, streaming, alpha ML, IgniteRDD for Spark integration
* Java, NET, C++ API
* Spark can use IgniteRDD
* Ignite vs Spark:
** data source agnostic, use many data
** full ACID
** ?

= Graph databases
* Neo4J, OrientDB, TitanDB:
** similar speed
** OrientDB needs much more memory

== Neo4J
* Cypher language
* for OLTP

== OrientDB
* Extended SQL language

== TitanDB
* Gremlin language (nice)
* for querying

= Accumulo
* on Hadoop; developed by NSA
* cell-level security

= Redis
* remote dictionary server
* key-value; types
* everything a string
* cluster
* autodiscovery
* no consistency guarantee
* very fast

= Cassandra
* by Amazon
* compression
* no join
* tunable consistency
* BASE instead of ACID
* not row-level consistent(?,!)
* AP

= HBase
* sparse data
* versioning, keep writing and clean old later
* row-level consistency
* Drill for SQL
* fast reads, slow writes
* CP
* no transactions
* for reporting

= MongoDB
* BSON
* no join
* complex where operations
* profiling
* only master is up-to-date
* for speed and distribution

= AWS DynamoDB
* schema-less
* row only 64kn
* query max 1MB
* limited checks on data
* no join, no complex queries
* priced on throughput
* speed as a service

= Teradata
* Petabyte tables
* SQL

= MonetDB
* free, for very large data
* SQL
* Python

== Spline
https://www.youtube.com/watch?v=T2vNPBCfA64
* Spark data lineage graphs (not in Atlas)
* reads Spark DAG and create info and visualization

+++++++++++++++
SQLite performance:
* explicit transactions
* INTEGER PRIMARY KEY faster since direct
* very large row to separate table
* BLOBs in separate table of file outside
* CREATE TEMP TABLE if not needed afted session
* prepared statements for multiple executions
* VACUUM to defragment
* column order affects performance: columns small data, often accessed -> columns first
* index contains full copy of indexed data
* fragmentation:
  * appending data to columns with indexes causes interleaved index and table data
  * appending data to multiple tables interleaves tables too
  * deleting data does not shrink the database, just marks pages as unused
* by default sqlite will grow/shrink the database file by |page_size|. This behavior causes new data blocks to be allocated too. This is especially problematic on OSX, Linux XFS.; SQLite CHUNK_SIZE feature is a way to minimize this problem 

SQLite:
* fulltext search
* multiple processes can access
* 

Postgres:
* better query planning
* do it right or not at all (MySQL cuts corners)
* streaming backups: https://github.com/wal-e/wal-e

At a bare minimum, every developer should know how to:

    Implement a binary search on a sorted array.
    Implement a singly linked list from scratch, and know how to determine whether a singly-linked list contains a cycle in O(n) time.
    Merge two sorted arrays into another sorted array. Assuming a developer knows that, they should be able to implement a merge sort from scratch.
    Implement a bubble sort.
    Solve the FizzBuzz problem. Seriously.
    Insert, lookup, and delete an item into a binary tree. (Note: shouldn't be concerned about keeping the tree balanced or not.)
    Implement a hashtable with O(1) (or very nearly O(1)) lookup, insert, and deletes.
    Construct an array of fibs from 0 to n in O(n) time.
    Construct an array of primes from 0 to n using Sieve of Eratosthenes.
    Implement a depth-first search of a binary tree.
    Implement a breadth-first search of binary tree.

None of these are particular difficult for a competent developer, even if you've never implemented them before.

And while its not a requirement, I think its important for developers to know:

    A smidgen about graph data structures, even if they never use them a lot in practice. Developers should familiarize themselves with Dijkstra's algorithm, Prim's algoritm, and Kruskal's algorithm.
    How to solve Knight's Tour and 8-Queens puzzle.
    How to write a basic minimax function.
    Given a particular arrangement of a chess board, determine whether a king is checkmated.
    How to find the best poker hand in a set of 5 or more cards.

------------
Good clean code:
    Make a good impression

Take some of the well-known books, e.g. Clean Code, Code Complete, Coders at Work, The Clean Coder: A Code of Conduct for Professional Programmers etc. (see here for full lists) and give them a couple of days to read one - at work, in a private office. Space those out, say 1 book a month or a quarter. They will see from the effort that what you're personally saying is really important, not just "the company line" to take with a grain of salt. Obviously make sure you've got management in line with this - you don't want them saying in passing "huh? what's person X up to? holed away in there with the door closed?"

Along with continual, more formal training classes in the latest technologies.

Also, as things are done "the right way" gave big encouragement and even rewards. Otherwise it can be more of a negative, punishing, rather than positive, rewarding environment. Most folks want to do and want to be known for doing "a good job" if they have the tools to do it.

    Practice what you preach, Preach what you practice

Talk about code, talk about the good principles, talk about new tools, become 'known' for it.

Provide / support /suggest screencasts, videos, peepcodes and whatever online tutorials and classes you can find.

Support and suggest appropriate local user groups, including those on sites like http://www.meetup.com

If you are in office (i.e. not virtual) a well-stock bookshelf of the actual books you would like people to use is good. Find a way to make this be "not just dusty bookshelf in the corner" but placing it really prominently, Moving the books around,etc. Use your imagination too. Maybe every programmer gets one book as 'homework' a month and you have a monthly meeting where they get to 'present their findings' !

This will make far more of an impression that any 10 minute conversation alone and it will remove you from the 'criticizer' role and allow them to learn how to fish for themselves (rather than giving them fish, you know the deal). Some junior folks also find it intimidating to have a senior folk always explain stuff, when sometimes all they really want is some time to study, practice and absorb it.

    Instill a culture of learning and excellence

Basically you want to create "a culture of learning and excellence" so that you can Practice what you Teach and inspire others to do the same.

This should be in conjunction with code reviews to see if/how the principles apply to the actual work being done. Conversely, code reviews done without the above can feel like whipping sessions to the student no matter how well intentioned by the teacher.
-------------
All programmers should always do code reviews with a senior programmer before committing/merging their code changes.

    This gives the senior programmers a chance to come to a consensus regarding which styles they want to enforce (and recognize which ones are just a matter of preference) among themselves.
    It gives junior programmers a chance to learn what practices are expected from their team right from the get-go. Within a couple of months, 90% of the code they write will already be following those practices before the code review.

Also, most IDEs have tools you can configure to enforce the vast majority of coding style preferences with handsome little squiggly underlines and options to automatically correct the style throughout a file. Junior programmers will pick up on that pretty fast.
-------------
Good Code:
easy modify
You come back to it after a weekend off, and you can jump straight in
Unit tests are easy to write
If it's hard to document, it's probably wrong. If it's hard to test, it's almost certainly wrong
You have enough information to call methods WITHOUT looking at the body of them.
BAD:
If you change the implementation of one method (but not the interface) you need to change the implementation of other methods. 
You have code that never runs in your application to provide "future flexibility" 
Large try/catch blocks
If I feel compelled to copy/paste blocks of code I'm doing something wrong
If I can't take the whole code in my head I'm doing something wrong
If someone jumps in and gets lost in my code I'm doing something wrong
When you've done everything you reasonably can to reduce coupling between classes; so that if Class A depends on Class B, and Class B is removed and Class C is put in it's place, little to no changes have to be made to Class A.



YARN:
* split resource mgmt (ResourceManager) from job scheduling/monitoring (ApplicationMaster)
* computation: ResourceManager, NodeManager
* memory, CPU, disk
* YARN_NODEMANAGER_OPTS env var
* to see resources from nodes go to RM UI //...:8088/cluster/nodes
* memory/CPU: min-alloc=increments; max-alloc=max
* container killed if vmem-pmem-ratio factor on map or reduce exceeded
* Mapper/Reduce ask for memory/CPU/disk; AM only ask for memory/CPU
* each container is JVM process
* usually java.opts=0.8 container memory

Hadoop?
* Cloudera sometimes use Kudu (distributed columnar store) instade of HDFS; rely on Spark instead of MapReduce
* Hortonworks: also Spark
* Databricks: Spark-people; prefer Mesos over YARN
* Hive useful for file-to-table mgmt
* Impala for massively parallel processing
* Tableau want to support Spark directly sometime
* Spark streaming a bit weak?
* Kerberos only full implemented auth for Hadoop (even though bad); use Ranger or Sentry to reduce pain (need, otherwise each part does own auth)
* Ranger (Hortonsworks) slightly ahead of Sentry (Cloudera)
* HBase good; with Phoenix you can query by SQL tool; Ambari integrated
* Impala: massively parallel on HDFS; much faster than Hive; can replace Teradata and Netezza; supported by Cloudera (Hortonswork Phoenix instead)
* Kafka: distributed messaging; a bit basic though
* Storm/Apex: better than Spark at streaming; but mgmt and API not so good
* Ambari/Cloudera Mgr: Best to monitor cluster
* Pig: won't be there for long; Spark better; other alternatives are Apache Nifi or Kettle
* Nifi/Kettle: Kettle more mature; improvement over Oozie?
* Knox: Edge protection; reverse proxy written in Java with auth; not well written; obscure errors
* Kylin: precalc into cubes(?)
* Atlas/Navigator: Hortonworks new data governance tool; no fully baked; may surpass Cloudera's Navigator

obsolete(?):
* Oozie: error-hiding; diff from docs; broken validator; written poorly
* MapReduce
* Tez

eSATA:
* is 1-to-1 only; may be faster than USB when many USB connected
* might be slightly faster on writes
* no voltage supply

USB3:
* backwards compatible (but not an USB3 cable)

HD:
* rpm (7000)
* 3 Jahre Garantie
* hardware encryption?
* included software
* Mac compatible?
* for large 3.5 drives a good fan

Filesystem:
Ext3: can be slow, max 16TB
Ext4: small files stored more efficiently, faster mkfs; still not suitable for very large files
XFS: many years of use for very large data; less known
BTRFS: copy-on-write; snapshots; compression; not yet production

Excel:
IF(ISNUMBER(SEARCH("word", field));"yes";"no")

\includegraphics[width=10cm,height=10cm,keepaspectratio]
\usepackage[space]{grffile}   % for spaces in filenames
% use / as path separator!

For portable Miktex:
Download full Miktex with Net-installer to USB
Connect to that repository with miktex\bin\mpm.exe
Packages will be installed automatically on demand

\usepackage[latin1]{inputenc}
\usepackage[extendedchars]{grffile}   % for umlauts in filename

fig.suptitle(title)

SELECT * can be slower since wouldn't use an index

Tableau:
ELSEIF
ISNULL()

Excel:
VERGEICH(wert;bereich;0)   -> gibt Zeilennummer zurück (engl. MATCH)

Identify Big Data role for business (actionable)
Hire architect that creates Big Data blueprint
ML experts: tech startups, defense, aerospace, oil exploration, universities, PhD
Soft skills: listen more than talk, balance team with strong leader, justify what working, quantify success, patient with others who not digital
Ppl who track record of innovation and agility, ppl need to adapt quickly
Data hunters, seasonsed business veterans: interpret, relevance, define KPI, define set of questions, "integrators"
CTO: wrangle technologies available to marketers into a strategy and technology solution for business
Good Ppl are hard to keep, fail fast on bad ppl; work needs to be valued, ppl well-utilized; not just fancy reporters
open up: supplement in-house staff with external specialists who can partner in leading-edge research and development
Go big or go home: make Big Data focus of whole company and not just marketing function; demonstrate value in C-suite


SQLite:
usually BEGIN TRANSACTION, COMMIT TRANSACTION
rather use apsw since Python DB API not good enough?


ORC vs Parquet:
* ACID only with ORC?
* metadata in ORC, bloomfilter (esp. for sum operations)
* Avro also fast, but row-wise
* Parquet: for highly nested data
* Parquet no indices yet?

== Machine Learning and additional algorithms

* Smile (https://github.com/haifengl/smile): ML kit
* PySparNN (https://github.com/facebookresearch/pysparnn): Approximate nearest neighbors on sparse data on Spark
* Annoy (https://github.com/spotify/annoy): Approximate nearest neighbors
* Thunder (https://github.com/thunder-project/thunder): Distributed analysis of images and time-series
* SpatialSpark (https://github.com/syoummer/SpatialSpark): Spatial operations on Spark
* Sparkit-learn (https://github.com/lensacom/sparkit-learn): Sklearn API on Spark
* FluctuationAnalysis (https://github.com/kamir/FluctuationAnalysis): Set of time series analysis
* Spark-Ext (https://github.com/collectivemedia/spark-ext): Additional ML transformers, estimators, aggregators for Spark
* Keystone ML (http://keystone-ml.org/): End-to-end ML on Spark
* Spark-clustering (https://github.com/TugdualSarazin/spark-clustering): Clustering on Spark
* DeepMining (https://github.com/HDI-Project/DeepMining): Gaussian copula process for hyperparameter optimization
* Exploratory-data-analysis (https://github.com/vicpara/exploratory-data-analysis): Some simple ideas for Spark/Scala(?)
* SparkAlgorithms (https://github.com/anantasty/SparkAlgorithms): old; few additional ML algorithms
* Blueyonder Vikos (https://github.com/blue-yonder/vikos): Supervised training of ML models

== Additional simple UDFs

* DataSketches (https://datasketches.github.io/): stochastic streaming algorithms for Hive UDFs
* Netflix Surus (https://github.com/Netflix/Surus): UDFs for Pig and Hive
* Dataiku Hive UDF (https://github.com/dataiku/dataiku-hive-udf): UDF and storage handlers for Hive
* HiveSwarm (https://github.com/livingsocial/HiveSwarm): useful Hive UDFs

== Performance tests

* Linkedin DrElephant (https://github.com/linkedin/dr-elephant): Performance monitoring for Hadoop and Spark
* Databricks Spark-perf: Spark performance tests
* Databricks Spark-sql-perf (https://github.com/databricks/spark-sql-perf): Spark performance tests
* Grafana-spark-dashboards (https://github.com/hammerlab/grafana-spark-dashboards): Scripts for generating Grafana dashboards for Spark performance
* Spark-bench (https://github.com/SparkTC/spark-bench): Spark benchmark suite

== Data processing

* Apache Samza (https://github.com/apache/samza): Stream processing
* Apache Storm (https://github.com/apache/storm): Real-time computation
* Oryx (https://github.com/OryxProject/oryx): Lambda with Spark and Kafka
* Onyx (https://github.com/onyx-platform/onyx): Computation framework in Clojure
* Bahir (https://github.com/apache/bahir): Extension for analytics on Spark/Flink
* Beam (https://github.com/apache/incubator-beam): Unified batch+stream processing on runners for Spark/Flink/GoogleCloudDataflow
* Falcon (https://github.com/apache/falcon): Feed processing
* Gobblin (https://github.com/linkedin/gobblin): Data ingestion
* Sqoop (https://github.com/apache/sqoop): SQL to Hadoop
* Blueflood (https://github.com/rackerlabs/blueflood): Ingest massive time-series data
* Apache NiFi (https://github.com/apache/nifi): Data routing, transformation etc.
* Hindsight (https://github.com/mozilla-services/hindsight): C based data processing infrastricutre based on Lua Sandbox; light skeleton around Lua Sandbox
* Twitter Scalding (https://github.com/twitter/scalding): Cascading with Scala
* Apache Apex (http://apex.apache.org/): Unified stream and batch processing

== Deep learning

* SparkNet (https://github.com/amplab/SparkNet): distributed neural networks on Spark
* Elephas (https://github.com/maxpumperla/elephas): Keras on Spark
* CaffeOnSpark (https://github.com/yahoo/CaffeOnSpark): Caffe on Spark
* DeepDist (https://github.com/dirkneumann/deepdist): Train deep belief networks on Spark
* Samsung Veles (https://github.com/Samsung/veles): Distributed ML
* Apache Singa (https://github.com/apache/incubator-singa): Distributed deep learning

== Analytics solutions

* Lumify (http://lumify.io/): Analyze, aggregate, visualize
* Druid (https://github.com/druid-io/druid): Fast column oriented analytics
* Linkedin Pinot (https://github.com/linkedin/pinot): Real-time OLAP
* Apache Ignite (https://github.com/apache/ignite): In-memory high-performance computing
* Lumify (https://github.com/lumifyio/lumify): Data analytics and visualization
* Tajo (https://github.com/apache/tajo): Relational distributed DWH
* Apache Lens (https://lens.apache.org/): Unified analytics interface on Spark/Hive/...
* Tuktu (http://www.tuktu.io/): Analytics for less technical with visual web data flows
* Apache Drill (http://drill.apache.org/): Schema-free SQL query engine
* Cloudera Ibis (http://www.ibis-project.org/): Python analytics on Hadoop (?)

== Storage and memory

* Alluxio (https://github.com/Alluxio/alluxio): distributed memory storage; formerly Tachyon
* Apache Arrow (https://github.com/apache/arrow): Enable process and move data fast
* Cloudera Kudu (https://kudu.apache.org/): Fast analytics on fast data
* Pinterest Terrapin (https://github.com/pinterest/terrapin): fast Hadoop data from local files

== Other

* Linkedin WhereHows (https://github.com/linkedin/WhereHows): Data catalog
* Myriad (https://github.com/apache/incubator-myriad): Scaling YARN on Mesos
* Giraph (https://github.com/apache/giraph): Large-scale Graph processing
* Spork (https://github.com/sigmoidanalytics/spork): Pig on Spark
* DataGen (https://github.com/SparklineData/DataGen): Data generation tool

=== Apache NiFi

* web UI for data flows
* buffering
* modify at runtime, dynamically prioritize data
* extensible
* data provenance
* MiNiFi: adds design/deploy and warm re-deploy

=== CarbonData

* new Hadoop file format
* read-optimized columnar storage
* multi-level index
* column group to leverage benefits of row-based
* dictionary to optimize aggregations
* optionally column data as inverted index for low cardinality column and better filtering
* different compressions
* Carbon-Spark integration (query optimization: multi-index push down, column pruning, defer decoding for aggregation)

=== Beam
* combine all of batch and stream advantages
* modelled after Google Dataflow
* provides model and SDK (Java, Python[alpha], Scala)
* Runners for Spark, Flink, ...
 
 +++++++++++++
Apache Falcon:
* fault-tolerant feed processing and management
* notifications
* integration with metastore

SnappyData:
* in-memory analytics for OLAP
* integrate Spark and GemiFire XD (in-memory transactional store)

H-Rider:
* view and manipulate HBase data

Apache Arrow
* columnar in-memory
* process and move data fast
* use SIMD (vectorized) processor operations
* single arrow memory as mediator between parquet, pandas, etc.

Apache Tika
* content metadata analysis (?)

= Apache NiFi

* from "NiagraFiles"
* Dataflow management
* need format, schema, protocol, ontology
* challenges after Kafka:
  * difficult when many independent parties involved
  * authorization, schema, interest, prioritization
  * flow control
* features:
  * guaranteed delivery
  * buffering
  * prios
  * QoS setting
  * provenance
  * logs
  * templates
  * pluggable multi-role security

= Hollow
After two years of internal use, Netflix is offering a new open source project as a powerful option to cache data sets that change constantly.

Hollow is a Java library and toolset aimed at in-memory caching of data sets up to several gigabytes in size. Netflix says Hollow's purpose is threefold: It’s intended to be more efficient at storing data; it can provide tools to automatically generate APIs for convenient access to the data; and it can automatically analyze data use patterns to more efficiently synchronize with the back end.

Ditch
=====
* Tez: always buggy
* Oozie: buggy
* Flume: better StreamSets or Kafka
* Storm: Spark, Apex, Flink better
* Hive: slow?
* HDFS: Java memory slow; NameNode bottleneck; better MaprFS, Gluster

Alternatives
============
* Cassandra: Riak, CouchBar
* Kafka: RabbitMQ
* YARN: Mesos, Kubernetes

HBase vs Cassandra
==================
https://hortonworks.com/blog/hbase-cassandra-benchmark/
* HBase faster for read-heavy
* Cassandra faster for write-heavy

= Speed comparison Big Data Databases
* Impala may be faster than Spark; Kognitio even faster?; Greenplum slightly faster?
* Presto ~ Hive < Spark


