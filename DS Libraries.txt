= Data Science Libraries

:toc:

== RECOMMENDED in this document

=== High dim decision boundary plot
* understand points close to decision boundary

=== YellowBrick

* Rank/correlate features
* parallel coordinates
* cluster evaluation
* some text visualization
* some supervised learning reports

=== Hypertools

* evaluate missing values interpolated by PPCA
* align (rotated?) matrices
* cluster visualization
* explore data

=== ELI5

* Explain ML models
* Text model explainers
* Feature/prediction explainer
* LIME
* Permutation importance

=== Xam

* check out occasionally

=== Supersmoother

* curve smooths with automatic neighbourhood size

=== Quick Table Profiling
* https://github.com/tkrabel/edaviz[edaviz]
* https://github.com/pandas-profiling/pandas-profiling[pandas-profiling]
* https://pair-code.github.io/facets/[facets]

=== Other
* FastAI
* LightGBM
* PyTorch
* CatBoost

== Programming support

* Code Formatter https://github.com/ambv/black[black]: seems to do a good job formatting highly nested structures; black was made to specify PEP8 formatting more clearly and more sensibly
** seems better than Yapf (some weird indents after function calls and dict keys)

== Dashboarding

https://pyviz.org/dashboarding/index.html

=== Dash (Plotly)

=== Panel (Anaconda)

* on Bokeh
* many plot libraries

=== Viola (QuantStack)

* convert notebooks to dashboards (strip code, keep interactivity)

=== Bowtie

https://github.com/jwkvam/bowtie

== Scipy

* http://scipy-cookbook.readthedocs.io/index.html[Scipy Cookbook]
* https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html[scipy.optimize] https://docs.scipy.org/doc/scipy/reference/optimize.html[Ref]: Optimization and root finding
* https://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html[scipy.interpolate]
* https://docs.scipy.org/doc/scipy/reference/tutorial/fftpack.html[scipy.fftpack]
* https://docs.scipy.org/doc/scipy/reference/tutorial/signal.html[scipy.signal]: Splines, Filtering, Spectral
* https://docs.scipy.org/doc/scipy/reference/tutorial/linalg.html[scipy.linalg]
* https://docs.scipy.org/doc/scipy/reference/tutorial/csgraph.html[scipy.sparse.csgraph]: Compressed sparse Graph routines
* https://docs.scipy.org/doc/scipy/reference/tutorial/spatial.html[scipy.spatial]: Triangulations, Voronie, Convex hulls (Qhull library)
* https://docs.scipy.org/doc/scipy/reference/tutorial/stats.html[scipy.stats] https://docs.scipy.org/doc/scipy/reference/stats.html[Ref]: Random variable distributions, Statistical metrics
* https://docs.scipy.org/doc/scipy/reference/tutorial/ndimage.html[scipy.ndimage]: Images as Numpy arrays
* https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html#module-scipy.cluster.hierarchy[Hierarchical clustering]: Many hierarchical cluster methods
* https://docs.scipy.org/doc/scipy/reference/odr.html[Orthogonal distance regression]

== Statsmodels

* http://www.statsmodels.org/stable/examples/index.html[Statsmodels examples]
* http://www.statsmodels.org/stable/index.html#basic-documentation[Statsmodels]
* http://www.statsmodels.org/stable/regression.html[Linear Regression]
* http://www.statsmodels.org/stable/glm.html[Generalized Linear Models]
* http://www.statsmodels.org/stable/gee.html[Generalized estimating equations]: Generalized Estimating Equations estimate generalized linear models for panel, cluster or repeated measures data when the observations are possibly correlated withing a cluster but uncorrelated across clusters.
* http://www.statsmodels.org/stable/rlm.html[Robust Linear Models]
* http://www.statsmodels.org/stable/mixed_linear.html[Linear Mixed Effects Models]: Linear Mixed Effects models are used for regression analyses involving dependent data. Such data arise when working with longitudinal and other study designs in which multiple observations are made on each subject.
* http://www.statsmodels.org/stable/discretemod.html[Regression with discrete dependent variable]: Binary (Logit, Probit), Nominal (MNLogit), Count (Poisson)
* http://www.statsmodels.org/stable/anova.html[ANOVA]
* http://www.statsmodels.org/stable/tsa.html[Time series analysis]: Basic models include univariate autoregressive models (AR), vector autoregressive models (VAR) and univariate autoregressive moving average models (ARMA). Non-linear models include Markov switching dynamic regression and autoregression. It also includes descriptive statistics for time series, for example autocorrelation, partial autocorrelation function and periodogram, as well as the corresponding theoretical properties of ARMA or related processes. It also includes methods to work with autoregressive and moving average lag-polynomials. Additionally, related statistical tests and some useful helper functions are available.
* http://www.statsmodels.org/stable/statespace.html[Time series analysis by State Space Methods]
* http://www.statsmodels.org/stable/duration.html[Survival and duration analysis]: Several standard methods for working with censored data. These methods are most commonly used when the data consist of durations between an origin time point and the time at which some event of interest occurred.
* http://www.statsmodels.org/stable/stats.html[Various statistical tests and tools]
* http://www.statsmodels.org/stable/nonparametric.html[Non-parametric methods]: Kernel density estimation, Kernel regression
* http://www.statsmodels.org/stable/gmm.html[Generalized methods of moments]
* http://www.statsmodels.org/stable/contingency_tables.html[Contingency tables]: Variety of approaches for analyzing contingency tables, including methods for assessing independence, symmetry, homogeneity, and methods for working with collections of tables from a stratified population.
* http://www.statsmodels.org/stable/imputation.html[Multiple Imputation with Chained Equations (MICE)]
* http://www.statsmodels.org/stable/emplike.html[Empirical likelihood]: Empirical likelihood is a method of nonparametric inference and estimation that lifts the obligation of having to specify a family of underlying distributions. Moreover, empirical likelihood methods do not require re-sampling but still uniquely determine confidence regions whose shape mirrors the shape of the data. In essence, empirical likelihood attempts to combine the benefits of parametric and nonparametric methods while limiting their shortcomings.
* http://www.statsmodels.org/stable/miscmodels.html[Misc. models]: Poisson variations, t-distributed errors
* http://www.statsmodels.org/stable/distributions.html[Statistical distributions]
* http://www.statsmodels.org/stable/graphics.html[Graphics]: Goodness of fits, Boxplots, Correlation plots, Regression plots, Time series plots, ...
* http://www.statsmodels.org/stable/tools.html[Tools]: Basic data transformations, Numerical differentiation, Performance measures

== Scikit-learn related

* http://scikit-learn.org/stable/related_projects.html[Sklearn Related Projects]
* https://github.com/scikit-learn-contrib[Sklearn Contrib]
** https://github.com/scikit-learn-contrib/categorical-encoding[Sklearn Contrib Categorical Encoding]
** https://github.com/scikit-learn-contrib/imbalanced-learn[Imblearn]
** https://github.com/scikit-learn-contrib/hdbscan[HDBSCAN]
** https://github.com/scikit-learn-contrib/forest-confidence-interval[Forest Confidence Interval]
** https://github.com/scikit-learn-contrib/boruta_py[Boruta Py]
** https://github.com/scikit-learn-contrib/sklearn-pandas[Sklearn Pandas]
** https://github.com/scikit-learn-contrib/lightning[Lightning]: Large-scale linear classification/regression/ranking
** https://github.com/scikit-learn-contrib/py-earth[PyEarth]: MARS
** https://github.com/scikit-learn-contrib/polylearn[Polylearn]: Factorization and polynomial networks
* Structured learning
** https://github.com/larsmans/seqlearn[SeqLearn]
** https://github.com/hmmlearn/hmmlearn[HMMLearn]
** https://pystruct.github.io/[PyStruct]
** https://github.com/jmschrei/pomegranate[Pomegranate]
** https://github.com/TeamHG-Memex/sklearn-crfsuite[CRFSuite]

== Algorithms

* https://github.com/clara-labs/spherecluster[SphereCluster]: Clustering on Sphere (von Mises distr)
* https://github.com/scikit-learn-contrib/hdbscan[HDBScan]
* https://github.com/nicodv/kmodes[KModes]
* https://github.com/jmetzen/sparse-filtering[Sparse filtering]: Unsupervised feature learning
* https://github.com/lda-project/lda[LDA]: Fast
* https://github.com/alexfields/multiisotonic[Multidimensional Isotonic Regression]
* https://github.com/trevorstephens/gplearn[GPLearn]: Genetic programming
* https://github.com/jmetzen/kernel_regression[Kernel Regression]: Nadarays-Watson kernel regression with automatic bandwidth
* https://github.com/scikit-learn-contrib/lightning[Lightning]: Large-scale linear machine learning

== Production

* https://github.com/ajtulloch/sklearn-compiledtrees/[Compiled Decision Trees]: C++
* https://github.com/nok/sklearn-porter[Sklearn Porter]: Transpiled estimators for C, Java, Javascript, ..s

== Performance libraries

* https://pypi.python.org/pypi/xfork[xfork]: Using a Lazy Proxy object, submit CPU-bound tasks to processes and IO-bound tasks to threads

==== Vaex
* https://vaex.io/[Vaex]: Lazy Out-of-Core DataFrames for Python
* large data, memory-mapped for speed
* lazy expressions instead of eager
* with plotting
* bin_by instead of group_by
* can also work on remote data

=== Weld

* express operations (Numpy, Pandas, Spark) in intermediate repr which is first optimized
* Grizzly (Pandas), WeldNumpy

== Data libraries

* https://python-lenses.readthedocs.io/en/latest/tutorial/intro.html[Lenses]: Query/modify nested Python data structures
* glom: access nested data; error/exception treatment
* https://github.com/ContinuumIO/intake
* https://github.com/SciTools/iris[Iris]: Working with (geo) data; regridding etc.
* https://yt-project.org/[Yt]: Volumetric data

== REP (Yandex)

* http://yandex.github.io/rep/[REP]
* Helper functions for machine learning
* Documentation a bit raw -> hard to understand and not clear what is better than self-made
* Wraps estimators(?)
* Work with multiple classifiers at once
* Parallelization
* Metric objects

== High dim decision boundary plot

* https://github.com/tmadl/highdimensional-decision-boundary-plot[Highdim Decision Boundary Plot] (last update June 2016)
* _project only points close to decision boundary_ (better than full 2D projection)
* amount of over-/under-fitting -> see whether surface in projection makes extra squiggles
* choose any dim reduction method
* requires NLopt
* inspect misclassified
* model complexity
* contribution of data points to decision surface
* find uncertain regions

== Artemis

* https://github.com/QUVA-Lab/artemis[Artemis]
* Run experiments; wants to be more intuitive than Sacred
* Live plots and monitoring; in browser
* Define experiments, log results
* Download/Cache data to local machine
* Experiment script will store output and plot to `~/.artemis/experiments`
* https://rawgit.com/petered/data/master/gists/experiment_tutorial.html[Experiment Example]

    from artemis.experiments import experiment_function
    @experiment_function
    def multiply_3_numbers(a=1, b=2, c=3):
        return a*b*c

    record = multiply_3_numbers.run()

[cols="m,d"]
|===
| multiply_3_numbers.browse()               | See past experiments with parameters
|===

== Sacred

* https://github.com/IDSIA/sacred[Sacred]
* Run parametrized experiments
* `@ex.config` turn all local vars into config
* easily run diff param from CLI

    from sacred import Experiment
    ex = Experiment('hello_config')

    @ex.config
    def my_config():
        recipient = "world"
        message = "Hello %s!" % recipient

    @ex.automain          # needs to be last function in file
    def my_main(message):
        print(message)

    > python hello_world.py
    INFO - hello_world - Running command 'my_main'
    INFO - hello_world - Started
    Hello world!
    INFO - hello_world - Completed after 0:00:00


[cols="m,d"]
|===
| python script.py                          | Run experiment
| ... print_config                          | Print config vars to main
| ... with <varname>="<value>"              | Set parameter manually
| from my_exp import ex +
  run = ex.run()                            | Run from other script
|===

* `Experiment` will fail in Jupyter since not reproducibly; use `Experiment(interactive=True)` to force
* `ex.main` instead of `ex.automain` would require you to write `__main__` and call `ex.run_commandline()`
* run creates `Run` object
* Continue in http://sacred.readthedocs.io/en/latest/experiment.html

== Scikit-Learn Laboratory Skll

* https://github.com/EducationalTestingService/skll[Skll]
* run experiments with text config
* define text config `[Input]`, `learners=..`, `[Output] metrics=...`, ...
* rigid structure for simple tests?

== YellowBrick DistrictDataLabs

* https://github.com/DistrictDataLabs/yellowbrick[YellowBrick]
* _Visualizers to help model selection_
* wrap estimators whose goal is to visualize
* https://github.com/DistrictDataLabs/yellowbrick/blob/develop/examples/examples.ipynb[Examples]
* Rank feature pairs to detect covariance
* RadViz to detect separability
* Parallel coordinates to detect clusters

== Hypertools

* https://github.com/ContextLab/hypertools[Hypertools]
* _interactive data dim reduction in 3D (or 2D)_
* static or animated plots
* data manipulation (hyperalignment, k-means, normalizing, ...)
* create plot from high dim data and dim reduction function
* http://blog.kaggle.com/2017/04/10/exploring-the-structure-of-high-dimensional-data-with-hypertools-in-kaggle-kernels/[Kaggle Blog Article]
* https://arxiv.org/abs/1701.08290[Arxiv Paper]
* evaluate missing values interpolated by PPCA
* procrustes function to align matrix that has been rotated
* align data: http://hypertools.readthedocs.io/en/latest/auto_examples/plot_align.html[Align matrices 1], http://hypertools.readthedocs.io/en/latest/auto_examples/plot_procrustes.html[Align matrices 2]
* `describe` to evaluate integrity of dim reduction (by variance correlation) http://hypertools.readthedocs.io/en/latest/auto_examples/plot_describe.html[Example]
* http://hypertools.readthedocs.io/en/latest/auto_examples/explore.html[Explore] data

== Xcessiv

* https://github.com/reiinakano/xcessiv[Xcessiv]
* Web-based scalable, automated hyperparameter tuning and stacking

== ELI5

* https://github.com/TeamHG-Memex/eli5/[ELI5]
* Debug/inspect ML models
* Text (also named entity), Features

== Scikit-Plot

* https://github.com/reiinakano/scikit-plot[Scikit Plot]
* Some plotting functions
* Self-made or other libs (e.g. YellowBrick) prob better

== Xam

* https://github.com/MaxHalford/xam[xam]
* mainly some sparsely documented personal functions
* but interesting:
** `xam.linear.AUCRegressor`
** https://github.com/MaxHalford/xam/blob/master/docs/preprocessing.md[Preprocessing]

== Parfit

* https://github.com/jmcarpenter2/parfit[Parfit]
* parallelize fit and scoring
* seems good, but low on documentation

== TadViewer

* http://tadviewer.com/[TadViewer]
* CSV viewer by Pivot table, loads into SQLite

== XGBoost

* http://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-tree-booster[Parameters]

== Better Exceptions

* https://github.com/Qix-/better-exceptions[Better Exceptions]
* print variable values
* does not work in interactive; only the script from file called

== Ray

* https://github.com/ray-project/ray[Ray]
* Distribute work
* Support for hyperparam tuning, deep learning, reinforcement learning

    import time
    import ray
    ray.init()

    @ray.remote
    def f():
        time.sleep(1)
        return 1

    object_ids = [f.remote() for i in range(4)]
    results = ray.get(object_ids)

== Supersmoother

* https://github.com/jakevdp/supersmoother[Supersmoother]
* Non-parametric locally-linear smooth with size of neighborhood tuned http://www.slac.stanford.edu/cgi-wrap/getdoc/slac-pub-3477.pdf[Paper]

== MLXtend

* http://rasbt.github.io/mlxtend/USER_GUIDE_INDEX/[MLXtend]
* some additional (meta) classifiers
* some plotting
* http://rasbt.github.io/mlxtend/user_guide/plotting/plot_decision_regions/[decision regions]
* most of this can probably done differently better

== Sklearn Deap

* https://github.com/rsteca/sklearn-deap[Sklearn Deap]
* evolutionary parameter search for Sklearn

== Jupyter

* https://github.com/pixiedust/pixiedust[PixieDust]: Improve user-experience of Jupyter notebooks with Spark, Python/Scala, Graphical, embedded Apps

== EXCLUDED

* Expyriment
* Experimentator
* OpenSesame

== 3D visualizer

https://pyviz.org/scivis/index.html