== Beam
* frame to create pipeline
* executed on engine like Spark or Flink

== Ignite
* in-memory distributed

== Onyx
* masterless distributed computing
* Clojure, Java

== Apache Fluo
* Incrementally update Apache Accumulo

== Arrow
* columnar in-memory format

== Beringei
https://github.com/facebookincubator/beringei
* in-memory storage for time-series (Facebook)
* open source of paper "Gorilla'15"
* backed by disk for persistence
* efficient streaming compression
* http service enables Grafana integration

== Stream processing

=== Apex
* stream & batch processing
* Malhar: operators and connectors
* native streaming
* Java
* very low latency
* advanced partitioning (parallel pipes, unifiers)
* advanced processing locality
* dynamic topology
* Kerberos, RBAC, LDAP
* automatic backpressure
* end-to-end exactly once
* GUI interface
* similar to Flink (more production tested?)

=== Heron
* successor of Storm

=== Benchmarks
* Apex lower latency than Flink (Nov16, https://www.datatorrent.com/blog/throughput-latency-and-yahoo/)
* Spark lower latency, but high throughput

== Machine Learning Frameworks

=== Ignite-ML
https://github.com/DevJoker/ignite-ml
* for Apache Ignite

=== Neon
https://github.com/NervanaSystems/neon
* Python DL by Nervana
* fast image networks

=== Brainstorm
https://github.com/IDSIA/brainstorm
* multiple computing backends

=== Veles
https://velesnet.ml
* distributed DL by Samsung

=== Microsoft Cognitive Toolkit
https://www.microsoft.com/en-us/research/product/cognitive-toolkit/
* formerly CNTK

=== Apache Singa
http://singa.apache.org/en/downloads.html
* Py2.7

=== Pentuum
http://www.petuum.com


=== Distributed ML Toolkit
http://www.dmtk.io
* by Microsoft

=== Matex
https://github.com/abhinavvishnu/matex
* Machine Learning Toolkit for Extreme scales

=== Apache SAMOA
https://github.com/apache/incubator-samoa
* distributed streaming ML
* works with Apex

=== MLPack
http://mlpack.org
* scalable ML in C++

=== Fregata
https://github.com/TalkingData/Fregata
* light-weight, fast ML on Spark
* higher accuracy, faster than MLLib
* parameter free SGD through GSA SGD
