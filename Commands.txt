eng=create_engine("mysql+pymysql://root:sonne@localhost")
conn=eng.connect()
list(conn.execute("show tables in ing"))

with open("filename", newline="", encoding="utf8") as f:
	reader=csv.reader(f, delimiter=";")
	for row in reader:
	    ...
		
df=pd.read_csv("filename")

df.drop(["col1", ..], axis=1)
df.dropna()

df["logA"]=np.log(df.A+1)

df.A.map({l:i for i, l in enumerate(df.A.unique())})

df1=df.dropna(subset=["A", "B"])
sns.violinplot(df.contvar, df.catvar)

sns.jointplot(df.contvar1, df.contvar2)

df_num=df.select_dtypes(include=[np.number]).drop(["idcol"], axis=1)
pd.scatter_matrix(df_num)

for col in df_num:
    sns.distplot(df_num[col])
    plt.show()
	
sns.jointplot(df.A, df.B, kind="reg");
	
sns.jointplot(df.A, df.B, kind="hex", joint_kws={"bins":"log"});
	
plt.hexbin(df.A, df.B, bins="log", cmap="YlOrRd");
plt.colorbar();

Xs=StandardScaler().fit_transform(X)

sns.factor(x, ycont, hue, data=df)

pd.get_dummies(df.ix[:,["A", "B"]], columns=[..])  # columns if not just object/category encoded
df.join(pd.get_dummies(df.A,prefix="A"))
df["A_fact"]=pd.factorize(df.A)[0]

np.hstack((df.loc[:,["A"]].values, pd.get_dummies(df.ix[:,["B"]]).values))
np.hstack((np.expand_dims(LabelEncoder().fit_transform(df1.A),axis=1), pd.get_dummies(df1.B)))
df.join(pd.get_dummies(df.ix[:,["A"]], columns=["A"]))

Imputer(strategy="mean").fit_transform(df.ix[:,["Age"]])   # "median", "most_frequent"

from sklearn.preprocessing import StandardScaler
scaler=StandardScaler().fit(X_train)

pd.concat((df, pd.DataFrame(Imputer().fit_transform(df.ix[:,["A"]]), columns=["AImp"]) ),axis=1)

X=df.ix[:,["A", "B"]]
y=df.C
X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.2, random_state=0)

from sklearn.ensemble import RandomForestClassifier
clf=RandomForestClassifier(200, n_jobs=-1)
sort(zip(clf.feature_importances_,feature_names)

from sklearn import svm
clf=svm.SVC()

from sklearn.linear_model import LogisticRegression
clf=LogisticRegression()

scores=cross_validation.cross_val_score(clf, X, y, cv=5)
print(scores.mean(), scores.std()*2)

# http://scikit-learn.org/stable/auto_examples/grid_search_digits.html
clfgrid=GridSearchCV(clf, [{"p1":val1, ..}, {..}], cv=5, scoring=score, n_jobs=-1)   # crossproduct of params in all dicts; score="precision", "recall", 
clfgrid.fit(X_train, y_train)
clfbest=clfgrid.best_estimator_
scorebest=clfgrid.best_score_
parambest=clfgrid.best_params_
for params, mean_score, scores in clf.grid_scores_:
    print(mean_score, scores.std()/", params)
	
clfgrid.RandomizedSearchCV(clf, [..], n_iter, scoring=, cv=5, n_jobs=-1)  # use refit=True to refit clfgrid on all data

from sklearn.linear_model import LogisticRegression
from sklearn.grid_search import GridSearchCV
clf=LogisticRegression()
clfgrid=GridSearchCV(clf, {"penalty":["l1", "l2"], "C":np.logspace(-5,1,13)}, refit=True)  # 13=6*2+1

from sklearn.metrics import *
y_pred=clfgrid.predict(X_test)
print(classification_report(y_test, y_pred))
	
from sklearn import metrics
y_pred=clfbest.predict(X_test)
metrics.accuracy_score(y_test, y_pred)
metrics.classification_report(y_test, y_pred)
metrics.confusion_matrix(y_test, y_pred)

clf=clfgrid.best_estimator_
clf.fit(X_train, y_train)
d=pd.DataFrame.from_records(sorted(zip(clf.feature_importances_, dfX.columns), reverse=True), columns=["Imp","Feat"])
d.set_index("Feat").plot(kind="bar")

clf.fit(X_train, y_train)
clf.predict(X_test)


from sklearn.dummy import DummyClassifier
for i in range(7):
    X1=X_train[:,i:i+1]
    clf=DummyClassifier("stratified")
    clf.fit(X1, y_train)
    y_pred=clf.predict(X_test[:,i:i+1])
    print("Dummy ", i)
    print(classification_report(y_test, y_pred))
	

ROC CURVE
from sklearn.metrics import roc_curve
y_pred = clf.predict_proba(X_test)[:,1]
fpr, tpr, th = roc_curve(y_test, y_pred)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.plot(fpr, tpr, label="..")

for i in range(X_test.shape[1]):
    vals=X_test[:,i]
    name=X.columns[i]
    if name=="Pclass_3":
        vals=-vals
    fpr, tpr, th = roc_curve(y_test, vals)
    plt.plot(fpr, tpr, label=name)
plt.legend(loc="best")

LEARNING CURVE	
train_sizes, train_scores, test_scores = learning_curve(clf, X, y, cv=10, n_jobs=-1, train_sizes=np.linspace(.1, 1., 10), verbose=0)
train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)
plt.figure()
plt.xlabel("Training examples")
plt.ylabel("Score")
#plt.gca().invert_yaxis()
plt.plot(train_sizes, train_scores_mean, 'o-', color="b", label="Training score")
plt.plot(train_sizes, test_scores_mean, 'o-', color="r", label="Test score")
plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color="b")
plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color="r")
plt.legend(loc="best")
plt.show()
#plt.gca().invert_yaxis()

VALIDATION CURVE
param_name="gamma"
score_name="accuracy"
clf=SVC()
train_scores, test_scores = validation_curve(clf, X, y, param_name=param_name, param_range=np.logspace(-6, -1, 5), cv=10, scoring=score_name, n_jobs=1)
train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)

plt.xlabel(param_name)
plt.ylabel(score_name)
#plt.ylim(0.0, 1.1)
plt.plot(param_range, train_scores_mean, label="Training score", color="r")
plt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color="r")
plt.plot(param_range, test_scores_mean, label="Cross-validation score", color="g")
plt.fill_between(param_range, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color="g")
plt.xscale("log")
plt.legend(loc="best")
plt.show()

Scoring
=======
Classification: accuracy, f1, precision, recall, roc_auc
Regression: mean_absolute_error, mean_squared_error, r2


<script type="text/javascript">
    $('div.input').hide();
</script>

chardetect filename