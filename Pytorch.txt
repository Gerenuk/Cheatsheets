= General

* http://pytorch.org/docs/master/[documentation] and http://pytorch.org/tutorials/[tutorials] stored separately
* https://discuss.pytorch.org/[forums]

== Already read

* https://towardsdatascience.com/pytorch-tutorial-distilled-95ce8781a89c (good)
* http://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/ (bad)

= Commands

|===
| from torch import *                       |
| t=Tensor(nparr)                           |
| nparr=t.numpy()                           |
| t.cuda()                                  | Use on GPU
| t.cpu()                                   | Back on CPU
|===

|===
| t.type(cuda.FloatTensor)                  | Set type and on GPU
| t.zero_()                                 | Make all zero
| FloatTensor([..])                         |
|===

    model = model.cuda()
    inputs = Variable(inputs.cuda())
    outputs = model(inputs)
    outputs = outputs.cpu()

= Variables

* Variables: for computational graph to enable autodiff
* in newer Pytorch just use tensor?

|===
| v.data                                    | Initial tensor
| v.grad                                    | Gradient
| v.backward()                              | Update gradients
|===

= Optimizer

    loss_fn = nn.MSELoss()
    optimizer = optim.SGD(model.parameters(), lr=learning_rate)

    for step in range(5):
        ...
        loss = loss_fn(pred, y)

        optimizer.zero_grad()  # manually zero all previous gradients
        loss.backward()
        optimizer.step()       # apply new gradients

= Autograd

To define new functions

    class F(torch.autograd.Function):
    @staticmethod
    def forward(ctx, args, gamma):
        ctx.gamma = gamma          # to store variables for backward
        ctx.save_for_backward(x)   # just for saving input or output (does some additional checks)
        pass

    @staticmethod
    def backward(ctx, args):
        pass

    F.apply(x, gamma)

* using __init__ and __call__ is old-style and will be deprecated

= Neural network module

* input variables, output variables; may hold state variables

    model = nn.Sequential([
        nn.Conv2d(1, 20, 5),
        nn.ReLU(),
        ...
        ])

    output = model(input_)

For more complex models subclass:

    class Model(nn.Module):
        def __init__(self):
            super().__init__()
            ...

        def forward(self, x):
            ...
            return x

|===
| nn.Module.train()                         | Set to training mode (default)
| nn.Module.eval()                          | Set to evaluation mode
|===

* `model.train()` important for DropOut and BatchNorm, since they behave differently in training vs evaluation model; `self.training=True` is default

== Weight initialization

|===
| torch.nn.init.normal(w)                   |
|===

== Exclude subgraphs from backprop

* http://pytorch.org/docs/master/notes/autograd.html

|===
| Variable(.., requires_grad=False?)        | Disable for current, but keep for children
| Variable(.., volatile=True?)              | Deprecated in 0.4? Disable for current and children
|===

== Freeze layers

    model.conv.weight.requires_grad=False
    model.conv.bias.requires_grad=False

    optimizer = SGD(filter(attrgetter("requires_grad"), model.parameters()))    # without this there will be an error

== Training process

* Learning rate scheduler http://pytorch.org/docs/master/optim.html#how-to-adjust-learning-rate[Ref]

== Save model

|===
| save(model.state_dict(), path)            | Recommended
| model.load_state_dict(load(path))         |
| save(model, path)                         | Save whole model
| model = load(path)                        |
|===

== Logging add-ons

* https://github.com/oval-group/logger
* https://github.com/torrvision/crayon
* https://github.com/TeamHG-Memex/tensorboard_logger
* https://github.com/lanpa/tensorboard-pytorch
* https://github.com/facebookresearch/visdom

= GPU

== Pinned memory

* http://pytorch.org/docs/master/notes/cuda.html#use-pinned-memory-buffers
* `.cuda(async=True)`, data-loader `pin_memory=True`


= Snippets

== Custom back-propagation

    import torch

    class MyFunction(torch.autograd.Function):

        @staticmethod
        def forward(ctx, input):
            ctx.save_for_backward(input)
            output = torch.sign(input)
            return output

        @staticmethod
        def backward(ctx, grad_output):
            # saved tensors - tuple of tensors, so we need get first
            input, = ctx.saved_variables
            grad_output[input.ge(1)] = 0
            grad_output[input.le(-1)] = 0
            return grad_output

    # usage
    x = torch.randn(10, 20)
    y = MyFunction.apply(x)
    # or
    my_func = MyFunction.apply
    y = my_func(x)

    # and if we want to use inside nn.Module
    class MyFunctionModule(torch.nn.Module):
        def forward(self, x):
            return MyFunction.apply(x)

`ctx` to store variables.

== Direct device allocation

    import torch

    # check is cuda enabled
    torch.cuda.is_available()

    # set required device
    torch.cuda.set_device(0)

    # work with some required cuda device
    with torch.cuda.device(1):
        # allocates a tensor on GPU 1
        a = torch.cuda.FloatTensor(1)
        assert a.get_device() == 1

        # but you still can manually assign tensor to required device
        d = torch.randn(2).cuda(2)
        assert d.get_device() == 2

== Custom data loader

    import torch
    import torchvision as tv

    class ImagesDataset(torch.utils.data.Dataset):
        def __init__(self, df, transform=None,
                     loader=tv.datasets.folder.default_loader):
            self.df = df
            self.transform = transform
            self.loader = loader

        def __getitem__(self, index):
            row = self.df.iloc[index]

            target = row['class_']
            path = row['path']
            img = self.loader(path)
            if self.transform is not None:
                img = self.transform(img)

            return img, target

        def __len__(self):
            n, _ = self.df.shape
            return n

    # what transformations should be done with our images
    data_transforms = tv.transforms.Compose([
        tv.transforms.RandomCrop((64, 64), padding=4),
        tv.transforms.RandomHorizontalFlip(),
        tv.transforms.ToTensor(),
    ])

    train_df = pd.read_csv('path/to/some.csv')
    # initialize our dataset at first
    train_dataset = ImagesDataset(
        df=train_df,
        transform=data_transforms
    )

    # initialize data loader with required number of workers and other params
    train_loader = torch.utils.data.DataLoader(train_dataset,
                                               batch_size=10,
                                               shuffle=True,
                                               num_workers=16)

    # fetch the batch(call to `__getitem__` method)
    for img, target in train_loader:
        pass

== Pseudo-code of all

    class ImagesDataset(torch.utils.data.Dataset):
        pass

    class Net(nn.Module):
        pass

    model = Net()
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
    scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
    criterion = torch.nn.MSELoss()

    dataset = ImagesDataset(path_to_images)
    data_loader = torch.utils.data.DataLoader(dataset, batch_size=10)

    train = True
    for epoch in range(epochs):
        if train:
            lr_scheduler.step()

        for inputs, labels in data_loader:
            inputs = Variable(to_gpu(inputs))
            labels = Variable(to_gpu(labels))

            outputs = model(inputs)
            loss = criterion(outputs, labels)
            if train:
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

        if not train:
            save_best_model(epoch_validation_accuracy)
