////
* Sort alphabetically
////

= Data Science Cookbook

:toc: left

== Plotting

* https://github.com/bokeh/colorcet[Colorcet]: Perceptually uniform color maps

=== Statsmodels Statistical Plots

[cols="m,d"]
|===
| qqplot(data)                              | Quantile vs Quantile plot (default vs normal) http://www.statsmodels.org/stable/generated/statsmodels.graphics.gofplots.qqplot.html#statsmodels.graphics.gofplots.qqplot[Ref]
| _line_=45                                 |
| qqline(ax)                                | Plot reference line for qqplot http://www.statsmodels.org/stable/generated/statsmodels.graphics.gofplots.qqline.html#statsmodels.graphics.gofplots.qqline[Ref]
| qqplot_2samples(data1, data2)             | Quantile vs Quantile plot for two samples http://www.statsmodels.org/stable/generated/statsmodels.graphics.gofplots.qqplot_2samples.html#statsmodels.graphics.gofplots.qqplot_2samples[Ref]
| ProbPlot(data)                            | Convenience class for QQ/PP plots http://www.statsmodels.org/stable/generated/statsmodels.graphics.gofplots.ProbPlot.html#statsmodels.graphics.gofplots.ProbPlot[Ref]
| violinplot(data)                          | http://www.statsmodels.org/stable/generated/statsmodels.graphics.boxplots.violinplot.html#statsmodels.graphics.boxplots.violinplot[Ref]
| plot_corr(dcorr)                          | Plot correlation matrix http://www.statsmodels.org/stable/generated/statsmodels.graphics.correlation.plot_corr.html#statsmodels.graphics.correlation.plot_corr[Ref]
| plot_corr_grid(dcorrs)                    | Plot grid of correlation matrices http://www.statsmodels.org/stable/generated/statsmodels.graphics.correlation.plot_corr_grid.html#statsmodels.graphics.correlation.plot_corr_grid[Ref]
| scatter_ellipse(data)                     | Scatter plot with confidence ellipses http://www.statsmodels.org/stable/generated/statsmodels.graphics.plot_grids.scatter_ellipse.html#statsmodels.graphics.plot_grids.scatter_ellipse[Ref]
| fboxplot(data)                            | Like boxplot for continuous x-axis http://www.statsmodels.org/stable/generated/statsmodels.graphics.functional.fboxplot.html#statsmodels.graphics.functional.fboxplot[Ref]
| rainbowplot(data)                         | Plot all lines colored by order of funtional depth http://www.statsmodels.org/stable/generated/statsmodels.graphics.functional.rainbowplot.html#statsmodels.graphics.functional.rainbowplot[Ref]
| banddepth(data)                           | Plot centrality of data (e.g. median curve is highest) http://www.statsmodels.org/stable/generated/statsmodels.graphics.functional.banddepth.html#statsmodels.graphics.functional.banddepth[Ref]
| abline_plot(intercept, slope)             | Plot line http://www.statsmodels.org/stable/generated/statsmodels.graphics.regressionplots.abline_plot.html#statsmodels.graphics.regressionplots.abline_plot[Ref]
| influence_plot(results)                   | Plot studentized resids vs leverage http://www.statsmodels.org/stable/generated/statsmodels.graphics.regressionplots.influence_plot.html#statsmodels.graphics.regressionplots.influence_plot[Ref]
| plot_acf(x)                               | Plot autocorrelation http://www.statsmodels.org/stable/generated/statsmodels.graphics.tsaplots.plot_acf.html#statsmodels.graphics.tsaplots.plot_acf[Ref]
| plot_pacf(x)                              | Plot partial autocorrelation http://www.statsmodels.org/stable/generated/statsmodels.graphics.tsaplots.plot_pacf.html#statsmodels.graphics.tsaplots.plot_pacf[Ref]
| mosaic(data)                              | Mosaic plot from contingency table http://www.statsmodels.org/stable/generated/statsmodels.graphics.mosaicplot.mosaic.html#statsmodels.graphics.mosaicplot.mosaic[Ref]
| robust_skewness(y)                        | Robust measures of general skew http://www.statsmodels.org/stable/generated/statsmodels.stats.stattools.robust_skewness.html#statsmodels.stats.stattools.robust_skewness[Ref]
| OLSInfluence(results)                     | Calculate outlier and influence measures for OLS result http://www.statsmodels.org/stable/generated/statsmodels.stats.outliers_influence.OLSInfluence.html#statsmodels.stats.outliers_influence.OLSInfluence[Ref]
| randmvn(rho)                              | Random draws from equi-correlated multivariate normal http://www.statsmodels.org/stable/generated/statsmodels.sandbox.stats.multicomp.randmvn.html#statsmodels.sandbox.stats.multicomp.randmvn[Ref]
| set_remove_subs(sets)                     | Remove sets that are subsets of others http://www.statsmodels.org/stable/generated/statsmodels.sandbox.stats.multicomp.set_remove_subs.html#statsmodels.sandbox.stats.multicomp.set_remove_subs[Ref]
| ci_low, ci_upp = proportion_confint(count, nobs)  | Confidence interval for binomal proportion http://www.statsmodels.org/stable/generated/statsmodels.stats.proportion.proportion_confint.html#statsmodels.stats.proportion.proportion_confint[Ref]s
| regressionplots.plot_leverage_resid2      |
|===

* http://www.statsmodels.org/dev/graphics.html[Statsmodels Graphics]
* http://www.statsmodels.org/stable/examples/notebooks/generated/regression_plots.html[Regression plots examples]
* ! http://www.statsmodels.org/devel/examples/notebooks/generated/regression_diagnostics.html[Statsmodels Diagnostics]s

=== Other statistical plots

[cols="m,d"]
|===
| scipy.stats.probplot(x, plot=plt)          | Probability quantile plot https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.probplot.html[Ref]
|===

== Plot correlation

    sns.heatmap(df.corr())

== Regression and Fitting

=== Linear Regression

==== Scikit-learn Linear Regression

    from sklearn import linear_model
    
    model=linear_model.LinearRegression()
    
    model.fit(np.reshape(x, (len(x),1)),y)
    print(model.intercept_, model.coef_)

==== Statsmodels Linear Regression

    import statsmodels.api as sm
    
    X = sm.add_constant(x)
    
    model = sm.OLS(y,X).fit()
    coef=model.params                       # intercept last
    model.summary()                          # print exhaustive summary of statistics
    
    y_pred = model.predict(X)

    coef_confs = res.conf_int()

Regression result variables can be found on http://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.RegressionResults.html[Ref].
The specific OLSResults adds `get_influence`, `outlier_test`, `el_test` (emp.likelihood test), `conf_int_el` http://www.statsmodels.org/dev/_modules/statsmodels/regression/linear_model.html#OLSResults[Ref].


==== Numpy Linear Regression

    X = np.vstack([x, np.ones(len(x))]).T
    
    coef, residuals, rank, singular_values = np.linalg.lstsq(X, y)

==== Numpy Linear Regression

    coef=np.polyfit(x,y,1)

* more than coef if options full=True, rcond=True, etc. https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html[Ref]

==== Scipy Linear Regression

    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x,y)

lingress(x) if Nx2 array

=== Splines

[cols="m,d"]
|===
| scipy.interpolate.UnivariateSpline(x, y)  | https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.UnivariateSpline.html#scipy.interpolate.UnivariateSpline[Ref]
|===


=== Robust regression

    X=sm.add_constant(X)
    
    model=sm.RLM(y, X, M=sm.robust.norms.HuberT())
    res=model.fit()
    coef=res.params

=== Orthogonal regression

https://docs.scipy.org/doc/scipy/reference/odr.html[Scipy Orthogonal regression]

    model=scipy.odr.Model(f)                # f(param, x)
    data=scipy.odr.Data(x_part, y_part)
    odr=scipy.odr.ODR(data, model)
    result=odr.run()
    result.pprint()

=== Smoothing

|===
| statsmodels.nonparametric.smoothers_lowess.lowess | http://www.statsmodels.org/devel/generated/statsmodels.nonparametric.smoothers_lowess.lowess.html[Ref]
| scipy.stats.uniform(-1, 2).rvs(size=..)   | Sample from distribution https://docs.scipy.org/doc/scipy/reference/stats.html[More functions]
|===

=== General curve fitting

    def func(xdata, *params):
        ...

    param_opt, param_cov = scipy.optimize.curve_fit(func, xdata, ydata)

F-test to compare models that are _nested_. Generally cross-validation also might work.

== Optimization

* https://github.com/lmfit/lmfit-py/[LmFit]: Higher level on scipy.optimize; Also Interactive fitting(?)

=== Non-derivative optimization

* https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fminbound.html[scipy.optimize.fminbound]
* https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.brent.html[scipy.optimize.brent]
* https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.golden.html[scipy.optimize.golden]

== Statistics

=== Find correlation

[col="m,d"]
|===
| numpy.corrcoef                            |
| sklearn.metrics.matthews_corrcoef         |
| np.linalg.conf(..)                        | Condition number to see multi-collinearity
|===

=== Scipy statistical functions

* Functions from `scipy.stats`
* Functions for masked arrays are in https://docs.scipy.org/doc/scipy/reference/stats.mstats.html[scipy.stats.mstats]

[col="m,d"]
|===
| tmean(a)                                  | Trimmed mean https://docs.scipy.org/doc/scipy/reference/generated/.tmean.html#.tmean[Ref]
| cumfreq(a, numbins=..)                    | Cumulative histogram https://docs.scipy.org/doc/scipy/reference/generated/.cumfreq.html#.cumfreq[Ref]
| percentileofscore(a, score)               | Percentile of a score relative to a list https://docs.scipy.org/doc/scipy/reference/generated/.percentileofscore.html#.percentileofscore[Ref]s
| r, p = pearsonr(x, y)                     | Pearson correlation https://docs.scipy.org/doc/scipy/reference/generated/.pearsonr.html#.pearsonr[Ref]
| c, p = spearmanr(a, b=None)               | Spearman rank-order correlation https://docs.scipy.org/doc/scipy/reference/generated/.spearmanr.html#.spearmanr[Ref]
| med_sl, med_intercep, lo_sl, up_sl = theilslopes(y, x=None)   | Robust linear regression estimators https://docs.scipy.org/doc/scipy/reference/generated/.theilslopes.html#.theilslopes[Ref]
| itemfreq(a)                               | 2D array of item frequencies https://docs.scipy.org/doc/scipy/reference/generated/.itemfreq.html#.itemfreq[Ref]
| relfreq(a, numbins=..)                    | Relative frequency histogram https://docs.scipy.org/doc/scipy/reference/generated/.relfreq.html#.relfreq[Ref]
| binned_statistics(x, values, statistic="mean")    | Compute binned statistic within each bin https://docs.scipy.org/doc/scipy/reference/generated/.binned_statistic.html#.binned_statistic[Ref] (also `_2d` and `_dd` versions)
| gmean(a)                                  | Geometric mean https://docs.scipy.org/doc/scipy/reference/generated/.gmean.html#.gmean[Ref]
| rankdata(a, method="average")             | Assign ranks, deal with ties https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rankdata.html#scipy.stats.rankdata[Ref]
| circmean(x, high=6.28..)                  | Circular mean https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.circmean.html#scipy.stats.circmean[Ref]
| ppcc_plot(x, a, b)                        | Probability plot correlation coef. Determine optimal shape parameter https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ppcc_plot.html#scipy.stats.ppcc_plot[Ref]
| probplot(x)                               | Calc/plot quantiles for probability plot https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.probplot.html#scipy.stats.probplot[Ref]
| gaussian_kde(data)                        | Gaussian kernel density https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html#scipy.stats.gaussian_kde[Ref]
| detrend(data)                             | https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.detrend.html#scipy.signal.detrend[Ref]
| scipy.special.entr(..)                    | Computes `-x*log(x)` (or Kullback) to calc entropy manually
|===

=== Scipy statistical tests

[col="m,d"]
|===
| binom_test(x, n=None, p=0.5)              | Exact test whether probability of experiment is p https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom_test.html#scipy.stats.binom_test[Ref]
| ttest_1samp(a, popmean)                   | One group t-test https://docs.scipy.org/doc/scipy/reference/generated/.ttest_1samp.html#.ttest_1samp[Ref]
| wilcoxon(x, y=None)                       | Whether x-y symmetric, i.e. same distribution. Like non-parametric paired t-test https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.wilcoxon.html#scipy.stats.wilcoxon[Ref]
| kruskal(..)                               | Whether a group medians equal. Like non-parametric ANOVA https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kruskal.html#scipy.stats.kruskal[Ref]
| normaltest(a)                             | Whether normal distribution https://docs.scipy.org/doc/scipy/reference/generated/.normaltest.html#.normaltest[Ref]
| shapiro(x)                                | Whether normal https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.shapiro.html#scipy.stats.shapiro[Ref]
| anderson(x, dist="norm")                  | Whether sample is from particular distribution https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.anderson.html#scipy.stats.anderson[Ref]
| kurtosistest(a)                           | Whether kurtosis that of normal https://docs.scipy.org/doc/scipy/reference/generated/.kurtosistest.html#.kurtosistest[Ref]
| skewtest(a)                               | Whether skew that of normal https://docs.scipy.org/doc/scipy/reference/generated/.skewtest.html#.skewtest[Ref]
| jarque_bera(x)                            | Whether skew and kurtosis normal https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.jarque_bera.html#scipy.stats.jarque_bera[Ref]
| friedmanchisquare(..)                     | Whether repeated measurements on same individuals have same distribution https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.friedmanchisquare.html#scipy.stats.friedmanchisquare[Ref]
| power_divergence(f_obs)                   | Cressie-Read test if categorical data jas given freqs https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.power_divergence.html#scipy.stats.power_divergence[Ref]
| combine_pvalues(pvalues)                  | Combine p-values of independent tests with same hypothesis https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.combine_pvalues.html#scipy.stats.combine_pvalues[Ref]
| ansari(x, y)                              | Non-parametric test for equality of scale parameters https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ansari.html#scipy.stats.ansari[Ref]
| mood(x, y)                                | Non-parametric two-sample test if two samples from same distribution with same scale https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mood.html#scipy.stats.mood[Ref]
| median_test(..)                           | Whether many samples from same median https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.median_test.html#scipy.stats.median_test[Ref]
| fisher_exact(table)                       | Exact test in 2x2 table https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.fisher_exact.html#scipy.stats.fisher_exact[Ref]
| chi2_contingency(obs)                     | Whether contigency table independent https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html#scipy.stats.chi2_contingency[Ref]
| mannwhitneyu(x, y)                        | Rank test https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html#scipy.stats.mannwhitneyu[Ref]
|===


== Mathematics

[col="m,d"]
|===
| np.linalg.cond(mat)                       | Condition number of matrix (different norms possible) https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.cond.html[Ref]
| np.arctan2(y, x)                          |
|===

=== Eigenvalues

[col="m,d"]
|===
| np.linalg.eigvals(a)                      | Eigenvalues https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eigvals.html[Ref]
| np.linalg.eig(a)                          | Eigenvalues and right eigenvectors https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eig.html#numpy.linalg.eig[Ref]
| np.linalg.eigvalsh(a)                     | Eigenvalues of Hermitian/real symmetric matrix https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eigvalsh.html#numpy.linalg.eigvalsh[Ref]
| np.linalg.eigh(a)                         | Eigenvalues and Eigenvectors of Hermitian/real symmatrix matrix https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eigh.html#numpy.linalg.eigh[Ref]
|===

* https://docs.scipy.org/doc/scipy/reference/tutorial/arpack.html[ARPACK] to find a few eigenvalues/-vectors from large sparse matricess


=== Polynomials

* https://docs.scipy.org/doc/numpy/reference/generated/numpy.poly1d.html[numpy.poly1d]
* https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html[Numpy Polyfit]

[col="m,d"]
|===
| coef = np.polyfit(x, y)                   | Fit polynomial
| np.polyval(coef, x)                       | Calculate polynomial (coef from highest to lowest) https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyval.html[Ref]
|===

=== Approximate fraction

    fractions.Fraction(x).limit_denominator(denom)


== Setting Import/Project path

    from pathlib import Path
    import os, sys
    
    project_dir=Path.home().joinpath("Projects/Name")               # Python 3.5
    sys.path.append(str(project_dir.joinpath("scripts/lib")))       # add to PYTHONPATH

== Cluster heatmap with block grouping

    from scipy.cluster.hierarchy import linkage
    link=linkage(dd, metric="cosine")   # or other param? method="centroid"??
    sns.clustermap(dd, row_linkage=link, col_linkage=link, cmap="magma_r")

== Scikit-learn tools

[col="m,d"]
|===
| utils.check_X_y(X, y)                     | Make sense-checks on data (shape, NaNs, ...) http://scikit-learn.org/stable/modules/generated/sklearn.utils.check_X_y.html[Ref]
| preprocessing.robust_scale(X)             | Center to median and scale by IQR http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.robust_scale.html[Ref]
| model_selection.permutation_test_score(clf, X, y) | Eval significance of CV score with permutations http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.permutation_test_score.html[Ref]
| model_selection.TimeSeriesSplit(n_splits).fit(..) | Make splits such that test indices are always higher than train http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html[Ref]
| isotonic.check_increasing(x, y)           | Check whether monotonic http://scikit-learn.org/stable/modules/generated/sklearn.isotonic.check_increasing.html[Ref]
| feature_selection.mutual_info_regression(X, y)    | Estimate mutual information (with nearest neighbours) to _continuous_ `y` http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html[Ref]
| feature_selection.mutual_info_classif(X, y)       | Estimate mutual information (with nearest neighbours) to _discrete_ `y` http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html[Ref]
|===

=== Evaluation

[col="m,d"]
|===
| model_selection.validation_curve(clf, X, y)   | Scores vs parameters http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html[Ref]s
| model_selection.learning_curve(clf, X, y) | Learning curve, Performance vs Train size http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html[Ref]
| ensemble.partial_dependence.plot_partial_dependence(gbrt, X, ...) | Partial dependence plot http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.partial_dependence.plot_partial_dependence.html[Ref]
| ensemble.partial_dependence.partial_dependence(gbrt, target)      | Partial dependence http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.partial_dependence.partial_dependence.html[Ref]
|===

== Color

* https://matplotlib.org/users/colormaps.html[Matplotlib color maps]
* https://matplotlib.org/examples/color/colormaps_reference.html[Matplotlib color map examples]
* https://bokeh.github.io/colorcet/[Bokeh colorcet]
* http://colorbrewer2.org/#type=qualitative&scheme=Paired&n=8[Colorbrewer]
* https://jiffyclub.github.io/palettable/[Palettable]
* https://bokeh.pydata.org/en/latest/docs/reference/palettes.html[Bokeh palettes]
* suggested qualitative color palettes:
** https://matplotlib.org/examples/color/colormaps_reference.html[Set1]
** https://bokeh.pydata.org/en/latest/docs/reference/palettes.html[Category10]
** https://jiffyclub.github.io/palettable/cartocolors/qualitative/#bold_10[Palettable Cartocolors Bold10]

=== Print Matplotlib colormap names

    for name, pal in sns.cm.mpl_cm.cmap_d.items():
        if name.endswith("_r") or name in ["jet", "spectral"] or name.startswith("Vega"):
            continue
        try:
            sns.palplot(sns.color_palette(name, 10), 0.6)
            plt.title(name)
            plt.show()
        except Exception as e:
            print(f"Failed {name} due to {e}")


== Data Sources

[col="m,d"]
|===
| img = scipy.misc.face(gray=False)         | Image of a racoon
| np.random.randn(100, 10)                  | Standard normal matrix
| np.random.normal(loc=0, scale=1, size=(100,10))   | Non-standard normal matrix
| columns=list(string.ascii_lowercase[:10]) | Quick alphabetic column names
| sm.datasets.get_rdataset(name)            | Download and return R dataset http://www.statsmodels.org/dev/datasets/statsmodels.datasets.get_rdataset.html#statsmodels.datasets.get_rdataset[Ref]
| patsy.demo_data(*"pqr")                   | Generated data. "a-m" categorical, "p-z" numerical http://patsy.readthedocs.io/en/latest/API-reference.html#patsy.demo_data[Ref]
| imblearn.datasets.fetch_datasets()[".."]  | http://contrib.scikit-learn.org/imbalanced-learn/stable/datasets/index.html[Ref]
| imblearn.datasets.make_imbalance(X, y, ratio) | Make data imbalanced
| sklearn.datasets.load_*() +               
| sklearn.datasets.fetch_*()                | Small sample of standard datasets http://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets[Ref]
| sklearn.datasets.load_sample_image(name)  | Load "china.jpg" or "flower.jpg" http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_sample_image.html#sklearn.datasets.load_sample_image[Ref]
| sklearn.datasets.mldata_filename(name)    | Convert data name to mldata.org filename http://scikit-learn.org/stable/modules/generated/sklearn.datasets.mldata_filename.html#sklearn.datasets.mldata_filename[Ref]
| sklearn.datasets.make_*(..)               | Create artifical data http://scikit-learn.org/stable/modules/classes.html#samples-generator[Ref]
| pandas_datareader.*                       | Different sources of financial data https://pandas-datareader.readthedocs.io/en/latest/remote_data.html[Ref]
| matplotlib.cbook.get_sample_data(..)      | Matplotlib sample data https://matplotlib.org/api/cbook_api.html#matplotlib.cbook.get_sample_data[Ref]
| bokeh.sampledata.download()               | Bokeh sample data https://bokeh.pydata.org/en/latest/docs/reference/command/subcommands/sampledata.html[Ref]
| quandl                                    | Financial data https://www.quandl.com/tools/python[Ref]
| openml                                    | General data https://openml.github.io/openml-python/stable/usage.html[Ref]
| datadotworld                              | Data https://github.com/datadotworld/data.world-py[Ref]
| data retriever                            | Data https://retriever.readthedocs.io/en/latest/datasets.html[Ref]
| skdata                                    | Data https://github.com/jaberg/skdata/wiki/Data-Set-Modules[Ref]
| PySAL sample data                         | http://pysal.readthedocs.io/en/latest/users/tutorials/examples.html[Ref]
| PyDataset                                 | Data https://github.com/iamaziz/PyDataset[Ref]
| Yellowbrick sample data                   | Samples from UCI http://www.scikit-yb.org/en/latest/api/datasets.html[Ref]
|===

== Data Operations

=== Reduce multiple conditions

    functools.reduce(lambda x,y:x & y, [..])
    
=== Binning

   [..] = np.digitize(x, bins=[..])            # bins[i-1] <= x < bins[i]; 0 or len(x) if beyond bounds
   
   hist, bin_edges = np.histogram(x, bins)
   # bins = "auto"    -> good allround for plotting
   # weights = [..]
   # density = True   -> integral over range is 1
   
   np.histogramdd(..)   # Multidimensional
   
=== Search in list

   np.searchsorted(..)

=== Bisecting

* bisect `left` vs. `right` matters only for _exact_ boundary hits
* `bisect` same as `bisect_right`

    i = bisect(a, x)
    if i > 0:
        print("Next value smaller or equal:", a[i-1])
    else:
        print("Smaller than all values")

    i = bisect.bisect_left(a, x)
    if i < len(a):
        print("Next value larger or equal:", a[i])
    else:
        print("Larger than all values")

=== Merge dicts

    Merge dicts:
    d={}
    d.update(..)
    d.update(..)

    dict(chain(d1.items(), d2.items()))

    ChainMap({}, d2, d1)   # {} so that dont modify old

    d={**d1, **d2}  # in Python 3.5

(Wrong:
* dict(d1, **d2)   # needs to be keyword-like
* dict(d1.items() | d2.items())   # unpredicatable order
)


== Pandas

http://pandas.pydata.org/pandas-docs/stable/cookbook.html

=== Various

|===
| df.dropna(how="all")                      | Drop only when all columns NaNs
| pd.to_numeric(.., errors="coerce")        | Parse invalid to NaN
| idx.to_series()                           | Create series with NewIdx=NewVals=Idx
| s.to_frame()                              | Convert Series to DataFrame
|===

=== Selecting columns
   
    df.filter(like="_min", axis=1)    # also regex possible

=== Pipelines

    (df
     .assign(newcol=series)
     .assign(newcol=lambda dffunc:...)
     .pipe(df_func, arg1=.., ..)
     .pipe((df_func, "df_argname"), arg1=.., ..)
     .rename(index=.., columns=..)               # scalar/list for series; dict/func for map
     .rename_axis(.., axis=0)                    # scalar/list for Index.name/MultiIndex.names; dict/func for labels
     .where(cond, other=nan)                     # select self if cond and otherwise from other; return same shape
     .mask(cond, other=nan)                      # replace by other where cond is true; return same shape
     .query(expr)
    )
    
=== Sort within part of group

    dfgr_sort = dfgr["var"].groupby(level=0, group_keys=False).apply(lambda x: x.order())
    
=== Find all correlates

	df.corr().unstack().sort_values(ascending=False)
	
But also includes self-pairs.

=== Sort then take first for groups

    df.sort_values(..).groupby(.., as_index=False).first()
    
=== Groupby like itertools

    df.A.groupby((df.A != df.A.shift())

=== Heatmap

   dp = df.apply(lambda x: Series(np.histogram(x, bins=bins)[0], index=bins[:-1]))
   plt.pcolor(dp)
   
=== Groupby and index

   catidxs, bins = pd.cut(df, bins=.., retbins=True, labels=False, include_lowest=True)
   df.groupby(bins[catidxs])...

   pd.cut(s).apply(attrgetter("left"))
   
http://stackoverflow.com/questions/17050202/plot-timeseries-of-histograms-in-python
    
=== Mask values

    df["newcol"] = np.where(cond, then_val, else_val)
    df.where(df_mask, new_val)
       
=== Window functions

    s.resample("D").max().rolling(window=5).max()
    s.rolling("5D").max()
    
    s.rolling(..).apply(func)
                       
    def flatindex(df):
        df.columns=df.columns.map("_".join)
        return df
                       
    df1=(df
        .set_index("time")
        .groupby("cat")
        [cols]
        .apply(lambda d:d
               .sort_index()
               .rolling("7d")     # http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases
               .agg(["min", "max"])
               .pipe(flatindex)
              )
        )

=== Other

    df.isnull().any(axis=0) to check for columns with missing values

https://pandas.pydata.org/pandas-docs/stable/cookbook.html[Pandas Cookbook] examples from StackOverflow.

    # Select values which are closest
    closest_idxs = (df.A-val).abs().argsort()

    # Combine conditions
    tot_cond = functools.reduce(operator.and_, [cond1, cond2, ..])

    # itertools.groupby-like
    df.A.groupby( (df.A != df.A.shift()) .cumsum()).groups    
    
== Spark

=== Window operations

http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Window

    from pyspark.sql.window import Window

	window=(Window
	        .orderBy(*cols)
	        .partitionBy(*cols)
	        .rowsBetween(start, end)
	        .rangeBetween(start, end)  # by value?
	       )
	
    df.withColumn("newcol", F.sum("col").over(window))
    
Special values for range: `Window.unboundedPreceding`, `Window.unboundedFollowing`, `Window.currentRow`.

Previous customer value

    window=Window.partitionBy("customer").orderBy("date")
    df.withColumn("lastval", F.lag("col", 1).over(window))

=== User defined functions

    from pyspark.sql.functions import udf, col
    from pyspark.sql.types import StringType
    
    udf_func=udf(my_func, StringType())
    df1=df.withColumn(udf_func(col("col")))
    
=== Cumulative sum

    from pyspark.sql.window import Window
    
    df.withColumn("cumul", F.sum("vals").over(Window.orderBy(F.desc("col_sort))))
    
=== Top examples in groups

    from pyspark.sql.window import Window

    (df
     .groupby("part_col", "group_col").sum("order_vals")
                                      .withColumnRenamed("sum(order_vals)", "order_col")
     .withColumn("rank", F.rank().over(Window.partitionBy("part_col")
                                             .orderBy("order_col")))
    )
    
  
=== Multiple values into UDF

    udf(..)(F.struct(df[..], df[..]))
    

== Scikit-learn

=== Basic usage

    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import auc, roc_curve
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
    
    clf=RandomForestClassifier(n_estimators=300)
    
    clf.fit(X_train, y_train)
    
    y_pred=clf.predict(X_test)
    
    fpr, tpr, thresh = roc_curve(y_test, y_pred)
    auc(fpr, tpr)
	
=== Pipeline

    p=sklearn.pipeline.Pipeline([("trans1", t1), ("trans2", t2), ("pred", pred)])
    p.get_params("pred__C")
    
    sklearn.pipeline.FeatureUnion([("pred1", p1), ("pred2", p2)], transformer_weights=[w1, w2], n_jobs=1)
    
    make_pipeline([t1, t2, pred])  # make pipeline, names automatic
    make_union([p1, p2]) # make union, names automatic
	
=== Write tree to PDF

    from sklearn.externals.six import StringIO  
    import pydot 
    dot_data = StringIO.StringIO() 
    tree.export_graphviz(clf, out_file=dot_data) 
    graph = pydot.graph_from_dot_data(dot_data.getvalue()) 
    graph.write_pdf("iris.pdf") 
    
== Seaborn

=== Heatmap

    sns.jointplot("var1", "var2", data=df, kind="hex", joint_kws=dict(gridsize=40))

    df.plot.hexbin("x", "y", bins="log", mincnt=1, cmap="viridis")

=== Colors

Color points in any scatter plot

    sns.regplot(..., scatter_kws=dict(color=mpl.cm.viridis(mpl.colors.Normalize()(vals))))

For example with `dtidx.astype(int)` to map time.

== Bokeh

=== Basic usage

   from bokeh.plotting import figure, output_notebook, show
   p=figure(title="..",
            plot_width=300, plot_height=300,
            x_axis_label="..",
            x_range=(a,b),
            )
   
   p.line(x=.., y=.., legend="..")
   
   output_notebook()
   show(p)
   # save(p)

=== Custom Ticker format

    def ticker():   # no argument, since `tick` will be available
            return "{}".format(tick)
            
    p.xaxis.formatter = FuncTickFormatter.from_py_func(ticker)


    p.xaxis.from_coffeescript(
    """
    function (tick) {
        return ...
    };
    """)

=== Hover tool

    source=ColumnDataSource(data=dict(x=[..],..))

    hover=HoverTool(tooltips=[("(x,y)", "(@x, @y)"), # $ for special values
                              ("desc", "@desc"),     # @ for variables
                             ],
                    attachment="vertical",
                    line_policy="nearest",
                    )
    p = figure(tools=[hover])
    p.circle("x", "y", source=source)
    
Tooltips can also be a HTML string. Special variables like "$color[hex]:fill_color"

* Shift for multi-select
* Also touch screen support (pinch, tap, etc.)


    tooltips=[('Col1:', '@col1'), ('Col2:', '@col2')]
	p = Scatter(.., tooltips=tooltips) 


=== Twin axis

	p.extra_y_ranges={"abc":Range1d(..,..)}
	p.circle(.., y_range_name="abc")
	p.add_layout(LinearAxis(y_range_name="abc"), "left")
	
=== Matplotlib compatibility
	
	from bokeh import mpl
	sns.violinplot(...)
	show(mpl.to_bokeh())
	
Relies on `mplexporter`. Will be better when Matplotlib adopts native JSON.
	
   
== Matplotlib

=== Annotate Bar plot

    plt.bar(X, +Y1)
    for x,y in zip(X,Y1):
        plt.text(x+0.4, y+0.05, '%.2f' % y, ha='center', va= 'bottom')

=== Make labels in a box

    for label in ax.get_xticklabels() + ax.get_yticklabels():
        label.set_fontsize(16)
        label.set_bbox(dict(facecolor='white', edgecolor='None', alpha=0.65 ))

=== Set color of bar in bar-plot

	barlist=plt.bar([1,2,3,4], [1,2,3,4])
	barlist[0].set_color('r')
	
=== Set data position and format    
    
	ax.xaxis_date()
	ax.xaxis.set_major_locator(mpl.dates.MonthLocator())
	ax.xaxis.set_major_formatter(mpl.dates.DateFormatter('%Y-%m-%d'))
	
=== Clip color in colorbar

                
	cmap.set_bad(color="..")
	
	cmap.set_under(color="..")
	plot(..., cmap=cmap, vmin=1e-10)
	
=== Hexbin with dates

	ax.set_aspect("equal")
	ax.hexbin(mpl.dates.date2num(df.a), mpl.dates.date2num(df.b), gridsize=20)
	ax.xaxis_date()
	ax.yaxis_date()
	ax.xaxis.major.formatter.scaled[1.0] = "%Y-%m-%d"
	ax.yaxis.major.formatter.scaled[1.0] = "%Y-%m-%d"
	
=== Increase Figure size

	zoom = 2
	w, h = fig.get_size_inches()
	fig.set_size_inches(w * zoom, h * zoom)
	
	dpi = fig.get_dpi()   # This will get dpi that is set matplotlibrc
	fig.savefig("test.jpg", dpi=dpi*2)
	
=== Print all color names

	for name, hex in matplotlib.colors.cnames.iteritems(): # print all color names
	    print(name, hex)

=== Fix midpoint of colormap

Midpoint of colormap (unless use vmin, vmax)

	from numpy import ma
	from  matplotlib import cbook
	
	class MidPointNorm(Normalize):    
	    def __init__(self, midpoint=0, vmin=None, vmax=None, clip=False):
	        Normalize.__init__(self,vmin, vmax, clip)
	        self.midpoint = midpoint
	
	    def __call__(self, value, clip=None):
	        if clip is None:
	            clip = self.clip
	
	        result, is_scalar = self.process_value(value)
	
	        self.autoscale_None(result)
	        vmin, vmax, midpoint = self.vmin, self.vmax, self.midpoint
	
	        if not (vmin < midpoint < vmax):
	            raise ValueError("midpoint must be between maxvalue and minvalue.")       
	        elif vmin = vmax:
	            result.fill(0) # Or should it be all masked? Or 0.5?
	        elif vmin > vmax:
	            raise ValueError("maxvalue must be bigger than minvalue")
	        else:
	            vmin = float(vmin)
	            vmax = float(vmax)
	            if clip:
	                mask = ma.getmask(result)
	                result = ma.array(np.clip(result.filled(vmax), vmin, vmax),
	                                  mask=mask)
	
	            # ma division is very slow; we can take a shortcut
	            resdat = result.data
	
	            #First scale to -1 to 1 range, than to from 0 to 1.
	            resdat -= midpoint            
	            resdat[resdat>0] /= abs(vmax - midpoint)            
	            resdat[resdat<0] /= abs(vmin - midpoint)
	
	            resdat /= 2.
	            resdat += 0.5
	            result = ma.array(resdat, mask=result.mask, copy=False)                
	
	        if is_scalar:
	            result = result[0]            
	        return result
	
	    def inverse(self, value):
	        if not self.scaled():
	            raise ValueError("Not invertible until scaled")
	        vmin, vmax, midpoint = self.vmin, self.vmax, self.midpoint
	
	        if mpl.cbook.iterable(value):
	            val = ma.asarray(value)
	            val = 2 * (val-0.5)  
	            val[val>0]  *= abs(vmax - midpoint)
	            val[val<0] *= abs(vmin - midpoint)
	            val += midpoint
	            return val
	        else:
	            val = 2 * (val - 0.5)
	            if val < 0: 
	                return  val*abs(vmin-midpoint) + midpoint
	            else:
	                return  val*abs(vmax-midpoint) + midpoint

=== Plot histogram

    width=2
    hist=np.histogram(data, bins=np.arange(min(data),max(data)+3,width))
    plt.bar(hist[1][:-1], hist[0]/sum(hist[0]), width=width*0.9)

=== Plot KDE density    

    density=scipy.stats.gaussian_kde(data)
    x=np.linspace(min(data),max(data),100)
    plt.plot(x,density(x)*width)

=== Midpoint colormap

Midpoint of colormap
(unless use vmin, vmax)

    from numpy import ma
    from  matplotlib import cbook

    class MidPointNorm(Normalize):    
        def __init__(self, midpoint=0, vmin=None, vmax=None, clip=False):
            Normalize.__init__(self,vmin, vmax, clip)
            self.midpoint = midpoint

        def __call__(self, value, clip=None):
            if clip is None:
                clip = self.clip

            result, is_scalar = self.process_value(value)

            self.autoscale_None(result)
            vmin, vmax, midpoint = self.vmin, self.vmax, self.midpoint

            if not (vmin < midpoint < vmax):
                raise ValueError("midpoint must be between maxvalue and minvalue.")       
            elif vmin == vmax:
                result.fill(0) # Or should it be all masked? Or 0.5?
            elif vmin > vmax:
                raise ValueError("maxvalue must be bigger than minvalue")
            else:
                vmin = float(vmin)
                vmax = float(vmax)
                if clip:
                    mask = ma.getmask(result)
                    result = ma.array(np.clip(result.filled(vmax), vmin, vmax),
                                      mask=mask)

                # ma division is very slow; we can take a shortcut
                resdat = result.data

                #First scale to -1 to 1 range, than to from 0 to 1.
                resdat -= midpoint            
                resdat[resdat>0] /= abs(vmax - midpoint)            
                resdat[resdat<0] /= abs(vmin - midpoint)

                resdat /= 2.
                resdat += 0.5
                result = ma.array(resdat, mask=result.mask, copy=False)                

            if is_scalar:
                result = result[0]            
            return result

        def inverse(self, value):
            if not self.scaled():
                raise ValueError("Not invertible until scaled")
            vmin, vmax, midpoint = self.vmin, self.vmax, self.midpoint

            if mpl.cbook.iterable(value):
                val = ma.asarray(value)
                val = 2 * (val-0.5)  
                val[val>0]  *= abs(vmax - midpoint)
                val[val<0] *= abs(vmin - midpoint)
                val += midpoint
                return val
            else:
                val = 2 * (val - 0.5)
                if val < 0: 
                    return  val*abs(vmin-midpoint) + midpoint
                else:
                    return  val*abs(vmax-midpoint) + midpoint

=== Add right axis with different scaling

    # use only ax. commands and not plt. command thereafter
    def right_axis(scale_func, ax):
        def convert_ax_callback(ax):
            y1, y2=ax.get_ylim()
            ax2.set_ylim(scale_func(y1), scale_func(y2))
            ax2.figure.canvas.draw()

        ax2=ax.twinx()
        ax.callbacks.connect("ylim_changed", convert_ax_callback)
        return ax2


=== Hexbin with dates

    ax.set_aspect("equal")
    ax.hexbin(mpl.dates.date2num(df.a), mpl.dates.date2num(df.b), gridsize=20)
    ax.xaxis_date()
    ax.yaxis_date()
    ax.xaxis.major.formatter.scaled[1.0] = "%Y-%m-%d"
    ax.yaxis.major.formatter.scaled[1.0] = "%Y-%m-%d"

=== Plotting maps

    from mpl_toolkits.basemap import Basemap
    import matplotlib.cm as cm
    
    m = Basemap(projection='robin',lon_0=0,resolution='c')
    x, y = m(reg['longitude'],reg['latitude'])
    
    figure(figsize=(15,15))
    m.drawcoastlines(linewidth=0.25)
    m.drawcountries(linewidth=0.25)
    m.fillcontinents(color='coral',lake_color='aqua')
    m.drawmapboundary(fill_color='white')
    m.drawmeridians(np.arange(0,360,30))
    m.drawparallels(np.arange(-90,90,30))
    m.scatter(x,y,s=reg['Number']*3,c=reg['Number']/5,marker='o',zorder=4, cmap=cm.Paired,alpha=0.5)

=== ??? axis

    ax.set_frame_on(False)
    ax.set_yticks(np.arange(nba_sort.shape[0]) + 0.5, minor=False)
    ax.invert_yaxis()
    ax.xaxis.tick_top()
    ax.grid(False)
    for t in ax.xaxis.get_major_ticks():
        t.tick1On = False
        t.tick2On = False

=== Legend changes

For on-the-fly legend order changes, resort self.lines+self.patches+self.collections+self.containers (see matplotlib/axes/_axes.py: def _get_legend_handles(), line 221
(e.g. for barplot `ax._containers_=list(reversed(ax.containers))`)

    legend(_handles_=[mpl.patches.Rectangle((0, 0), 0, 0, _fc_="r", _label_="Rect"), +
    mpl.lines.Line2D((0,0),(0,0), _c_="b", _label_="Line")])    | http://matplotlib.org/1.3.1/users/legend_guide.html[Ref]

    legend([mpl.patches.Patch(_color_=...),...],[label1,...])

    legend([hbars.patches[0],...], [hbars.patches[0].get_label(),...]) # custom order

=== Scientific labels

    formatter = mpl.ticker.*ScalarFormatter*(_useMathText_=True) +
    formatter.*set_scientific*(True)  +
    formatter.*set_powerlimits*((-1,1)) +
    ax.yaxis.*set_major_formatter*(formatter)

=== Plot colormap

    pcolor(np.array([list(range(cmap.N))]), cmap=cmap)

== Logging

    import daiquiri
    daiquiri.setup(level=logging.INFO, outputs=[daiquiri.output.STDOUT])

    import logging
    logging.basicConfig()
    
    logg=logging.getLogger()
    logg.setLevel(logging.INFO)

    logging.basicConfig(stream=sys.stdout, level=logging.INFO)  # without stdout it would go to stderr
    logger=logging.getLogger(__name__)


=== File only logging

    import logging
    logger=logging.getLogger()
    logger.setLevel(logging.DEBUG)
    logger.addHandler(logging.FileHandler("out.log"))

=== Reset logging in Jupyter

    logging.shutdown()
    from imp import reload
    reload(logging)

== Date and Time

=== Alternative libraries

* https://arrow.readthedocs.io/en/latest/[Arrow] (quite popular)
* https://pendulum.eustace.io/docs/#introduction[Pendulum]
* http://delorean.readthedocs.io/en/latest/quickstart.html[Delorean]
* https://github.com/kennethreitz/maya[Maya](?)
* some https://pendulum.eustace.io/faq/[Some comparison]

=== Business days

    from pandas.tseries.offsets import CDay
    cdays=CDay(holidays=["2016-01-01"])
    cdays.rollforward(dt).to_pydatetime()
    
=== Month days

    weekday, days_in_month = calendar.monthrange(year, month) 
    
=== Business days

    busdaycal = np.busdaycalendar(holidays=[..])
    
    busday_diff = np.busday_cnt(start_date, end_date, busdaycal=busdaycal)
    
    end_date = np.busday_offset(start_date, busday_diff, "forward", busdaycal=busdaycal)
    
    from pandas.tseries.offsets import BMonthEnd
    offset=BMonthEnd()
    offset.rollforward(date).to_pydatetime()
    
    
=== Numpy datetime

    datetime = dt.datetime.utcfromtimestamp((numpydatetime - np.datetime64("1970-01-01T00:00:00Z"))/np.timedelta(1, "s"))
    
    datetime = pd.Timestamp(numpydatetime)   # subclass of dt.datetime


== Sympy

    from sympy import *
    init_printing()

    x, y, sigma = symbols("x y sigma")


[cols="m,d"]
|===
| simplify(..)                              | Simplify expression
| oo                                        | Infinity
| diff(expr, x)                             | Derivative
| integrate(expr, x)                        | Integrate
| plot(expr)                                | Plot with Matplotlib
| E                                         | Eulers number
| factor(..)                                | Find simple factor version of expanded polynomial
| cancel(../..)                             | Cancel factors in a fraction
| together(../.. + ../..)                   | Common denominator
| a.subs(x, y+1)                            | Substitute an expression
| symbols("..", positive=True)              | Helps simplifying square root etc. better
| expr1.rewrite(expr2)                      | Try to rewrite with other expression
| trigsimp(..)                              | Simplify trigonometrics
| expand_log(..)                            |
| expand_trig(..)                           |
| expand_*                                  |
| Symbol(.., real=True)                     | Assumptions about variables http://docs.sympy.org/latest/modules/core.html?highlight=assumptions#module-sympy.core.assumptions[Ref]
|===

* http://nbviewer.jupyter.org/url/www.inp.nsk.su/~grozin/python/sympy.ipynb[Notebook 1]

== Fourier transform

If your effective range of a discrete signal array is `[-a/2, a/2]` unlike what a discrete Fourier transform assumes `[0, a]`, you need to shift your data to get common results.
For example, to get a Gaussian from a (centered) Gaussian with a discrete transform:

    def four(y):
        return np.fft.fftshift(np.fft.fft(np.fft.fftshift(y)))

Without shifting, you get additional oscillations (due to the phase shift).


== Parse command line arguments

    parser=argparse.ArgumentParser()
    parser.add_argument("file_to_test")
    args=parser.parse_args()
    args.file_to_test


== Threaded CSV

    from time import sleep
    from csv import DictReader
    from Queue import Queue
    from threading import Thread

    q = Queue()
    workers = []

    def worker():
        while True:
            line = q.get()
            print "processing: %s\n\n" % line
            q.task_done()

    for i in range(10):
        t = Thread(target=worker)
        t.setDaemon(True)
        workers.append(t)
        t.start()

    with open('myfile.csv','r') as fin:
        for line in DictReader(fin):
            q.put(line)

    q.join()

* also check out https://pypi.python.org/pypi/xfork[xfork]

== Parallel execution

* See my IPython Ref for `ipyparallel`
* Cython tricks https://homes.cs.washington.edu/~jmschr/lectures/Parallel_Processing_in_Python.html[Link]
* `joblib` possible
* `dask.pipeline.Pipeline` (like sklearn, but parallelizable)

=== IPyParallel

To install

    conda install ipyparallel
    ipcluster nbextension enable       # for Jupyter extension tab

* single/multiple program; multiple data; task farming
* components:
** Engine: extended kernel which connect to controller; listen to requests over network, run code, return result
** Controller: Hub + Schedulers; SPOC to access engines; Engine and Clients connect; Direct interface of LoadBalanced interface
** Hub: Center of cluster; track engine, connections, schedulers, clients, task requests/results; facilitate query of cluster state
** Scheduler: all actions on engines go through here; Async (whereas engines block)
* `Client` to connect to cluster
* `View` for each execution model
* ZeroMQ communication (current no good https://ipyparallel.readthedocs.io/en/latest/intro.html#security[Security], https://ipyparallel.readthedocs.io/en/latest/security.html[Security])
* IPython on https://ipyparallel.readthedocs.io/en/latest/process.html#ipython-on-ec2-with-starcluster[Amazon EC2]
* Hub https://ipyparallel.readthedocs.io/en/latest/process.html#database-backend[stores] all messages and results passed
* https://ipyparallel.readthedocs.io/en/latest/magics.html[IPython Magics] automatically available when you create `Client`
* https://ipyparallel.readthedocs.io/en/latest/dag_dependencies.html[DAG Dependencies]
* careful not to modify passed Numpy array until all jobs are done
* everything pickled; only bytes, np.array, memoryviews send as raw

==== Getting started

* https://ipyparallel.readthedocs.io/en/latest/intro.html#getting-started[Getting Started]
* https://ipyparallel.readthedocs.io/en/latest/process.html#parallel-process[Start] controller and some engines
* automated with `ipcluster`; manual with `ipcontroller` and `ipengine`
* https://ipyparallel.readthedocs.io/en/latest/details.html[Details]
* load balanced view is better, since direct view has fixed distribution of tasks irrespective of runtime (when using `.map`)

    !ipcluster start -n 4 --debug

    import ipyparallel as ipp
    par = ipp.Client()
    view = par.load_balanced_view()

    data = range(20)

    ##### If imports needed
    with view.sync_imports():
        import numpy

    ##### Map version
    @view.parallel()
    def square(x):                  
        return x**2

    res = square.map(data)      # Split into num_engines and started only *once*

    ##### External functions
    @view.parallel()
    @ipp.require(func1, func2, "numpy", ...)
    def func(x):
        ... func1 ...

    ##### Partitions with progress
    res=[]
    part_len=3
    for data_part in cytoolz.partition_all(part_len, tqdm(data)):
        res.extend(square.map(data_part))

    ##### Function call version
    @view.parallel()
    def square(xs):                 # takes *multiple* (due to function-call-version)
        res=[]
        for x in xs:
            res.append(x**2)
        return res

    res = square(data)              # Split into num_engines and started once
    # print(res) : <AsyncMapResult: square>

    ##### See result
    if res.ready():
        # print(res) : <AsyncMapResult: square:finished>
        print(res.get())

    ##### Individual addressing
    res = par[0].apply(func, *args, **kwargs)

==== Commands

[cols="m,d"]
|===
| par.ids                                   | Show indices of engines to check
| view = par[:]                             | DirectView on all engines
| view.apply_sync(func)                     |
| async_res = view.apply(func, *args, **kwargs)     | `AsyncResult` (default non-blocking) https://ipyparallel.readthedocs.io/en/latest/direct.html#apply[Guide]
| if async_res.ready(): +
      res = async_res.get()                 | https://ipyparallel.readthedocs.io/en/latest/asyncresult.html#parallel-asyncresult[Guide]
| res = async_res.get(_max_secs_)           | Ask for result waiting max some seconds
| view.map(func, data)                      |
| view["a"]=..                              | Send data
| a = views["a"]                            | Receive data
| view.execute("<code>")                    |
| view.wait([asr1, asr2, ..])               | Wait for all async results
| view.apply_sync(..)                       | Blocking
| view.push({"a":.., ..})                   | Push data
| view.pull("a")                            | Pull data
| view.update                               |
| view.get                                  |
| view.scatter("a", [..])                   | Scatter data to engines as variable `a` (list is partitioned equally)
| view.gather("a").get()                    | Gather data to client
| @view.remote() +
  def func(..)                              | Decorate function
| @view.parallel() +
  def func(..)                              | Scatter data and distribute
| par_func.map([..])                        | Parallel `@v.parallel` function
| view.scatter("x", [..]) +
  %px y=[f(x_) for x_ in x] +
  y = view.gather("y")                      | Parallel execution of a single command
| %pxresult                                 | Get result (of last line?)
| %pxconfig --noblock                       | Make non-blocking
| %%px ..                                   | Run this same cell on all (or specified) engines https://ipyparallel.readthedocs.io/en/latest/magics.html#px-cell-magic[Guide]
| %autopx                                   | Run all following cells on all engines (until `%autopx` again)
| with view.sync_imports():
      import numpy                          | Import on all
| @ipp.require("re")
  def func(..)                              | Require imports
| view.activate([suffix])                   | Link activate magics to this view (suffix to create multiple magics)
| ipp.bind_kernel()                         | Bind to engine kernel for debugging
| %px %qtconsole                            | Start consoles for all if local
| view = parcl.load_balanced_view()         | Default `LoadBalancedView`
| ar.get_dict()                             | Engine ID as key
| for x in ar                               | Iterate of async results as they come in
|===

* https://ipyparallel.readthedocs.io/en/latest/api/ipyparallel.html[API]
* add `_sync` to make calls blocking
* `AsyncResult` is superset of `multiprocessing.pool.AsyncResult`
* usually imports in functions or by `view.execute("import ..")`
* most commands accept parameters `block` and `targets` (engine ids)
* `scatter` and `gather` between engines and client; for inter-engine communication need another system (e.g. https://ipyparallel.readthedocs.io/en/latest/mpi.html[MPI])
* `sync_imports()` does not work with renaming `import A as B`; use `B = A` instead
* `%debug` after `CompositeError` works
* plots will also be collected by `%px`
* Magics linked to a particular `View`; change with `view2.activate()`; can
* AsyncResults has more https://ipyparallel.readthedocs.io/en/latest/asyncresult.html#metadata[meta info] (time since submission, ...)
* Tasks are stored in a https://ipyparallel.readthedocs.io/en/latest/db.html[database]

==== Dependency handling

* https://ipyparallel.readthedocs.io/en/latest/task.html#dependencies[Guide Dependency]
* Functional dependencies, Graph dependencies
* Retries possible
* Different schedulers possible


    
== SQL in Python

    from sqlalchemy import *
    eng=create_engine("mysql+pymysql://root:password@localhost")
	eng=create_engine("sqlite:///:memory:")
	conn=eng.connect()
	
	from sqlalchemy import create_engine
	engine = create_engine('sqlite:///:memory:', echo=True)
	conn = engine.connect()
	conn.execute(...)
		
	import pymysql
	conn=pymysql.connect(host="localhost", user="root", passwd="passwd")
	cur=conn.cursor()
	cur.execute("select * from db.tbl")
	fields=list(map(itemgetter(0),cur.description))
	for row in cur.fetchall():
	    ...
	conn.commit() # after writing
	conn.close()

    conn = sqlite3.connect('example.db')
    result=conn.execute("...")
    conn.commit()
    conn.close()

    conn.executemany("...?..", [(..,..), (.., ..), ..])

    sqlite3.connect(':memory:', detect_types=sqlite3.PARSE_DECLTYPES)

    conn.execute("begin")   # for transaction
    ...
    conn.execute("commit")


SQLAlchemy: removes column names parts before a dot due to bugs in the SQLite driver as a workaround use

    res = conn.execution_options(sqlite_raw_colnames=True).execute("select * from test")
    
or

    eng = create_engine("sqlite://", execution_options={"sqlite_raw_colnames": True})

    http://docs.python.org/library/sqlite3.html#sqlite3.Connection.iterdump to dump to memory

    import sqlite3
    from StringIO import StringIO
    def init_sqlite_db(app):
        # Read database to tempfile
        con = sqlite3.connect(app.config['SQLITE_DATABASE'])
        tempfile = StringIO()
        for line in con.iterdump():
            tempfile.write('%s\n' % line)
        con.close()
        tempfile.seek(0)

        # Create a database in memory and import from tempfile
        app.sqlite = sqlite3.connect(":memory:")
        app.sqlite.cursor().executescript(tempfile.read())
        app.sqlite.commit()
        app.sqlite.row_factory = sqlite3.Row

    >>> from sqlalchemy import create_engine
    >>> engine = create_engine('sqlite:///:memory:', echo=True)

== Run system command

Starting with Python 3.5 https://docs.python.org/3/library/subprocess.html#subprocess.run[subprocess.run] (`encoding=` from Python 3.6)

    import subprocess
    proc_res = subprocess.run("ls", stdout=subprocess.PIPE, encoding="utf8")
    out = proc_res.stdout

Return type https://docs.python.org/3/library/subprocess.html#subprocess.CompletedProcess[subprocess.CompletedProcess]


== Debugging

    import code; code.interact(local=locals())
    import pdb; pdb.set_trace()
    from IPython.core.debugger import Tracer; Tracer()()

* !check whether still up-to-date
* also consider new `breakpoint()` command in Python

    exc_type, exc_value, exc_traceback = sys.exc_info()
    filename, line number, function name, text=traceback.extract_tb(exc_traceback)[0] # 0 to get first element

== Terminal color

    # https://github.com/dslackw/colored
    from colored imoprt fg, attr
    print("{}Hello{}".format(fg.light_coral, attr(0))

== Hierarchical modelling

http://nbviewer.jupyter.org/github/fonnesbeck/multilevel_modeling/blob/master/multilevel_modeling.ipynb

    with Model() as varying_intercept:
        # Priors
        mu_a = Normal('mu_a', mu=0., tau=0.0001)
        sigma_a = Uniform('sigma_a', lower=0, upper=100)
        tau_a = sigma_a**-2
        
        # Random intercepts
        a = Normal('a', mu=mu_a, tau=tau_a, shape=len(set(county)))
        # Common slope
        b = Normal('b', mu=0., tau=0.0001)
        
        # Model error
        sigma_y = Uniform('sigma_y', lower=0, upper=100)
        tau_y = sigma_y**-2
        
        # Expected value
        y_hat = a[county] + b * floor_measure
        
        # Data likelihood
        y_like = Normal('y_like', mu=y_hat, tau=tau_y, observed=log_radon)

    with varying_intercept:
        step = NUTS()
        varying_intercept_samples = sample(2000, step)

    from pymc3 import forestplot, traceplot, summary

    plt.figure(figsize=(6,10))
    forestplot(varying_intercept_samples, vars=['a'])

    traceplot(varying_intercept_samples[-1000:], vars=['sigma_a', 'b'])

    summary(varying_intercept_samples[-1000:], vars=['b'])

Or varying different:

    with Model() as varying_slope:
        # Priors
        mu_b = Normal('mu_b', mu=0., tau=0.0001)
        sigma_b = Uniform('sigma_b', lower=0, upper=100)
        tau_b = sigma_b**-2
        
        # Model intercept
        a = Normal('a', mu=0., tau=0.0001)
        # Random slopes
        b = Normal('b', mu=mu_b, tau=tau_b, shape=len(set(county)))
        ...
        # Expected value
        y_hat = a + b[county] * floor_measure
        ...


    with Model() as varying_intercept_slope:
        # Priors    
        mu_a = Normal('mu_a', mu=0., tau=0.0001)
        sigma_a = Uniform('sigma_a', lower=0, upper=100)
        tau_a = sigma_a**-2
        
        mu_b = Normal('mu_b', mu=0., tau=0.0001)
        sigma_b = Uniform('sigma_b', lower=0, upper=100)
        tau_b = sigma_b**-2
        
        # Random intercepts
        a = Normal('a', mu=mu_a, tau=tau_a, shape=len(set(county)))
        # Random slopes
        b = Normal('b', mu=mu_b, tau=tau_b, shape=len(set(county)))
        ...
        # Expected value
        y_hat = a[county] + b[county] * floor_measure
        ...

Or making coefficients be a model of a new variable:

    with Model() as hierarchical_intercept:
        ...
        # County uranium model for slope
        gamma_0 = Normal('gamma_0', mu=0., tau=0.0001)
        gamma_1 = Normal('gamma_1', mu=0., tau=0.0001)
        
        # Uranium model for intercept
        mu_a = gamma_0 + gamma_1*u
        # County variation not explained by uranium
        eps_a = Normal('eps_a', mu=0, tau=tau_a, shape=len(set(county)))
        a = Deterministic('a', mu_a + eps_a[county])

        # Random slope
        b = Normal('b', mu=0, tau=0.001)
        ...
        # Expected value
        y_hat = a + b * floor_measure
        ...

For contextual effects/correlations among levels (correlation between individual-level variables and group residuals) include group average (here: for intercept)

    with Model() as contextual_effect:        
        ...
        # County uranium model for slope
        gamma_0 = Normal('gamma_0', mu=0., tau=0.0001)
        gamma_1 = Normal('gamma_1', mu=0., tau=0.0001)
        gamma_2 = Normal('gamma_2', mu=0., tau=0.0001)
        
        # Uranium model for intercept
        mu_a = gamma_0 + gamma_1*u + gamma_2*xbar[county]
        
        # County variation not explained by uranium
        eps_a = Normal('eps_a', mu=0, tau=tau_a, shape=len(set(county)))
        a = Deterministic('a', mu_a + eps_a[county])

        # Random slope
        b = Normal('b', mu=0, tau=0.001)
        ...
        # Expected value
        y_hat = a + b * floor_measure
        ...

Errors from cross-validation:

* unpooled 0.86
* pooled 0.84
* multilevel 0.79

Can make predictions for old or a completely new group.

== Write Zipped Files

    with gzip.open("test.gz", "wt", newline="") as file:
        writer = csv.writer(file)

== Monkey Patch

    def override(p, methods):
        oldType = type(p)
        newType = type(oldType.__name__ + "_Override", (oldType,), methods)
        p.__class__ = newType

    class Test(object):
        def __str__(self):
            return "Test"

    def p(self):
        print(str(self))

    def monkey(x):
        override(x, {"__del__": p})

    a=Test()
    b=Test()
    monkey(a)
    print "Deleting a:"
    del a
    print "Deleting b:"
    del b

== System information

    print os.environ.get( "USERNAME" )
    win32api.GetUserName()
    win32api.GetUserNameEx (win32con.NameSamCompatible)
    getpass.getuser()

== Selenium screengrabber

    from selenium import webdriver
    browser=webdriver.Firefox()
    browser.get(url)
    browser.save_screenshot("file.png")
    elems=browser.find_elements_by_xpath(xpath)
    links=[e.get_attribute("href") for e in elems]
    browser.quit()
    e.get_attribute("innerHTML")  # if hidden HTML (not visible on page; greyed out in Firebug)

== Misc

You can use `scipy.signal.convolve` and an artificial `[1,1,1],[1,100,1],[1,1,1]` filter to do the "Game of Life" with arrays http://nbviewer.jupyter.org/gist/jiffyclub/3778422[Ref].

== Sympy

=== Integration

    from sympy import *
    init_printing()
    a, b=symbols("a b")
    simplify(integrate(a, (a, -oo, oo)))
