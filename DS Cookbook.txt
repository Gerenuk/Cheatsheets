////
* Sort alphabetically
////

= Data Science Cookbook

:toc: left

== Plotting

* https://github.com/bokeh/colorcet[Colorcet]: Perceptually uniform color maps

=== Statsmodels Statistical Plots

[cols="m,d"]
|===
| qqplot(data)                              | Quantile vs Quantile plot (default vs normal) http://www.statsmodels.org/stable/generated/statsmodels.graphics.gofplots.qqplot.html#statsmodels.graphics.gofplots.qqplot[Ref]
| _line_=45                                 |
| qqline(ax)                                | Plot reference line for qqplot http://www.statsmodels.org/stable/generated/statsmodels.graphics.gofplots.qqline.html#statsmodels.graphics.gofplots.qqline[Ref]
| qqplot_2samples(data1, data2)             | Quantile vs Quantile plot for two samples http://www.statsmodels.org/stable/generated/statsmodels.graphics.gofplots.qqplot_2samples.html#statsmodels.graphics.gofplots.qqplot_2samples[Ref]
| ProbPlot(data)                            | Convenience class for QQ/PP plots http://www.statsmodels.org/stable/generated/statsmodels.graphics.gofplots.ProbPlot.html#statsmodels.graphics.gofplots.ProbPlot[Ref]
| violinplot(data)                          | http://www.statsmodels.org/stable/generated/statsmodels.graphics.boxplots.violinplot.html#statsmodels.graphics.boxplots.violinplot[Ref]
| plot_corr(dcorr)                          | Plot correlation matrix http://www.statsmodels.org/stable/generated/statsmodels.graphics.correlation.plot_corr.html#statsmodels.graphics.correlation.plot_corr[Ref]
| plot_corr_grid(dcorrs)                    | Plot grid of correlation matrices http://www.statsmodels.org/stable/generated/statsmodels.graphics.correlation.plot_corr_grid.html#statsmodels.graphics.correlation.plot_corr_grid[Ref]
| scatter_ellipse(data)                     | Scatter plot with confidence ellipses http://www.statsmodels.org/stable/generated/statsmodels.graphics.plot_grids.scatter_ellipse.html#statsmodels.graphics.plot_grids.scatter_ellipse[Ref]
| fboxplot(data)                            | Like boxplot for continuous x-axis http://www.statsmodels.org/stable/generated/statsmodels.graphics.functional.fboxplot.html#statsmodels.graphics.functional.fboxplot[Ref]
| rainbowplot(data)                         | Plot all lines colored by order of funtional depth http://www.statsmodels.org/stable/generated/statsmodels.graphics.functional.rainbowplot.html#statsmodels.graphics.functional.rainbowplot[Ref]
| banddepth(data)                           | Plot centrality of data (e.g. median curve is highest) http://www.statsmodels.org/stable/generated/statsmodels.graphics.functional.banddepth.html#statsmodels.graphics.functional.banddepth[Ref]
| abline_plot(intercept, slope)             | Plot line http://www.statsmodels.org/stable/generated/statsmodels.graphics.regressionplots.abline_plot.html#statsmodels.graphics.regressionplots.abline_plot[Ref]
| influence_plot(results)                   | Plot studentized resids vs leverage http://www.statsmodels.org/stable/generated/statsmodels.graphics.regressionplots.influence_plot.html#statsmodels.graphics.regressionplots.influence_plot[Ref]
| plot_acf(x)                               | Plot autocorrelation http://www.statsmodels.org/stable/generated/statsmodels.graphics.tsaplots.plot_acf.html#statsmodels.graphics.tsaplots.plot_acf[Ref]
| plot_pacf(x)                              | Plot partial autocorrelation http://www.statsmodels.org/stable/generated/statsmodels.graphics.tsaplots.plot_pacf.html#statsmodels.graphics.tsaplots.plot_pacf[Ref]
| mosaic(data)                              | Mosaic plot from contingency table http://www.statsmodels.org/stable/generated/statsmodels.graphics.mosaicplot.mosaic.html#statsmodels.graphics.mosaicplot.mosaic[Ref]
| robust_skewness(y)                        | Robust measures of general skew http://www.statsmodels.org/stable/generated/statsmodels.stats.stattools.robust_skewness.html#statsmodels.stats.stattools.robust_skewness[Ref]
| OLSInfluence(results)                     | Calculate outlier and influence measures for OLS result http://www.statsmodels.org/stable/generated/statsmodels.stats.outliers_influence.OLSInfluence.html#statsmodels.stats.outliers_influence.OLSInfluence[Ref]
| randmvn(rho)                              | Random draws from equi-correlated multivariate normal http://www.statsmodels.org/stable/generated/statsmodels.sandbox.stats.multicomp.randmvn.html#statsmodels.sandbox.stats.multicomp.randmvn[Ref]
| set_remove_subs(sets)                     | Remove sets that are subsets of others http://www.statsmodels.org/stable/generated/statsmodels.sandbox.stats.multicomp.set_remove_subs.html#statsmodels.sandbox.stats.multicomp.set_remove_subs[Ref]
| ci_low, ci_upp = proportion_confint(count, nobs)  | Confidence interval for binomal proportion http://www.statsmodels.org/stable/generated/statsmodels.stats.proportion.proportion_confint.html#statsmodels.stats.proportion.proportion_confint[Ref]s
| regressionplots.plot_leverage_resid2      |
|===

* http://www.statsmodels.org/dev/graphics.html[Statsmodels Graphics]
* http://www.statsmodels.org/stable/examples/notebooks/generated/regression_plots.html[Regression plots examples]
* ! http://www.statsmodels.org/devel/examples/notebooks/generated/regression_diagnostics.html[Statsmodels Diagnostics]s

=== Other statistical plots

[cols="m,d"]
|===
| scipy.stats.probplot(x, plot=plt)          | Probability quantile plot https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.probplot.html[Ref]
|===

== Faceted plotting

    %output size=150
    %opts Curve {+framewise +axiswise } [tools=["hover", "box_zoom"]]
    hm=hv.HoloMap({kdims:hv.Curve()})
    (hm                              # Holomap
       .overlay(dims1)               # in same plot
       .layout(dims2).cols(..)       # different plots
    )                                # remaining variables will be slider

== Binning

[cols="m,d"]
|===
| hist, bin_edges = np.histogram(x, bins="auto")    | https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html[numpy.histogram]
| weights = [..]                            |
| density = True                            | integral over range is 1
| range=(min, max)                          |
| np.histogramdd(..)                        | https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogramdd.html[Ref]
| np.digitize(x, bins)                      | Return indices of bins https://docs.scipy.org/doc/numpy/reference/generated/numpy.digitize.html[Ref]
| np.searchsorted(a, v)                     | Find indices into sorted array https://docs.scipy.org/doc/numpy/reference/generated/numpy.searchsorted.html#numpy.searchsorted[Ref]
| scipy.stats.binned_statistic(..)          | Apply functions on bin insread of counting https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binned_statistic.html[Ref]
| scipy.stats.binned_statistic_dd(..)       | Apply functions on bin insread of counting https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binned_statistic_dd.html[Ref]
| scipy.stats.cumfreq(a, numbins=..)        | Cumulative histogram https://docs.scipy.org/doc/scipy/reference/generated/.cumfreq.html#.cumfreq[Ref]
| scipy.stats.relfreq(a, numbins=..)        | Relative frequency histogram https://docs.scipy.org/doc/scipy/reference/generated/.relfreq.html#.relfreq[Ref]
|===

* https://github.com/EelcoHoogendoorn/Numpy_arraysetops_EP[numpy-index]: faster library?
* https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.histogram_bin_edges.html#numpy.histogram_bin_edges[bin edge calculation]
* for increasing bins: `np.digitize(x, bins, right=True) == np.searchsorted(bins, x, side='left')`
* `digitize`: `bins[i-1] <= x < bins[i]``; `0` or `len(x)` if beyond bounds

=== Grouping

    edges = pd.cut(dd.col, bins=100).apply(attrgetter("left"))
    groups = dd.groupby(edges)

* https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html#pandas.cut[pandas.cut]

== Plot histogram

|===
| sns.distplot(df[col])                     | https://seaborn.pydata.org/generated/seaborn.distplot.html#seaborn.distplot[Ref]
| df.hist(col)                              | https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.hist.html[Ref]
|===

    # manual
    width=2
    hist=np.histogram(data, bins=np.arange(min(data), max(data)+3, width))
    plt.bar(hist[1][:-1], hist[0]/sum(hist[0]), width=width*0.9)

* set `mpl.rcParams["hist.bins"]='auto'`

== Heatmap

    sns.jointplot(
        "x", "y", data=df,
        kind="hex", size=10,
        gridsize=100,
        bins="log", mincnt=1,
        cmap="viridis",
        )

    df.plot.hexbin(
        "x", "y",
        bins="log", mincnt=1,
        cmap="viridis",
        )

    # manual
    dp = df.apply(lambda x: Series(np.histogram(x, bins=bins)[0], index=bins[:-1]))
    plt.pcolor(dp)

* https://seaborn.pydata.org/generated/seaborn.jointplot.html[seaborn.jointplot]
* https://matplotlib.org/api/_as_gen/matplotlib.pyplot.hexbin.html[pyplot.hexbin]
* https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.plot.hexbin.html[pandas hexbin]

== Exploration

    p=sns.FacetGrid(df, hue="c")
    p.map(sns.distplot, "y").add_legend()

    sns.plt.legend() # ?

== Plot correlation

    sns.heatmap(df.corr())

    df.corr().style.background_gradient("bwr")   # Table format

== Regression and Fitting

=== Linear Regression

==== Numpy Linear Regression (fastest)

    X = np.vstack([x, np.ones(len(x))]).T

    coef, residuals, rank, singular_values = np.linalg.lstsq(X, y)

2x as fast a scikit-learn.LinearRegression

==== Scipy Linear Regression (fastest too)

    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x,y)

lingress(x) if Nx2 array. Same as numpy raw version


==== Scikit-learn Linear Regression (fast)

    from sklearn import linear_model

    model=linear_model.LinearRegression()

    model.fit(np.reshape(x, (len(x),1)),y)
    print(model.intercept_, model.coef_)

==== Numpy Linear Regression (slow)

    coef=np.polyfit(x,y,1)

* more than coef if options full=True, rcond=True, etc. https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html[Ref]

==== Statsmodels Linear Regression (slow, but info)

    import statsmodels.api as sm

    X = sm.add_constant(x)

    model = sm.OLS(y,X).fit()
    coef=model.params                       # intercept last
    model.summary()                          # print exhaustive summary of statistics

    y_pred = model.predict(X)

    coef_confs = res.conf_int()

Regression result variables can be found on http://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.RegressionResults.html[Ref].
The specific OLSResults adds `get_influence`, `outlier_test`, `el_test` (emp.likelihood test), `conf_int_el` http://www.statsmodels.org/dev/_modules/statsmodels/regression/linear_model.html#OLSResults[Ref].

=== Splines

[cols="m,d"]
|===
| scipy.interpolate.UnivariateSpline(x, y)  | https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.UnivariateSpline.html#scipy.interpolate.UnivariateSpline[Ref]
|===


=== Robust regression

    X=sm.add_constant(X)

    model=sm.RLM(y, X, M=sm.robust.norms.HuberT())
    res=model.fit()
    coef=res.params

=== Orthogonal regression

https://docs.scipy.org/doc/scipy/reference/odr.html[Scipy Orthogonal regression]

    model=scipy.odr.Model(f)                # f(param, x)
    data=scipy.odr.Data(x_part, y_part)
    odr=scipy.odr.ODR(data, model)
    result=odr.run()
    result.pprint()

=== Smoothing

|===
| statsmodels.nonparametric.smoothers_lowess.lowess | http://www.statsmodels.org/devel/generated/statsmodels.nonparametric.smoothers_lowess.lowess.html[Ref]
| scipy.stats.uniform(-1, 2).rvs(size=..)   | Sample from distribution https://docs.scipy.org/doc/scipy/reference/stats.html[More functions]
|===

=== General curve fitting

    def func(xdata, *params):
        ...

    param_opt, param_cov = scipy.optimize.curve_fit(func, xdata, ydata)

F-test to compare models that are _nested_. Generally cross-validation also might work.

== Optimization

* https://github.com/lmfit/lmfit-py/[LmFit]: Higher level on scipy.optimize; Also Interactive fitting(?)

=== Non-derivative optimization

* https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fminbound.html[scipy.optimize.fminbound]
* https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.brent.html[scipy.optimize.brent]
* https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.golden.html[scipy.optimize.golden]

== Statistics

=== Find correlation

[col="m,d"]
|===
| numpy.corrcoef                            |
| sklearn.metrics.matthews_corrcoef         |
| np.linalg.conf(..)                        | Condition number to see multi-collinearity
|===

=== Scipy statistical functions

* Functions from `scipy.stats`
* Functions for masked arrays are in https://docs.scipy.org/doc/scipy/reference/stats.mstats.html[scipy.stats.mstats]

[col="m,d"]
|===
| tmean(a)                                  | Trimmed mean https://docs.scipy.org/doc/scipy/reference/generated/.tmean.html#.tmean[Ref]
| cumfreq(a, numbins=..)                    | Cumulative histogram https://docs.scipy.org/doc/scipy/reference/generated/.cumfreq.html#.cumfreq[Ref]
| percentileofscore(a, score)               | Percentile of a score relative to a list https://docs.scipy.org/doc/scipy/reference/generated/.percentileofscore.html#.percentileofscore[Ref]s
| r, p = pearsonr(x, y)                     | Pearson correlation https://docs.scipy.org/doc/scipy/reference/generated/.pearsonr.html#.pearsonr[Ref]
| c, p = spearmanr(a, b=None)               | Spearman rank-order correlation https://docs.scipy.org/doc/scipy/reference/generated/.spearmanr.html#.spearmanr[Ref]
| med_sl, med_intercep, lo_sl, up_sl = theilslopes(y, x=None)   | Robust linear regression estimators https://docs.scipy.org/doc/scipy/reference/generated/.theilslopes.html#.theilslopes[Ref]
| itemfreq(a)                               | 2D array of item frequencies https://docs.scipy.org/doc/scipy/reference/generated/.itemfreq.html#.itemfreq[Ref]
| relfreq(a, numbins=..)                    | Relative frequency histogram https://docs.scipy.org/doc/scipy/reference/generated/.relfreq.html#.relfreq[Ref]
| binned_statistics(x, values, statistic="mean")    | Compute binned statistic within each bin https://docs.scipy.org/doc/scipy/reference/generated/.binned_statistic.html#.binned_statistic[Ref] (also `_2d` and `_dd` versions)
| gmean(a)                                  | Geometric mean https://docs.scipy.org/doc/scipy/reference/generated/.gmean.html#.gmean[Ref]
| rankdata(a, method="average")             | Assign ranks, deal with ties https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rankdata.html#scipy.stats.rankdata[Ref]
| circmean(x, high=6.28..)                  | Circular mean https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.circmean.html#scipy.stats.circmean[Ref]
| ppcc_plot(x, a, b)                        | Probability plot correlation coef. Determine optimal shape parameter https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ppcc_plot.html#scipy.stats.ppcc_plot[Ref]
| probplot(x)                               | Calc/plot quantiles for probability plot https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.probplot.html#scipy.stats.probplot[Ref]
| gaussian_kde(data)                        | Gaussian kernel density https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html#scipy.stats.gaussian_kde[Ref]
| detrend(data)                             | https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.detrend.html#scipy.signal.detrend[Ref]
| scipy.special.entr(..)                    | Computes `-x*log(x)` (or Kullback) to calc entropy manually
|===

=== Scipy statistical tests

[col="m,d"]
|===
| binom_test(x, n=None, p=0.5)              | Exact test whether probability of experiment is p https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom_test.html#scipy.stats.binom_test[Ref]
| ttest_1samp(a, popmean)                   | One group t-test https://docs.scipy.org/doc/scipy/reference/generated/.ttest_1samp.html#.ttest_1samp[Ref]
| wilcoxon(x, y=None)                       | Whether x-y symmetric, i.e. same distribution. Like non-parametric paired t-test https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.wilcoxon.html#scipy.stats.wilcoxon[Ref]
| kruskal(..)                               | Whether a group medians equal. Like non-parametric ANOVA https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kruskal.html#scipy.stats.kruskal[Ref]
| normaltest(a)                             | Whether normal distribution https://docs.scipy.org/doc/scipy/reference/generated/.normaltest.html#.normaltest[Ref]
| shapiro(x)                                | Whether normal https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.shapiro.html#scipy.stats.shapiro[Ref]
| anderson(x, dist="norm")                  | Whether sample is from particular distribution https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.anderson.html#scipy.stats.anderson[Ref]
| kurtosistest(a)                           | Whether kurtosis that of normal https://docs.scipy.org/doc/scipy/reference/generated/.kurtosistest.html#.kurtosistest[Ref]
| skewtest(a)                               | Whether skew that of normal https://docs.scipy.org/doc/scipy/reference/generated/.skewtest.html#.skewtest[Ref]
| jarque_bera(x)                            | Whether skew and kurtosis normal https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.jarque_bera.html#scipy.stats.jarque_bera[Ref]
| friedmanchisquare(..)                     | Whether repeated measurements on same individuals have same distribution https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.friedmanchisquare.html#scipy.stats.friedmanchisquare[Ref]
| power_divergence(f_obs)                   | Cressie-Read test if categorical data jas given freqs https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.power_divergence.html#scipy.stats.power_divergence[Ref]
| combine_pvalues(pvalues)                  | Combine p-values of independent tests with same hypothesis https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.combine_pvalues.html#scipy.stats.combine_pvalues[Ref]
| ansari(x, y)                              | Non-parametric test for equality of scale parameters https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ansari.html#scipy.stats.ansari[Ref]
| mood(x, y)                                | Non-parametric two-sample test if two samples from same distribution with same scale https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mood.html#scipy.stats.mood[Ref]
| median_test(..)                           | Whether many samples from same median https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.median_test.html#scipy.stats.median_test[Ref]
| fisher_exact(table)                       | Exact test in 2x2 table https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.fisher_exact.html#scipy.stats.fisher_exact[Ref]
| chi2_contingency(obs)                     | Whether contigency table independent https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html#scipy.stats.chi2_contingency[Ref]
| mannwhitneyu(x, y)                        | Rank test https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html#scipy.stats.mannwhitneyu[Ref]
|===


== Mathematics

[col="m,d"]
|===
| np.linalg.cond(mat)                       | Condition number of matrix (different norms possible) https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.cond.html[Ref]
| np.arctan2(y, x)                          |
| np.isfinite(x)                            | Not NaN and not Inf
|===

=== Eigenvalues

[col="m,d"]
|===
| np.linalg.eigvals(a)                      | Eigenvalues https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eigvals.html[Ref]
| np.linalg.eig(a)                          | Eigenvalues and right eigenvectors https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eig.html#numpy.linalg.eig[Ref]
| np.linalg.eigvalsh(a)                     | Eigenvalues of Hermitian/real symmetric matrix https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eigvalsh.html#numpy.linalg.eigvalsh[Ref]
| np.linalg.eigh(a)                         | Eigenvalues and Eigenvectors of Hermitian/real symmatrix matrix https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eigh.html#numpy.linalg.eigh[Ref]
|===

* https://docs.scipy.org/doc/scipy/reference/tutorial/arpack.html[ARPACK] to find a few eigenvalues/-vectors from large sparse matricess


=== Polynomials

* https://docs.scipy.org/doc/numpy/reference/generated/numpy.poly1d.html[numpy.poly1d]
* https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html[Numpy Polyfit]

[col="m,d"]
|===
| coef = np.polyfit(x, y)                   | Fit polynomial
| np.polyval(coef, x)                       | Calculate polynomial (coef from highest to lowest) https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyval.html[Ref]
|===

=== Approximate fraction

    fractions.Fraction(x).limit_denominator(denom)


== Setting Import/Project path

    from pathlib import Path
    import os, sys

    project_dir=Path.home().joinpath("Projects/Name")               # Python 3.5
    sys.path.append(str(project_dir.joinpath("scripts/lib")))       # add to PYTHONPATH

== Cluster heatmap with block grouping

    from scipy.cluster.hierarchy import linkage
    link=linkage(dd, metric="cosine")   # or other param? method="centroid"??
    sns.clustermap(dd, row_linkage=link, col_linkage=link, cmap="magma_r")


    from scipy.cluster.hierarchy import linkage
    link=linkage(dd.T, metric="correlation", method="complete")    # distance = 1 - correlation
    link[:,2]=np.log10(link2[:,2]+1e-10)+11                        # resolve small distances better
    fig, ax = plt.subplots(figsize=(20, 80))
    denres=dendrogram(link, labels=dd.columns, orientation="right", ax=ax, color_threshold=8, leaf_font_size=8, above_threshold_color="lightgray");

    pipe(zip(fcluster(linkage_data, 0.9, "distance"), labels), groupby(0))  # get clusters

You can use https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.to_tree.html[to_tree] to get a traversable tree with functions like https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.ClusterNode.pre_order.html[pre_order]

== Scikit-learn tools

[col="m,d"]
|===
| utils.check_X_y(X, y)                     | Make sense-checks on data (shape, NaNs, ...) http://scikit-learn.org/stable/modules/generated/sklearn.utils.check_X_y.html[Ref]
| preprocessing.robust_scale(X)             | Center to median and scale by IQR http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.robust_scale.html[Ref]
| model_selection.permutation_test_score(clf, X, y) | Eval significance of CV score with permutations http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.permutation_test_score.html[Ref]
| model_selection.TimeSeriesSplit(n_splits).fit(..) | Make splits such that test indices are always higher than train http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html[Ref]
| isotonic.check_increasing(x, y)           | Check whether monotonic http://scikit-learn.org/stable/modules/generated/sklearn.isotonic.check_increasing.html[Ref]
| feature_selection.mutual_info_regression(X, y)    | Estimate mutual information (with nearest neighbours) to _continuous_ `y` http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html[Ref]
| feature_selection.mutual_info_classif(X, y)       | Estimate mutual information (with nearest neighbours) to _discrete_ `y` http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html[Ref]
|===

=== Evaluation

[col="m,d"]
|===
| model_selection.validation_curve(clf, X, y)   | Scores vs parameters http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html[Ref]s
| model_selection.learning_curve(clf, X, y) | Learning curve, Performance vs Train size http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html[Ref]
| ensemble.partial_dependence.plot_partial_dependence(gbrt, X, ...) | Partial dependence plot http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.partial_dependence.plot_partial_dependence.html[Ref]
| ensemble.partial_dependence.partial_dependence(gbrt, target)      | Partial dependence http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.partial_dependence.partial_dependence.html[Ref]
|===

== Numpy

|===
| ranks=np.empty_like(a) +
  ranks[a.argsort()]=np.arange(len(a))      | Rank array
|===

== Color

* https://matplotlib.org/users/colormaps.html[Matplotlib color maps]
* https://matplotlib.org/examples/color/colormaps_reference.html[Matplotlib color map examples]
* https://bokeh.github.io/colorcet/[Bokeh colorcet]
+ https://colorcet.pyviz.org/user_guide/index.html[colorcet]
* http://colorbrewer2.org/#type=qualitative&scheme=Paired&n=8[Colorbrewer]
* https://jiffyclub.github.io/palettable/[Palettable]
* https://bokeh.pydata.org/en/latest/docs/reference/palettes.html[Bokeh palettes]
* https://matplotlib.org/cmocean/[cmocean]
* https://seaborn.pydata.org/tutorial/color_palettes.html[Plotting colormaps with Seaborn]
* https://nicoguaro.github.io/posts/cyclic_colormaps/[Cyclic color maps]
* suggested qualitative color palettes:
** https://matplotlib.org/examples/color/colormaps_reference.html[Set1]
** https://bokeh.pydata.org/en/latest/docs/reference/palettes.html[Category10]
** https://jiffyclub.github.io/palettable/cartocolors/qualitative/#bold_10[Palettable Cartocolors Bold10]
* Matplotlib Color:
** RGB, RGBA tuple, hex string
** float as string for grayscales
** "rgb cymk w", X11 colorname, "xkcd:sky blue" (xkcd names), "tab:blue" (Tableau)
** "C1", "C2", ... `axes.prop_cycle` colors
** all case-insensitive apart from "C1"

[col="m,d"]
|===
| plt.cm.get_cmap("viridis", N)             | Get discrete colormap (e.g. to see contours in hexbin plots
| cycler.cycler("color", plt.cm.Set3.colors)    |
|===

=== Print Matplotlib colormap names

    for name, pal in sns.cm.mpl_cm.cmap_d.items():
        if name.endswith("_r") or name in ["jet", "spectral"] or name.startswith("Vega"):
            continue
        try:
            sns.palplot(sns.color_palette(name, 10), 0.6)
            plt.title(name)
            plt.show()
        except Exception as e:
            print(f"Failed {name} due to {e}")


=== Choose colormaps

|===
| sns.diverging_palette(..)                 | Choose diverging palette with custom end colors
| sns.choose_colorbrewer_palette("diverging")   | Choose one of color brewer palettes
| sns.choose_diverging_palette()            | Interactive choice from custom end colors
| hv.plotting.list_cmaps()                  | List all colormaps
|===

* use with `%matplotlib inline`
* http://colorcet.pyviz.org/
* http://holoviews.org/user_guide/Colormaps.html

==== Selected colormaps

* Time plots: "gnuplot"

== Data Sources

[col="m,d"]
|===
| img = scipy.misc.face(gray=False)         | Image of a racoon
| np.random.randn(100, 10)                  | Standard normal matrix
| np.random.normal(loc=0, scale=1, size=(100,10))   | Non-standard normal matrix
| random.choices(pop, k=..)                 | https://docs.python.org/3/library/random.html#random.choices[Ref]
| np.random.choice(pop, size=..)            | https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.choice.html[Ref]
| pd.read_csv("https://..")                 |
| DataFrame(..., columns=list(string.ascii_lowercase[:10])) | Quick alphabetic column names
| sm.datasets.get_rdataset(name)            | Download and return R dataset http://www.statsmodels.org/dev/datasets/statsmodels.datasets.get_rdataset.html#statsmodels.datasets.get_rdataset[Ref]
| patsy.demo_data(*"pqr")                   | Generated data. "a-m" categorical, "p-z" numerical http://patsy.readthedocs.io/en/latest/API-reference.html#patsy.demo_data[Ref]
| imblearn.datasets.fetch_datasets()[".."]  | http://contrib.scikit-learn.org/imbalanced-learn/stable/datasets/index.html[Ref]
| imblearn.datasets.make_imbalance(X, y, ratio) | Make data imbalanced
| sklearn.datasets.load_*() +
| sklearn.datasets.fetch_*()                | Small sample of standard datasets http://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets[Ref]
| sklearn.datasets.load_sample_image(name)  | Load "china.jpg" or "flower.jpg" http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_sample_image.html#sklearn.datasets.load_sample_image[Ref]
| sklearn.datasets.fetch_openml(..)         | http://scikit-learn.org/dev/modules/generated/sklearn.datasets.fetch_openml.html[Ref]
| sklearn.datasets.make_*(..)               | Create artifical data http://scikit-learn.org/stable/modules/classes.html#samples-generator[Ref]
| pandas_datareader.*                       | Different sources of financial data https://pandas-datareader.readthedocs.io/en/latest/remote_data.html[Ref]
| matplotlib.cbook.get_sample_data(..)      | Matplotlib sample data https://matplotlib.org/api/cbook_api.html#matplotlib.cbook.get_sample_data[Ref]
| bokeh.sampledata.download()               | Bokeh sample data https://bokeh.pydata.org/en/latest/docs/reference/command/subcommands/sampledata.html[Ref]
| quandl                                    | Financial data https://www.quandl.com/tools/python[Ref]
| openml                                    | General data https://openml.github.io/openml-python/stable/usage.html[Ref]
| datadotworld                              | Data https://github.com/datadotworld/data.world-py[Ref]
| data retriever                            | Data https://retriever.readthedocs.io/en/latest/datasets.html[Ref]
| skdata                                    | Data https://github.com/jaberg/skdata/wiki/Data-Set-Modules[Ref]
| PySAL sample data                         | http://pysal.readthedocs.io/en/latest/users/tutorials/examples.html[Ref]
| PyDataset                                 | Data https://github.com/iamaziz/PyDataset[Ref]
| Yellowbrick sample data                   | Samples from UCI http://www.scikit-yb.org/en/latest/api/datasets.html[Ref]
| pd.util.testing.makeTimeDataFrame()       | Test data frame
| pd.util.testing.makeDataFrame()           | Test data frame
| sklearn.datasets.load_files(..)           | Subfolder names are categories
| quilt                                     | https://docs.quiltdata.com/get-started/quick-start[Quick Start]
|===

* OpenML data sets which are not too easy (for RF): 37, 4134, 41142, 298, 336, 716, 732, 750, 753, 783, 850, 863, 886, 889, 907, 908, 915, 922, 955, 983, 1012, 1045, 1048, 1064, 1071, 1412, 1436, 1441, 1446, 1448, 1450, 1453, 1463, 1473, 1479, 1484, 1506, 1547, 1566, 1572, 1589, 4329, 40645, 40646, 40647, 40648, 40649, 40650
** where `min_samples_split` help(?) vs overfitting of `max_features` alone: 137, 850, 1064, 1484, 908, 1071, 888, 983, 40647, 298

== Data Operations

=== Reduce multiple conditions

    functools.reduce(lambda x,y:x & y, [..])

=== Search in list

   np.searchsorted(..)

=== Bisecting

* bisect `left` vs. `right` matters only for _exact_ boundary hits
* `bisect` same as `bisect_right`

    i = bisect(a, x)
    if i > 0:
        print("Next value smaller or equal:", a[i-1])
    else:
        print("Smaller than all values")

    i = bisect.bisect_left(a, x)
    if i < len(a):
        print("Next value larger or equal:", a[i])
    else:
        print("Larger than all values")

=== Merge dicts

    Merge dicts:
    d={}
    d.update(..)
    d.update(..)

    dict(chain(d1.items(), d2.items()))

    ChainMap({}, d2, d1)   # {} so that dont modify old

    d={**d1, **d2}  # in Python 3.5

(Wrong:
* dict(d1, **d2)   # needs to be keyword-like
* dict(d1.items() | d2.items())   # unpredicatable order
)


== Pandas

http://pandas.pydata.org/pandas-docs/stable/cookbook.html

=== Various

|===
| df.dropna(how="all")                      | Drop only when all columns NaNs
| pd.to_numeric(.., errors="coerce")        | Parse invalid to NaN
| idx.to_series()                           | Create series with NewIdx=NewVals=Idx
| s.to_frame()                              | Convert Series to DataFrame
|===

=== Selecting columns

    df.filter(like="_min", axis=1)    # also regex possible

=== Pipelines

    (df
     .assign(newcol=series)
     .assign(newcol=lambda dffunc:...)
     .pipe(df_func, arg1=.., ..)
     .pipe((df_func, "df_argname"), arg1=.., ..)
     .rename(index=.., columns=..)               # scalar/list for series; dict/func for map
     .rename_axis(.., axis=0)                    # scalar/list for Index.name/MultiIndex.names; dict/func for labels
     .where(cond, other=nan)                     # select self if cond and otherwise from other; return same shape
     .mask(cond, other=nan)                      # replace by other where cond is true; return same shape
     .query(expr)
    )

=== Sort within part of group

    dfgr_sort = dfgr["var"].groupby(level=0, group_keys=False).apply(lambda x: x.order())

=== Find all correlates

    df.corr().unstack().sort_values(ascending=False)

But also includes self-pairs.

=== Sort then take first for groups

    df.sort_values(..).groupby(.., as_index=False).first()

=== Groupby like itertools

    df.A.groupby((df.A != df.A.shift())

=== Mask values

    df["newcol"] = np.where(cond, then_val, else_val)
    df.where(df_mask, new_val)

=== Window functions

    s.resample("D").max().rolling(window=5).max()
    s.rolling("5D").max()

    s.rolling(..).apply(func)

    def flatindex(df):
        df.columns=df.columns.map("_".join)
        return df

    df1=(df
        .set_index("time")
        .groupby("cat")
        [cols]
        .apply(lambda d:d
               .sort_index()
               .rolling("7d")     # http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases
               .agg(["min", "max"])
               .pipe(flatindex)
              )
        )

=== Other

    df.isnull().any(axis=0) to check for columns with missing values

https://pandas.pydata.org/pandas-docs/stable/cookbook.html[Pandas Cookbook] examples from StackOverflow.

    # Select values which are closest
    closest_idxs = (df.A-val).abs().argsort()

    # Combine conditions
    tot_cond = functools.reduce(operator.and_, [cond1, cond2, ..])

    # itertools.groupby-like
    df.A.groupby( (df.A != df.A.shift()) .cumsum()).groups

=== Time

|===
| date_col.dt.to_period("M")                | Convert to monthly (will be Period)
|===

== kNN custom metric

    def mydist(x, y):
        return np.sum((x-y)**2)

    nbrs = NearestNeighbors(n_neighbors=4, algorithm='ball_tree', metric='pyfunc', func=mydist)

== Missing data

* `sklearn.impute`

== Spark

=== Window operations

http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Window

    from pyspark.sql.window import Window

    window=(Window
            .orderBy(*cols)
            .partitionBy(*cols)
            .rowsBetween(start, end)
            .rangeBetween(start, end)  # by value?
           )

    df.withColumn("newcol", F.sum("col").over(window))

Special values for range: `Window.unboundedPreceding`, `Window.unboundedFollowing`, `Window.currentRow`.

Previous customer value

    window=Window.partitionBy("customer").orderBy("date")
    df.withColumn("lastval", F.lag("col", 1).over(window))

=== User defined functions

    from pyspark.sql.functions import udf, col
    from pyspark.sql.types import StringType

    udf_func=udf(my_func, StringType())
    df1=df.withColumn(udf_func(col("col")))

=== Cumulative sum

    from pyspark.sql.window import Window

    df.withColumn("cumul", F.sum("vals").over(Window.orderBy(F.desc("col_sort))))

=== Top examples in groups

    from pyspark.sql.window import Window

    (df
     .groupby("part_col", "group_col").sum("order_vals")
                                      .withColumnRenamed("sum(order_vals)", "order_col")
     .withColumn("rank", F.rank().over(Window.partitionBy("part_col")
                                             .orderBy("order_col")))
    )


=== Multiple values into UDF

    udf(..)(F.struct(df[..], df[..]))


== Scikit-learn

=== Basic usage

    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import auc, roc_curve

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

    clf=RandomForestClassifier(n_estimators=300)

    clf.fit(X_train, y_train)

    y_pred=clf.predict(X_test)

    fpr, tpr, thresh = roc_curve(y_test, y_pred)
    auc(fpr, tpr)

=== Pipeline

    p=sklearn.pipeline.Pipeline([("trans1", t1), ("trans2", t2), ("pred", pred)])
    p.get_params("pred__C")

    sklearn.pipeline.FeatureUnion([("pred1", p1), ("pred2", p2)], transformer_weights=[w1, w2], n_jobs=1)

    make_pipeline([t1, t2, pred])  # make pipeline, names automatic
    make_union([p1, p2]) # make union, names automatic

=== Write tree to PDF

    from sklearn.externals.six import StringIO
    import pydot
    dot_data = StringIO.StringIO()
    tree.export_graphviz(clf, out_file=dot_data)
    graph = pydot.graph_from_dot_data(dot_data.getvalue())
    graph.write_pdf("iris.pdf")

== Seaborn

=== Colors

Color points in any scatter plot

    sns.regplot(..., scatter_kws=dict(color=mpl.cm.viridis(mpl.colors.Normalize()(vals))))

For example with `dtidx.astype(int)` to map time.

== Bokeh

=== Basic usage

   from bokeh.plotting import figure, output_notebook, show
   p=figure(title="..",
            plot_width=300, plot_height=300,
            x_axis_label="..",
            x_range=(a,b),
            )

   p.line(x=.., y=.., legend="..")

   output_notebook()
   show(p)
   # save(p)

=== Custom Ticker format

    def ticker():   # no argument, since `tick` will be available
            return "{}".format(tick)

    p.xaxis.formatter = FuncTickFormatter.from_py_func(ticker)


    p.xaxis.from_coffeescript(
    """
    function (tick) {
        return ...
    };
    """)

=== Hover tool

    source=ColumnDataSource(data=dict(x=[..],..))

    hover=HoverTool(tooltips=[("(x,y)", "(@x, @y)"), # $ for special values
                              ("desc", "@desc"),     # @ for variables
                             ],
                    attachment="vertical",
                    line_policy="nearest",
                    )
    p = figure(tools=[hover])
    p.circle("x", "y", source=source)

Tooltips can also be a HTML string. Special variables like "$color[hex]:fill_color"

* Shift for multi-select
* Also touch screen support (pinch, tap, etc.)


    tooltips=[('Col1:', '@col1'), ('Col2:', '@col2')]
    p = Scatter(.., tooltips=tooltips)


=== Twin axis

    p.extra_y_ranges={"abc":Range1d(..,..)}
    p.circle(.., y_range_name="abc")
    p.add_layout(LinearAxis(y_range_name="abc"), "left")

=== Tight grid plots

    plt.tight_layout()          # may help
    plt.subplots_adjust(hspace=0.4, wspace=0.4)    # manual adjustment

Some Seaborn commands take a `gridspec_kws` parameter

=== Matplotlib compatibility

    from bokeh import mpl
    sns.violinplot(...)
    show(mpl.to_bokeh())

Relies on `mplexporter`. Will be better when Matplotlib adopts native JSON.


== Matplotlib

=== Annotate Bar plot

    plt.bar(X, +Y1)
    for x,y in zip(X,Y1):
        plt.text(x+0.4, y+0.05, '%.2f' % y, ha='center', va= 'bottom')

=== Make labels in a box

    for label in ax.get_xticklabels() + ax.get_yticklabels():
        label.set_fontsize(16)
        label.set_bbox(dict(facecolor='white', edgecolor='None', alpha=0.65 ))

=== Set color of bar in bar-plot

    barlist=plt.bar([1,2,3,4], [1,2,3,4])
    barlist[0].set_color('r')

=== Set data position and format

    ax.xaxis_date()
    ax.xaxis.set_major_locator(mpl.dates.MonthLocator())
    ax.xaxis.set_major_formatter(mpl.dates.DateFormatter('%Y-%m-%d'))

=== Clip color in colorbar


    cmap.set_bad(color="..")

    cmap.set_under(color="..")
    plot(..., cmap=cmap, vmin=1e-10)

=== Hexbin with dates

    ax.set_aspect("equal")
    ax.hexbin(mpl.dates.date2num(df.a), mpl.dates.date2num(df.b), gridsize=20)
    ax.xaxis_date()
    ax.yaxis_date()
    ax.xaxis.major.formatter.scaled[1.0] = "%Y-%m-%d"
    ax.yaxis.major.formatter.scaled[1.0] = "%Y-%m-%d"

=== Increase Figure size

    zoom = 2
    w, h = fig.get_size_inches()
    fig.set_size_inches(w * zoom, h * zoom)

    dpi = fig.get_dpi()   # This will get dpi that is set matplotlibrc
    fig.savefig("test.jpg", dpi=dpi*2)

=== Print all color names

    for name, hex in matplotlib.colors.cnames.iteritems(): # print all color names
        print(name, hex)

=== Fix midpoint of colormap

Midpoint of colormap (unless use vmin, vmax)

    from numpy import ma
    from  matplotlib import cbook

    class MidPointNorm(Normalize):
        def __init__(self, midpoint=0, vmin=None, vmax=None, clip=False):
            Normalize.__init__(self,vmin, vmax, clip)
            self.midpoint = midpoint

        def __call__(self, value, clip=None):
            if clip is None:
                clip = self.clip

            result, is_scalar = self.process_value(value)

            self.autoscale_None(result)
            vmin, vmax, midpoint = self.vmin, self.vmax, self.midpoint

            if not (vmin < midpoint < vmax):
                raise ValueError("midpoint must be between maxvalue and minvalue.")
            elif vmin = vmax:
                result.fill(0) # Or should it be all masked? Or 0.5?
            elif vmin > vmax:
                raise ValueError("maxvalue must be bigger than minvalue")
            else:
                vmin = float(vmin)
                vmax = float(vmax)
                if clip:
                    mask = ma.getmask(result)
                    result = ma.array(np.clip(result.filled(vmax), vmin, vmax),
                                      mask=mask)

                # ma division is very slow; we can take a shortcut
                resdat = result.data

                #First scale to -1 to 1 range, than to from 0 to 1.
                resdat -= midpoint
                resdat[resdat>0] /= abs(vmax - midpoint)
                resdat[resdat<0] /= abs(vmin - midpoint)

                resdat /= 2.
                resdat += 0.5
                result = ma.array(resdat, mask=result.mask, copy=False)

            if is_scalar:
                result = result[0]
            return result

        def inverse(self, value):
            if not self.scaled():
                raise ValueError("Not invertible until scaled")
            vmin, vmax, midpoint = self.vmin, self.vmax, self.midpoint

            if mpl.cbook.iterable(value):
                val = ma.asarray(value)
                val = 2 * (val-0.5)
                val[val>0]  *= abs(vmax - midpoint)
                val[val<0] *= abs(vmin - midpoint)
                val += midpoint
                return val
            else:
                val = 2 * (val - 0.5)
                if val < 0:
                    return  val*abs(vmin-midpoint) + midpoint
                else:
                    return  val*abs(vmax-midpoint) + midpoint

=== Plot KDE density

    density=scipy.stats.gaussian_kde(data)
    x=np.linspace(min(data),max(data),100)
    plt.plot(x,density(x)*width)

=== Midpoint colormap

Midpoint of colormap
(unless use vmin, vmax)

    from numpy import ma
    from  matplotlib import cbook

    class MidPointNorm(Normalize):
        def __init__(self, midpoint=0, vmin=None, vmax=None, clip=False):
            Normalize.__init__(self,vmin, vmax, clip)
            self.midpoint = midpoint

        def __call__(self, value, clip=None):
            if clip is None:
                clip = self.clip

            result, is_scalar = self.process_value(value)

            self.autoscale_None(result)
            vmin, vmax, midpoint = self.vmin, self.vmax, self.midpoint

            if not (vmin < midpoint < vmax):
                raise ValueError("midpoint must be between maxvalue and minvalue.")
            elif vmin == vmax:
                result.fill(0) # Or should it be all masked? Or 0.5?
            elif vmin > vmax:
                raise ValueError("maxvalue must be bigger than minvalue")
            else:
                vmin = float(vmin)
                vmax = float(vmax)
                if clip:
                    mask = ma.getmask(result)
                    result = ma.array(np.clip(result.filled(vmax), vmin, vmax),
                                      mask=mask)

                # ma division is very slow; we can take a shortcut
                resdat = result.data

                #First scale to -1 to 1 range, than to from 0 to 1.
                resdat -= midpoint
                resdat[resdat>0] /= abs(vmax - midpoint)
                resdat[resdat<0] /= abs(vmin - midpoint)

                resdat /= 2.
                resdat += 0.5
                result = ma.array(resdat, mask=result.mask, copy=False)

            if is_scalar:
                result = result[0]
            return result

        def inverse(self, value):
            if not self.scaled():
                raise ValueError("Not invertible until scaled")
            vmin, vmax, midpoint = self.vmin, self.vmax, self.midpoint

            if mpl.cbook.iterable(value):
                val = ma.asarray(value)
                val = 2 * (val-0.5)
                val[val>0]  *= abs(vmax - midpoint)
                val[val<0] *= abs(vmin - midpoint)
                val += midpoint
                return val
            else:
                val = 2 * (val - 0.5)
                if val < 0:
                    return  val*abs(vmin-midpoint) + midpoint
                else:
                    return  val*abs(vmax-midpoint) + midpoint

=== Add right axis with different scaling

    # use only ax. commands and not plt. command thereafter
    def right_axis(scale_func, ax):
        def convert_ax_callback(ax):
            y1, y2=ax.get_ylim()
            ax2.set_ylim(scale_func(y1), scale_func(y2))
            ax2.figure.canvas.draw()

        ax2=ax.twinx()
        ax.callbacks.connect("ylim_changed", convert_ax_callback)
        return ax2


=== Hexbin with dates

    ax.set_aspect("equal")
    ax.hexbin(mpl.dates.date2num(df.a), mpl.dates.date2num(df.b), gridsize=20)
    ax.xaxis_date()
    ax.yaxis_date()
    ax.xaxis.major.formatter.scaled[1.0] = "%Y-%m-%d"
    ax.yaxis.major.formatter.scaled[1.0] = "%Y-%m-%d"

=== Plotting maps

    from mpl_toolkits.basemap import Basemap
    import matplotlib.cm as cm

    m = Basemap(projection='robin',lon_0=0,resolution='c')
    x, y = m(reg['longitude'],reg['latitude'])

    figure(figsize=(15,15))
    m.drawcoastlines(linewidth=0.25)
    m.drawcountries(linewidth=0.25)
    m.fillcontinents(color='coral',lake_color='aqua')
    m.drawmapboundary(fill_color='white')
    m.drawmeridians(np.arange(0,360,30))
    m.drawparallels(np.arange(-90,90,30))
    m.scatter(x,y,s=reg['Number']*3,c=reg['Number']/5,marker='o',zorder=4, cmap=cm.Paired,alpha=0.5)

For problems with running basemap ("epsg" not found) set env var PROJ_

=== ??? axis

    ax.set_frame_on(False)
    ax.set_yticks(np.arange(nba_sort.shape[0]) + 0.5, minor=False)
    ax.invert_yaxis()
    ax.xaxis.tick_top()
    ax.grid(False)
    for t in ax.xaxis.get_major_ticks():
        t.tick1On = False
        t.tick2On = False

=== Legend changes

For on-the-fly legend order changes, resort self.lines+self.patches+self.collections+self.containers (see matplotlib/axes/_axes.py: def _get_legend_handles(), line 221
(e.g. for barplot `ax._containers_=list(reversed(ax.containers))`)

    legend(_handles_=[mpl.patches.Rectangle((0, 0), 0, 0, _fc_="r", _label_="Rect"), +
    mpl.lines.Line2D((0,0),(0,0), _c_="b", _label_="Line")])    | http://matplotlib.org/1.3.1/users/legend_guide.html[Ref]

    legend([mpl.patches.Patch(_color_=...),...],[label1,...])

    legend([hbars.patches[0],...], [hbars.patches[0].get_label(),...]) # custom order

=== Scientific labels

    formatter = mpl.ticker.*ScalarFormatter*(_useMathText_=True) +
    formatter.*set_scientific*(True)  +
    formatter.*set_powerlimits*((-1,1)) +
    ax.yaxis.*set_major_formatter*(formatter)

=== Plot colormap

    pcolor(np.array([list(range(cmap.N))]), cmap=cmap)

== Logging

    import logging
    logging.basicConfig(level=logging.INFO,
                        format="[%(levelname)s] %(funcName)s(%(lineno)s) %(message)s",
                        stream=sys.stdout,
                       )
    #logging.getLogger().setLevel("DEBUG")

    logger=logging.getLogger(__name__)

=== Daiquiri

    import daiquiri
    daiquiri.setup(level=logging.INFO, outputs=[daiquiri.output.STDOUT])


=== Color logs

Use my

    logger = create_color_logger()

==== coloredlogs

* https://coloredlogs.readthedocs.io/en/latest/api.html
* seems OK and featureful; default format not quite what I want
* does too much?
* lots of env variables to set default behaviour
* will color message by default (not the tag)

    import coloredlogs
    coloredlogs.install(isatty=True, stream=sys.stdout)            # isatty to enforce in Jupyter

    import logging
    logger = logging.getLogger(__name__)

==== logzero

* preconfigured logger seems pretty nice, but didn't find how to set output stream to stdout
* seems to be quickest way

    from logzero import logger
    logger.info("TEXT")

==== daiquiri

* didn't manager to enforce TTY mode though and hence no color support

    import daiquiri
    daiquiri.setup(level="INFO", outputs=[daiquiri.output.STDOUT])

    logger = daiquiri.getLogger(__name__)

==== colorlog

* https://github.com/borntyping/python-colorlog
* just provides formatter and seems sensible
* also has secondary log colors


=== File only logging

    import logging
    logger=logging.getLogger()
    logger.setLevel(logging.DEBUG)
    logger.addHandler(logging.FileHandler("out.log"))

=== Reset logging in Jupyter

    logging.shutdown()
    from imp import reload
    reload(logging)

== Date and Time

=== Alternative libraries

* https://arrow.readthedocs.io/en/latest/[Arrow] (quite popular)
* https://pendulum.eustace.io/docs/#introduction[Pendulum]
* http://delorean.readthedocs.io/en/latest/quickstart.html[Delorean]
* https://github.com/kennethreitz/maya[Maya](?)
* some https://pendulum.eustace.io/faq/[Some comparison]

=== Business days

    from pandas.tseries.offsets import CDay
    cdays=CDay(holidays=["2016-01-01"])
    cdays.rollforward(dt).to_pydatetime()

=== Month days

    weekday, days_in_month = calendar.monthrange(year, month)

=== Business days

    busdaycal = np.busdaycalendar(holidays=[..])

    busday_diff = np.busday_cnt(start_date, end_date, busdaycal=busdaycal)

    end_date = np.busday_offset(start_date, busday_diff, "forward", busdaycal=busdaycal)

    from pandas.tseries.offsets import BMonthEnd
    offset=BMonthEnd()
    offset.rollforward(date).to_pydatetime()


=== Numpy datetime

    datetime = dt.datetime.utcfromtimestamp((numpydatetime - np.datetime64("1970-01-01T00:00:00Z"))/np.timedelta(1, "s"))

    datetime = pd.Timestamp(numpydatetime)   # subclass of dt.datetime

== Warnings

    import logging
    logging.captureWarnings(True)

    logger=logging.basicConfig(stream=open("run.log", "w"))

== Neural network

=== CoordConv
* tasks of translating pixel to coordinates (or vice versa): ANN overfit (https://eng.uber.com/coordconv/)
* -> need CoordConv (also provide coordinates for each conv.)
* CoordConv only helps a bit for image classification though
* also less artifacts with GANs

== Time series

=== Rolling linear regression

    N = 5    # order
    window = np.linspace(-2,4,N)/N
    y_pred = y.rolling(N).apply(window.dot).shift()

=== Exploration

    from statsmodels.graphics import tsaplots

    y = dd["target_name"].dropna().sort_index()

    tsaplots.plot_pacf(y, lags=20)                   # check for autocorrelation

    tsaplots.plot_pacf(y.diff().dropna(), lags=20)   # check for autocorrelation of diff'ed series

=== SARIMAX

* http://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html
* http://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAXResults.html

    model = sm.tsa.SARIMAX(target, order=(....))
    result = model.fit()
    result.summary()

    statsmodels.graphics.gofplots.qqplot(result.resid, line="s")          # Plot errors vs Normal distr

    preds = result.predict(start=.., end=.., dynamic=True)

    result.get_forecast(steps=20)

* `dynamic=True` continue deriving predictions of target from previous predictions instead of real values (which were given)
** relative to `start` (`start=0` by default); can be date
* to generate data use `smt.arma_generate_sample(ar=ar, ma=ma, nsample=n, burnin=burn)`

=== Exponential Smoothing

    from statsmodels.tsa.holtwinters import Holt

    model=Holt(y)        # ExponentialSmoothing with seasons
    res=model.fit(smoothing_level=0.5, smoothing_slope=0.9, optimized=False)    # or leave it out for automatic

=== Other tutorials

    mdl = smt.AR(x).fit(maxlag=30, ic='aic', trend='nc')
    est_order = smt.AR(x).select_order(maxlag=30, ic='aic', trend='nc')

    # Using student T distribution usually provides better fit
    am = arch_model(TS, p=p_, o=o_, q=q_, dist='StudentsT')
    res = am.fit(update_freq=5, disp='off')

=== Pyramid (Auto Arima)

http://pyramid-arima.readthedocs.io/en/latest/_submodules/arima.html

    from pyramid.arima import auto_arima
    res = auto_arima(data, ...)
    res.summary()

Sample data `datasets.load_lynx` and `datasets.load_wineind`.

=== Making stationary

Test p-value with `test_stationarity()`. If p not small, so differencing

    df["a"]=df.a-df.a.shift()
    df["a"]=df.a-df.a.shift(12)

Or maybe multiple times of this.

=== VAR models

* usually need more `maxiter` due to large number of parameters

=== Tests on statsmodels state-space models

* http://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.mlemodel.MLEResults.test_normality.html
* http://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.mlemodel.MLEResults.test_heteroskedasticity.html
* http://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.mlemodel.MLEResults.test_serial_correlation.html

== Sympy

    from sympy import *
    init_printing()

    x, y, sigma = symbols("x y sigma")


[cols="m,d"]
|===
| simplify(..)                              | Simplify expression
| oo                                        | Infinity
| diff(expr, x)                             | Derivative
| integrate(expr, x)                        | Integrate
| plot(expr)                                | Plot with Matplotlib
| E                                         | Eulers number
| factor(..)                                | Find simple factor version of expanded polynomial
| cancel(../..)                             | Cancel factors in a fraction
| together(../.. + ../..)                   | Common denominator
| a.subs(x, y+1)                            | Substitute an expression
| symbols("..", positive=True)              | Helps simplifying square root etc. better
| expr1.rewrite(expr2)                      | Try to rewrite with other expression
| trigsimp(..)                              | Simplify trigonometrics
| expand_log(..)                            |
| expand_trig(..)                           |
| expand_*                                  |
| Symbol(.., real=True)                     | Assumptions about variables http://docs.sympy.org/latest/modules/core.html?highlight=assumptions#module-sympy.core.assumptions[Ref]
|===

* http://nbviewer.jupyter.org/url/www.inp.nsk.su/~grozin/python/sympy.ipynb[Notebook 1]

== Fourier transform

If your effective range of a discrete signal array is `[-a/2, a/2]` unlike what a discrete Fourier transform assumes `[0, a]`, you need to shift your data to get common results.
For example, to get a Gaussian from a (centered) Gaussian with a discrete transform:

    def four(y):
        return np.fft.fftshift(np.fft.fft(np.fft.fftshift(y)))

Without shifting, you get additional oscillations (due to the phase shift).


== Parse command line arguments

    parser=argparse.ArgumentParser()
    parser.add_argument("file_to_test")
    args=parser.parse_args()
    args.file_to_test


== Threaded CSV

    from time import sleep
    from csv import DictReader
    from Queue import Queue
    from threading import Thread

    q = Queue()
    workers = []

    def worker():
        while True:
            line = q.get()
            print "processing: %s\n\n" % line
            q.task_done()

    for i in range(10):
        t = Thread(target=worker)
        t.setDaemon(True)
        workers.append(t)
        t.start()

    with open('myfile.csv','r') as fin:
        for line in DictReader(fin):
            q.put(line)

    q.join()

* also check out https://pypi.python.org/pypi/xfork[xfork]

== Parallel execution

* See my IPython Ref for `ipyparallel`
* Cython tricks https://homes.cs.washington.edu/~jmschr/lectures/Parallel_Processing_in_Python.html[Link]
* `joblib` possible
* `dask.pipeline.Pipeline` (like sklearn, but parallelizable)

=== IPyParallel

To install

    conda install ipyparallel
    ipcluster nbextension enable       # for Jupyter extension tab

* single/multiple program; multiple data; task farming
* components:
** Engine: extended kernel which connect to controller; listen to requests over network, run code, return result
** Controller: Hub + Schedulers; SPOC to access engines; Engine and Clients connect; Direct interface of LoadBalanced interface
** Hub: Center of cluster; track engine, connections, schedulers, clients, task requests/results; facilitate query of cluster state
** Scheduler: all actions on engines go through here; Async (whereas engines block)
* `Client` to connect to cluster
* `View` for each execution model
* ZeroMQ communication (current no good https://ipyparallel.readthedocs.io/en/latest/intro.html#security[Security], https://ipyparallel.readthedocs.io/en/latest/security.html[Security])
* IPython on https://ipyparallel.readthedocs.io/en/latest/process.html#ipython-on-ec2-with-starcluster[Amazon EC2]
* Hub https://ipyparallel.readthedocs.io/en/latest/process.html#database-backend[stores] all messages and results passed
* https://ipyparallel.readthedocs.io/en/latest/magics.html[IPython Magics] automatically available when you create `Client`
* https://ipyparallel.readthedocs.io/en/latest/dag_dependencies.html[DAG Dependencies]
* careful not to modify passed Numpy array until all jobs are done
* everything pickled; only bytes, np.array, memoryviews send as raw

==== Getting started

* https://ipyparallel.readthedocs.io/en/latest/intro.html#getting-started[Getting Started]
* https://ipyparallel.readthedocs.io/en/latest/process.html#parallel-process[Start] controller and some engines
* automated with `ipcluster`; manual with `ipcontroller` and `ipengine`
* https://ipyparallel.readthedocs.io/en/latest/details.html[Details]
* load balanced view is better, since direct view has fixed distribution of tasks irrespective of runtime (when using `.map`)

    !ipcluster start -n 4 --debug

    import ipyparallel as ipp
    par = ipp.Client()
    view = par.load_balanced_view()

    data = range(20)

    ##### If imports needed
    with view.sync_imports():
        import numpy

    ##### Map version
    @view.parallel()
    def square(x):
        return x**2

    res = square.map(data)      # Split into num_engines and started only *once*

    ##### External functions
    @view.parallel()
    @ipp.require(func1, func2, "numpy", ...)
    def func(x):
        ... func1 ...

    ##### Partitions with progress
    res=[]
    part_len=3
    for data_part in cytoolz.partition_all(part_len, tqdm(data)):
        res.extend(square.map(data_part))

    ##### Function call version
    @view.parallel()
    def square(xs):                 # takes *multiple* (due to function-call-version)
        res=[]
        for x in xs:
            res.append(x**2)
        return res

    res = square(data)              # Split into num_engines and started once
    # print(res) : <AsyncMapResult: square>

    ##### See result
    if res.ready():
        # print(res) : <AsyncMapResult: square:finished>
        print(res.get())

    ##### Individual addressing
    res = par[0].apply(func, *args, **kwargs)

==== Commands

[cols="m,d"]
|===
| par.ids                                   | Show indices of engines to check
| view = par[:]                             | DirectView on all engines
| view.apply_sync(func)                     |
| async_res = view.apply(func, *args, **kwargs)     | `AsyncResult` (default non-blocking) https://ipyparallel.readthedocs.io/en/latest/direct.html#apply[Guide]
| if async_res.ready(): +
      res = async_res.get()                 | https://ipyparallel.readthedocs.io/en/latest/asyncresult.html#parallel-asyncresult[Guide]
| res = async_res.get(_max_secs_)           | Ask for result waiting max some seconds
| view.map(func, data)                      |
| view["a"]=..                              | Send data
| a = views["a"]                            | Receive data
| view.execute("<code>")                    |
| view.wait([asr1, asr2, ..])               | Wait for all async results
| view.apply_sync(..)                       | Blocking
| view.push({"a":.., ..})                   | Push data
| view.pull("a")                            | Pull data
| view.update                               |
| view.get                                  |
| view.scatter("a", [..])                   | Scatter data to engines as variable `a` (list is partitioned equally)
| view.gather("a").get()                    | Gather data to client
| @view.remote() +
  def func(..)                              | Decorate function
| @view.parallel() +
  def func(..)                              | Scatter data and distribute
| par_func.map([..])                        | Parallel `@v.parallel` function
| view.scatter("x", [..]) +
  %px y=[f(x_) for x_ in x] +
  y = view.gather("y")                      | Parallel execution of a single command
| %pxresult                                 | Get result (of last line?)
| %pxconfig --noblock                       | Make non-blocking
| %%px ..                                   | Run this same cell on all (or specified) engines https://ipyparallel.readthedocs.io/en/latest/magics.html#px-cell-magic[Guide]
| %autopx                                   | Run all following cells on all engines (until `%autopx` again)
| with view.sync_imports():
      import numpy                          | Import on all
| @ipp.require("re")
  def func(..)                              | Require imports
| view.activate([suffix])                   | Link activate magics to this view (suffix to create multiple magics)
| ipp.bind_kernel()                         | Bind to engine kernel for debugging
| %px %qtconsole                            | Start consoles for all if local
| view = parcl.load_balanced_view()         | Default `LoadBalancedView`
| ar.get_dict()                             | Engine ID as key
| for x in ar                               | Iterate of async results as they come in
|===

* https://ipyparallel.readthedocs.io/en/latest/api/ipyparallel.html[API]
* add `_sync` to make calls blocking
* `AsyncResult` is superset of `multiprocessing.pool.AsyncResult`
* usually imports in functions or by `view.execute("import ..")`
* most commands accept parameters `block` and `targets` (engine ids)
* `scatter` and `gather` between engines and client; for inter-engine communication need another system (e.g. https://ipyparallel.readthedocs.io/en/latest/mpi.html[MPI])
* `sync_imports()` does not work with renaming `import A as B`; use `B = A` instead
* `%debug` after `CompositeError` works
* plots will also be collected by `%px`
* Magics linked to a particular `View`; change with `view2.activate()`; can
* AsyncResults has more https://ipyparallel.readthedocs.io/en/latest/asyncresult.html#metadata[meta info] (time since submission, ...)
* Tasks are stored in a https://ipyparallel.readthedocs.io/en/latest/db.html[database]

==== Dependency handling

* https://ipyparallel.readthedocs.io/en/latest/task.html#dependencies[Guide Dependency]
* Functional dependencies, Graph dependencies
* Retries possible
* Different schedulers possible


== Doctests

    def func(..):
        """
        >>> func(1,
        ...      2)
        'expected_result'
        """

    doctest.testmod()


* however note that `testmod` will recursively inspect for attribute existence. this will break for `colorful` since it's attributes trigger special behaviour


== SQL in Python

    from sqlalchemy import *
    eng=create_engine("mysql+pymysql://root:password@localhost")
    eng=create_engine("sqlite:///:memory:")
    conn=eng.connect()

    from sqlalchemy import create_engine
    engine = create_engine('sqlite:///:memory:', echo=True)
    conn = engine.connect()
    conn.execute(...)

    conn.execute("insert into ... values (%s, %s, ...)", list(df[[..]].itertuples(False)))

    import pymysql
    conn=pymysql.connect(host="localhost", user="root", passwd="passwd")
    cur=conn.cursor()
    cur.execute("select * from db.tbl")
    fields=list(map(itemgetter(0),cur.description))
    for row in cur.fetchall():
        ...
    conn.commit() # after writing
    conn.close()

    conn = sqlite3.connect('example.db')
    result=conn.execute("...")
    conn.commit()
    conn.close()

    conn.executemany("...?..", [(..,..), (.., ..), ..])

    sqlite3.connect(':memory:', detect_types=sqlite3.PARSE_DECLTYPES)

    conn.execute("begin")   # for transaction
    ...
    conn.execute("commit")


* Placeholder (for `postgresql://`): `%s`


SQLAlchemy: removes column names parts before a dot due to bugs in the SQLite driver as a workaround use

    res = conn.execution_options(sqlite_raw_colnames=True).execute("select * from test")

or

    eng = create_engine("sqlite://", execution_options={"sqlite_raw_colnames": True})

    http://docs.python.org/library/sqlite3.html#sqlite3.Connection.iterdump to dump to memory

    import sqlite3
    from StringIO import StringIO
    def init_sqlite_db(app):
        # Read database to tempfile
        con = sqlite3.connect(app.config['SQLITE_DATABASE'])
        tempfile = StringIO()
        for line in con.iterdump():
            tempfile.write('%s\n' % line)
        con.close()
        tempfile.seek(0)

        # Create a database in memory and import from tempfile
        app.sqlite = sqlite3.connect(":memory:")
        app.sqlite.cursor().executescript(tempfile.read())
        app.sqlite.commit()
        app.sqlite.row_factory = sqlite3.Row

    >>> from sqlalchemy import create_engine
    >>> engine = create_engine('sqlite:///:memory:', echo=True)

== Run system command

Starting with Python 3.5 https://docs.python.org/3/library/subprocess.html#subprocess.run[subprocess.run] (`encoding=` from Python 3.6)

    import subprocess
    proc_res = subprocess.run("ls", stdout=subprocess.PIPE, encoding="utf8")
    out = proc_res.stdout

Return type https://docs.python.org/3/library/subprocess.html#subprocess.CompletedProcess[subprocess.CompletedProcess]


== Debugging

    import code; code.interact(local=locals())
    import pdb; pdb.set_trace()
    from IPython.core.debugger import Tracer; Tracer()()

* !check whether still up-to-date
* also consider new `breakpoint()` command in Python

    exc_type, exc_value, exc_traceback = sys.exc_info()
    filename, line number, function name, text=traceback.extract_tb(exc_traceback)[0] # 0 to get first element

== Encoding Textfiles

|===
| locale.getpreferredencoding()             | what `open()` will use by default; not utf8 on Windows!
| sys.getfilesystemencoding()               | default for most operations that both require an encoding and involve textual metadata in the filesystem (e.g. determining the results of os.listdir())
|===

* use `open(..., encoding="utf")` explicitely or it may fail in Windows when using UTF8 files
* `locale.getpreferredencoding()`:
** is not the same as `sys.getfilesystemencoding()`
** unaffected by PYTHONIOENCODING
* https://python-notes.curiousefficiency.org/en/latest/python3/text_file_processing.html
* new https://www.python.org/dev/peps/pep-0540/[UTF8 Mode] in Python 3.7: use UTF8 everywhere; also when in POSIX locale


=== BOM bytes in files

* only for unicode
* FFFE or FEFF : UTF-16 LE or BE
* or EFBBBF : utf8; special bytes that hopefully are not meaningful in other encodings
* UCS-2=UTF-16 (except TFF16 some more codepoints)
* UTF8 bytes: 0.. (<0x80); 110.. 10..; 1110.. 10.. 10..; 11110.. 10.. 10.. 10..
* Use `utf-8-sig`, if there is a BOM byte
* utf16 automatically does BOM (don't use utf-16LE unless BOM is missing)

=== Encoding detection

    file -i <file>
    iconv  # to convert
    uchardet
    chardet # (python)

== General Python

    out = io.StringIO()
    print("TEXT", file=out)
    val = out.getvalue()            # .read() will be empty

== Terminal color

* https://misc.flogisoft.com/bash/tip_colors_and_formatting
* https://stackoverflow.com/questions/4842424/list-of-ansi-color-escape-sequences
* you can display a color overview with `humanfriendly --demo`
* color names from https://en.wikipedia.org/wiki/X11_color_names[X11] (e.g. use `colorful.darkOrange`)

== Time series

* autocorrelation plots:
** https://www.statsmodels.org/dev/generated/statsmodels.graphics.tsaplots.plot_acf.html[statsmodels plot_acf]
** https://www.statsmodels.org/dev/generated/statsmodels.graphics.tsaplots.plot_pacf.html[statsmodels plot_pacf]
** better than `plt.acorr` and `pandas autocorrelation_plot`

=== Libraries

    # https://github.com/timofurrer/colorful (-> will use colorama library for Windows)
    import colorful

    # use one of the following in Jupyter to enforce color
    #colorful.use_8_ansi_colors()
    #colorful.use_256_ansi_colors()
    #colorful.use_true_colors()

    print(f"{colorful.coral(text)}")    # needs str conversion usually

Suggested colors are:

    blue
    purple
    lightSlateBlue
    cornflowerBlue
    dodgerBlue
    deepSkyBlue
    skyBlue

    aquamarine3
    cyan3
    lightSeaGreen
    seaGreen
    yellowGreen
    limeGreen

    magenta
    red
    hotPink
    violet
    plum
    lightPink

    tomato
    coral
    orange
    gold

image::RGB-txt hand-picked colors.png[/etc/X11/rgb.txt]
image::RGB-txt best hand-picked colors.png[/etc/X11/rgb.txt]

These color were found in Jupyter light theme. Jupyter dark theme could have some other colors becoming interesting.

* `blessings` did not work with Windows (https://github.com/erikrose/blessings/issues/21[Jira]) due to curses
* `colorama` seems to be basis for color on Windows
* seems uncomfortable: `sty`, `colored`

== XPath

=== Select all nodes with exceptions

    /root/*[not(self::script)]
    /root/(* except a)                     # XPath 2.0

=== Python

    page = lxml.html.soupparser.fromstring(htmltext)
    hits = page.xpath(".//*[not(self::script) and contains(text(),"searchword")]")
    hit = hits[0]
    elem = hit.getroottree().getpath(hit)
    text = elem.text

== Hierarchical modelling

http://nbviewer.jupyter.org/github/fonnesbeck/multilevel_modeling/blob/master/multilevel_modeling.ipynb

    with Model() as varying_intercept:
        # Priors
        mu_a = Normal('mu_a', mu=0., tau=0.0001)
        sigma_a = Uniform('sigma_a', lower=0, upper=100)
        tau_a = sigma_a**-2

        # Random intercepts
        a = Normal('a', mu=mu_a, tau=tau_a, shape=len(set(county)))
        # Common slope
        b = Normal('b', mu=0., tau=0.0001)

        # Model error
        sigma_y = Uniform('sigma_y', lower=0, upper=100)
        tau_y = sigma_y**-2

        # Expected value
        y_hat = a[county] + b * floor_measure

        # Data likelihood
        y_like = Normal('y_like', mu=y_hat, tau=tau_y, observed=log_radon)

    with varying_intercept:
        step = NUTS()
        varying_intercept_samples = sample(2000, step)

    from pymc3 import forestplot, traceplot, summary

    plt.figure(figsize=(6,10))
    forestplot(varying_intercept_samples, vars=['a'])

    traceplot(varying_intercept_samples[-1000:], vars=['sigma_a', 'b'])

    summary(varying_intercept_samples[-1000:], vars=['b'])

Or varying different:

    with Model() as varying_slope:
        # Priors
        mu_b = Normal('mu_b', mu=0., tau=0.0001)
        sigma_b = Uniform('sigma_b', lower=0, upper=100)
        tau_b = sigma_b**-2

        # Model intercept
        a = Normal('a', mu=0., tau=0.0001)
        # Random slopes
        b = Normal('b', mu=mu_b, tau=tau_b, shape=len(set(county)))
        ...
        # Expected value
        y_hat = a + b[county] * floor_measure
        ...


    with Model() as varying_intercept_slope:
        # Priors
        mu_a = Normal('mu_a', mu=0., tau=0.0001)
        sigma_a = Uniform('sigma_a', lower=0, upper=100)
        tau_a = sigma_a**-2

        mu_b = Normal('mu_b', mu=0., tau=0.0001)
        sigma_b = Uniform('sigma_b', lower=0, upper=100)
        tau_b = sigma_b**-2

        # Random intercepts
        a = Normal('a', mu=mu_a, tau=tau_a, shape=len(set(county)))
        # Random slopes
        b = Normal('b', mu=mu_b, tau=tau_b, shape=len(set(county)))
        ...
        # Expected value
        y_hat = a[county] + b[county] * floor_measure
        ...

Or making coefficients be a model of a new variable:

    with Model() as hierarchical_intercept:
        ...
        # County uranium model for slope
        gamma_0 = Normal('gamma_0', mu=0., tau=0.0001)
        gamma_1 = Normal('gamma_1', mu=0., tau=0.0001)

        # Uranium model for intercept
        mu_a = gamma_0 + gamma_1*u
        # County variation not explained by uranium
        eps_a = Normal('eps_a', mu=0, tau=tau_a, shape=len(set(county)))
        a = Deterministic('a', mu_a + eps_a[county])

        # Random slope
        b = Normal('b', mu=0, tau=0.001)
        ...
        # Expected value
        y_hat = a + b * floor_measure
        ...

For contextual effects/correlations among levels (correlation between individual-level variables and group residuals) include group average (here: for intercept)

    with Model() as contextual_effect:
        ...
        # County uranium model for slope
        gamma_0 = Normal('gamma_0', mu=0., tau=0.0001)
        gamma_1 = Normal('gamma_1', mu=0., tau=0.0001)
        gamma_2 = Normal('gamma_2', mu=0., tau=0.0001)

        # Uranium model for intercept
        mu_a = gamma_0 + gamma_1*u + gamma_2*xbar[county]

        # County variation not explained by uranium
        eps_a = Normal('eps_a', mu=0, tau=tau_a, shape=len(set(county)))
        a = Deterministic('a', mu_a + eps_a[county])

        # Random slope
        b = Normal('b', mu=0, tau=0.001)
        ...
        # Expected value
        y_hat = a + b * floor_measure
        ...

Errors from cross-validation:

* unpooled 0.86
* pooled 0.84
* multilevel 0.79

Can make predictions for old or a completely new group.

== Regex

=== Character classes

* https://www.regular-expressions.info/refcharclass.html
* need escaping with backslash: `^-]\`
* some support subtract classes: `[a-z-[aeiuo]]`
* some support intersections: `[a-z&&[^aeiuo]]`

|===
| [:alpha:]                                 |
| [:^alpha:]                                |
| [:d:], [:s:], [:w:]                       | digit, space, word
| [:u:], [:l:]                              | Upper-/lower-case letters
| \p{...}                                   | POSIX class
| [[=e=]]                                   | Equivalence class (here all `e` accents)
|===

=== Special regex

* `\W+ | (?<!\w) \w{1,2} (?!\w)` (ignore whitespace): Matches inverse of long words (use `(...)+` to make it single matches)
** alternatively `(\b \w{1,2})? (\W+ | $)`

== Pandas cookbook

|===
| s.str.contains(text, case=False)              | Search for substring (regex) https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.contains.html[Ref]
|===

== Write Zipped Files

    with gzip.open("test.gz", "wt", newline="") as file:
        writer = csv.writer(file)

== Monkey Patch

    def override(p, methods):
        oldType = type(p)
        newType = type(oldType.__name__ + "_Override", (oldType,), methods)
        p.__class__ = newType

    class Test(object):
        def __str__(self):
            return "Test"

    def p(self):
        print(str(self))

    def monkey(x):
        override(x, {"__del__": p})

    a=Test()
    b=Test()
    monkey(a)
    print "Deleting a:"
    del a
    print "Deleting b:"
    del b

== Whoosh text indexing

    from whoosh.fields import Schema, TEXT, ID
    from whoosh.index import create_in

    schema = Schema(filename=ID(stored=True), content=TEXT)
    ix = create_in("~/webpage_whoosh_index/", schema)

    webpage_dir=Path("~/documents")
    writer=ix.writer()
    for file in tqdm(list(webpage_dir.glob("*"))):
        if not file.is_file():
            continue
        text=file.open().read()
        writer.add_document(filename=str(file),
                            content=text,
                           )
    writer.commit()

    from whoosh.query import *

    with ix.searcher() as searcher:
        results=searcher.search(Term("content", "WORD"))
        display(results)

== Image processing

=== Segmentation

    from skimage.segmentation import felzenzwalb, clear_border, mark_boundaries

    seg=felzenszwalb(img, min_size=200, scale=500)
    seg=clear_border(seg)
    plt.imshow(mark_boundaries(img, seg), cmap=plt.cm.Set1_r)

== System information

    print os.environ.get( "USERNAME" )
    win32api.GetUserName()
    win32api.GetUserNameEx (win32con.NameSamCompatible)
    getpass.getuser()

== Web scraping

=== Selenium screengrabber

    from selenium import webdriver
    browser=webdriver.Firefox()
    browser.get(url)
    browser.save_screenshot("file.png")
    elems=browser.find_elements_by_xpath(xpath)
    links=[e.get_attribute("href") for e in elems]
    browser.quit()
    e.get_attribute("innerHTML")  # if hidden HTML (not visible on page; greyed out in Firebug)

=== Headless browsers

* -> use all with Scrapy
* Chrome
* Splash

== Misc

You can use `scipy.signal.convolve` and an artificial `[1,1,1],[1,100,1],[1,1,1]` filter to do the "Game of Life" with arrays http://nbviewer.jupyter.org/gist/jiffyclub/3778422[Ref].

== Convolution

    np.random.seed(123)
    x=np.random.uniform(size=(20,11))
    k=np.random.normal(size=(4,11))

    conv=convolve2d(x, k,  mode="same", boundary="wrap")

    fx=fft2(x)
    fk=fft2(k, s=x.shape)

    inv=ifft2(fx*fk)

    m=-k.shape[0]//2+1
    n=-k.shape[1]//2+1
    inv=np.roll(np.roll(inv, m, axis=0), n, axis=1)

    assert np.all(np.all(np.isclose(inv, conv)))

== Python Profiling

    import cProfile
    pr = cProfile.Profile()
    pr.enable()

    ...

    pr.disable()
    pr.dump_stats("run.prof")

    !snakeviz run.prof

== Time

    time.strftime("%H:%M:%S", time.localtime(time.time()))

    midnight = dt.time.min

=== Seconds since midnight

    seconds = ((t - t.replace(hour=0, minute=0, second=0, microsecond=0)).total_seconds()

You cannot subtract `dt.time()` instances.

== Sympy

=== Integration

    from sympy import *
    init_printing()
    a, b=symbols("a b")
    simplify(integrate(a, (a, -oo, oo)))

== Plotting

== Rotate labels

    for ax in axs.ravel():
        ax.xaxis.label.set_rotation(90)
        ax.yaxis.label.set_rotation(0)
        ax.yaxis.labelpad = 150

== Linked Data Histogram

    %%opts Points [tools=["lasso_select"]]

    def dynhist(points, color):
        def hist(index):
            selected = points.iloc[index]

            return hv.Histogram(np.histogram(selected["x"], bins="doane", density=True)).opts(
                fill_color=color, alpha=0.3, framewise=True,
            )

        selection = hv.streams.Selection1D(source=points)

        return hv.DynamicMap(hist, streams=[selection])


    points0 = hv.Points(data.query("col==0"), kdims=["x", "y"])
    points1 = hv.Points(data.query("col==1"), kdims=["x", "y"])

    selection1 = hv.streams.Selection1D(source=points1)

    (
        (points0.opts(color="black") * points1.opts(color="coral"))
        + (dynhist(points0, "black") * dynhist(points1, "coral"))
    )

== IPyVolume

    ipv.figure()
    ipv.quickscatter(dd_map[cols[0]], dd_map[cols[1]], dd_map[cols[2]], color=[...], size=1)
    ipv.selector_default()
    ipv.show()

    idx_selected = ipv.gcf().scatters[0].selected

== Spreadsheets

* Pyspread (but buggy dependencies last time)
* LibreOffice access:
** http://christopher5106.github.io/office/2015/12/06/openoffice-libreoffice-automate-your-office-tasks-with-python-macros.html[Tutorial]
** https://github.com/lfagundes/oosheet
** https://pypi.org/project/pyoo/

== Streaming Plots

    %%opts .. {+framewise}

    from holoviews.streams import Pipe

    def make_plot(data): # full data
        return ....options(framewise=True)

    pipe=Pipe(data=[])  # use correct data format here or plots will not show

    plot=hv.DynamicMap(make_plot, streams=[pipe])

    display(plot)
    -> pipe.send(..)

== Shell commands

|===
| find . -name '*part*'                     |
| -iname                                    | case-insensitive
| -regex                                    | regex
| -iregex                                   | regex (case-in.)
| grep -R 'str'                             | Search recursively
|===

== UNSORTED

=== Convex hull of points

    from scipy.spatial import ConvexHull

    def encircle(x,y, ax=None, **kw):
        p = np.c_[x,y]
        hull = ConvexHull(p)
        poly = plt.Polygon(p[hull.vertices,:], **kw)
        ax.add_patch(poly)

=== Pandas

    df.isna().sum()    # check missing values


Join string columns:

    reduce(add, [..])

Reading large files:
* Parquet
** engine="fastparquet", compression="uncompressed" ("snappy" not implemented): slightly faster than Pickle
** engine="pyarrow" (default): only a bit faster than Pickle, but Snappy compression small ("uncompressed" not supported); cannot do float16



=== Various

Powerset:

    for num in range(len(a)) for choice in combinations(a, num)

== BeakerX

* has a lot of toolkit features
* useful: table display
* but no docs; only notebooks: https://nbviewer.jupyter.org/github/twosigma/beakerx/blob/master/doc/python/TableAPI.ipynb

    df=pd.DataFrame(np.random.rand(100, 10), columns=list(string.ascii_letters[:10]))
    table = TableDisplay(df)
    for col in df.columns:
        table.addCellHighlighter(TableDisplayCellHighlighter.getUniqueEntriesHighlighter(col, HighlightStyle.SINGLE_COLUMN))
    table

