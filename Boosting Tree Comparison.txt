== General

* XGBoost:
** Exclusive Feature Bundling (EFB) to group similar columns

== Speed

* LightGBM: Gradient-based one-side sampling (GOSS) to filter data for split (faster than XGBoost approach)
* XGBoost: pre-sorted, histogram based for best split
* GOSS: Gradient represents the slope of the tangent of the loss function, so logically if gradient of data points are large in some sense, these points are important for finding the optimal split point as they have higher error
* CatBoost: slower?
** vs LightGBM: but predicts faster; less overfitting (lower train score); train only slightly larger(?)

== Categorical coding

* LightGBM: special algorithm (http://www.csiss.org/SPACE/workshops/2004/SAC/files/fisher.pdf)
* CatBoost: avg_target (prior smoothed); multiple random shuffling?
* XGBoost: none

== Parameters

=== XGBoost

* learning_rate (eta): 0.01-0.2
* max_depth
* min_child_weight
* for speed:
** colsample_bytree
** subsample
** n_estimators (overfitting if too high)

=== CatBoost

* learning_rate (automatic!)
* depth: 1-10 (up to 16)
* l2-leaf-reg: pos. integer; L2 regularisation
* one_hot_max_size: max size for one-hot encoding
* missings need to declared as Min or Max!
* automatically creates combinations of feats (cat/cat, cat/num?)
* cross validation: https://catboost.ai/docs/concepts/python-reference_cv.html ; but cannot be used for tuning (cannot get best model) -> can only see number of iterations
* predefined datasets to test: https://catboost.ai/docs/concepts/python-reference_datasets.html
* faster creation of Pool from FeatureData (https://catboost.ai/docs/concepts/python-features-data__desc.html)
* visualize metrics from stored files: https://catboost.ai/docs/concepts/python-reference_catboostipythonwidget.html
* faster way to pass features https://catboost.ai/docs/concepts/python-reference_pool.html
* numpy float32 also fast if only numeric
* models be be exported and applied on many way: https://catboost.ai/docs/concepts/applying__models.html
* loss functions: https://catboost.ai/docs/concepts/loss-functions.html
* feature importance: https://catboost.ai/docs/concepts/fstr.html#fstr
* shap values: https://catboost.ai/docs/concepts/shap-values.html
* feat analysis graph(!): https://catboost.ai/docs/concepts/feature-analysis-graph.html ; but only OH cat cols (-> maybe increase max_onehot)
* feat interactions(!): https://catboost.ai/docs/concepts/feature-interaction.html
* object importance: https://catboost.ai/docs/concepts/ostr.html
* use Pandas Categorical type
* https://catboost.ai/docs/features/visualization_jupyter-notebook.html
* .fit(..., plot=True)  | plot while training
* model.randomized_search(...)
* tensorboard can also display live plots
* cat feat have incremental encoding on train, but fixed encoding on val
* for weights use Pool(...)
* float and NaN value forbidden for cat features for consistency logic reasons
* To use sklearn gridsearch use cat_features:

    model = catboost.CatBoostRegressor(cat_features=[0,1,2])
    grid_search = sklearn.model_selection.GridSearchCV(model, param_grid)
    
* reduce size of final model
* missing values: https://catboost.ai/docs/concepts/algorithm-missing-values-processing.html

==== Tuning

* check for strong over/under fitting
* find best iterations by early stopping (use_best_model)
* if train does not converge -> increase learning_rate
* overfitting detect -> decrease learning_rate
* tree depth 6-10
* try l2_leaf_reg, random_strength, bagging_temperature
* for best quality increase max_bin (border_count) to 254
* SymmetricTree about 10x faster than regular tree and good quality; but also try other growing strategies
* increase pre-quantization for Golden features

==== Performance

save quantized data:

    train_dataset = Pool(train_data, train_labels)
    train_dataset.quantize()
    train_dataset.save(quantized_dataset_path)

=== LightGBM

* may overfit on small data(?)

Tune parameters

* num_leaves: larger <2^max_depth
* min_data_in_leaf (min_data, min_child_samples)
* max_depth: tune later(?)
* bagging_fraction(?)
* lambda_l2

Method

1. fix learning_rate
1. num_leaves, min_data_in_leaf
1. min_gain_to_split
1. bagging_fraction, bagging_freq, feature_fraction
1. lambda_l2
1. reduce learning_rate

Set parameters

* random_seed: set this; by maybe other specific seeds already fine?
* early_stopping: set to 100?
* n_iter (n_estimators,..): just a high value (early_stopping instead)
* cat_feature="auto": pandas cat by default; specify list otherwise

If overfitting:

* feature_fraction (colsample_bytree,..): [0.4, 1]; or speed up training
* bagging:
** bagging_fraction=1.0 (subsample, bagging): how much data to use in each iteration
** pos_bagging_fraction=1.0, neg_bagging_fraction=1.0: for imbalanced
** bagging_freq=0
* num_leaves smaller, lambda_l1, lambda_l2, min_gain_to_split
* max_bin: smaller
* min_data_in_bin=3: if one-data-one-bin issue
* min_data_in_leaf=20
* min_sum_hessian_in_leaf
* bagging_fraction

For speed:

* bagging_fraction, bagging_freq
* feature_fraction
* max_bin: use small
* parallel training https://github.com/Microsoft/LightGBM/blob/master/docs/Parallel-Learning-Guide.md?source=post_page---------------------------
* special modes:
** top_k: when parallel training
** max_conflict_rate=0

Other suggested somewhere:

* lambda_l1=0 (reg_alpha):
* lambda_l2=0 (reg_lambda):
* only one of lambda high(?)
* min_gain_to_split=0

Not tune:

* learning_rate=0.1: just lower if time permits
* max_bin=255: only if memory pressure

Sparse data:

* bin_contruct_sample_cnt=200000 (subsample_for_bin): how much to sample to construct histograms; larger

Interesting:

* lower learning rate: 0.001?
* device_type="gpu"
* predict_contrib=false: with true will predict SHAP values for feature per prediction
* metric: for eval set (https://lightgbm.readthedocs.io/en/latest/Parameters.html#metric-parameters)
* for filebase training with `train.txt`: use `train.txt.weight` for weights
* boosting="gbdt": also random forest `rf`, Dropout+MART `dart`, Gradient based one-side sampling `goss`

Objective:

* num_class=1: for multi-class
* is_unbalance=false: for imbalanced
* scale_pos_weight=: weight of positive class (use instead of `is_unbalance`)
* many parameters for special loss functions

Other:

* https://lightgbm.readthedocs.io/en/latest/Parameters.html
* seed parameters for everything
* boosting="gbdt": boosting type
* tree_learner="serial"
* min_sum_hessian_in_leaf=1e-3
* first_metric_only: use only first metric to early stopping
* max_delta_step=0
* `dart` only:
** drop_rate=0.1
** max_drop=50
** skip_drop=0.5
** xgboost_dart_mode=False
** uniform_drop=False
* `goss` only:
** top_rate
** other_rate
* for categorical:
** max_cat_threshold=32: limit number of categorical threshold points to avoid overfitting
** min_data_per_group=100
** cat_l2=10
** cat_smooth=10
** max_cat_to_onehot=4: one-vs-other-split instead
* top_k
* monotone_constraints
* feature_contri
* forced_splits_filename
* refit_decay_rate=0.9
* cegb_tradeoff=1
* cegb_penalty_split=0
* cegb_penalty_feature_lazy=0,0
* cegb_penaly_feature_coupled
* min_data_in_bin=3: min data in bin

Multiple IO parameters:

* verbose=1: >1 for debug
* snapshot_freq=-1, output_model=LightGBM_model.txt: snapshot model after so many iterations; CLI mode only
* two_round=true: if data too big for memory
* predict_leaf_index

GPU:

* use small max_bin=63
* use float32(?)

Other parameters:
* network parameters
* GPU parameters
* continued training

For imbalanced:

* https://github.com/Microsoft/LightGBM/issues/695

* num_leaves: [7, 4095]
* max_depth: [2, 63] and infinite (I personally saw metric performance increases with such 63 depth with small number of leaves on sparse unbalanced datasets)
* scale_pos_weight: [1, 10000] (if over 10000, something might be wrong because I never saw it that good after 5000)
* min_child_weight: [0.01, (sample size / 1000)] if you are using logloss (think about the hessian possible value range before putting "sample size / 1000", it is dataset-dependent and loss-dependent)
* subsample: [0.4, 1]
* bagging_freq: only 1, keep as is (otherwise overfitting)
* is_unbalance: false (make your own weighting with scale_pos_weight)
* USE A CUSTOM METRIC (to reflect reality without weighting, otherwise you have weights inside your metric with premade metrics like xgboost)


== Laurae
https://sites.google.com/view/lauraepp/parameters

* learning_rate: 0.05 (larger for tuning); do not tune
* nrounds, num_iterations: use large with early stopping
* num_parallel_tree, num_iterations (when booster=rf): like RF, set to 500; not so good to use
* early_stopping_rounds: 50, larger is better; too large may overfit
* max_depth [1,inf]: usually [3, 12]; xgboost gpu max 16; unlimited depth is long one-sided rule expected
* max_leaves, num_leaves: tune together with max_depth; 255, usually {15, 31, 63, 127, 255, 511, 1023, 2047, 4095}. Tips: adjust depth accordingly by allowing a slightly higher depth than the theoretical number of leaves (2^depth-1)
* subsample ]0,1]: typical 0.7


== CatBoost custom metric

https://catboost.ai/docs/concepts/python-usages-examples.html#custom-loss-function-eval-metric

    class CustomMetric(object):
       def get_final_error(self, error, weight):
           return 0.0
       def is_max_optimal(self):
           return True
       def evaluate(self, approxes, target, weight):
           # approxes - list of list-like objects (one object per approx dimension)
           # target - list-like object
           # weight - list-like object, can be None
           return 0.0, 0.0
           
    CatBoostClassifier(eval_metric=CustomMetric())
    
    
custom_metric='AUC:hints=skip_train~false', metric_period=20