= Data Science Notes

:stem:


== Geographical

=== Uber H3 grid
https://eng.uber.com/h3/
* better buckets (not coordinates)
* smooth gradients?
* hexagons: since cannot jump across corner and have sqrt(2) distance (if time is discrete); for hexagons all neighbors same distance away (there are no corner neighbors)
* just harder to sub-divide hexagons into hexagons (some corner of lower level will be outside higher level)
* whole earth tiled: use projection to icosahedron; tile with hexagons; just need pentagons at icosahedron corners


== Whats wrong with convolutional nets
http://techtv.mit.edu/collections/bcs/videos/30698-what-s-wrong-with-convolutional-nets
-> REWATCH!
* no notion of entity
* too few levels of structure (only neurons, layers, whole net)
* want: layer; group subsets; different activities represent different properties of same entity
* one entity per "mini-column"
* capsule:
** if entity is present; describe properties (pose, orientation,...)
** take predictions from low level capsules; look for predictions that agree tightly
** output probability that present -> output cluster center of properties (ANN cant do this well)
** high dim dont agree just by chance
* convnets:
** local features; replicated across space
** but interleaved pooling layers (only take most active one indep. of position) -> some translational invariance but not good
* pooling bad since:
** bad to psychology of shape perception
** ...
* neural nets arent good at high order parity problem (is it left hand or right hand)
* ...

== High cardinality

* linear models, FTRL, Vowpal wabbit, LbFFM, libFM, SVD

== Text mining

* tfidf, countvectorizers, word2vec, svd (dimensionality reduction). Stemming, spell checking, sparse matrices, likelihood encoding, one hot encoding (or dummies), hashing.

* for 5MB text, 6 letters context provide enough context for optimal compression

* Sliding suffix tree: https://arxiv.org/pdf/1801.07449v3.pdf

== Hyperparameter tuning

* Gaussian Processes: Interacting parameters
* Random Forest: Interacting param, Discrete param, Maybe easy config; smac lib
* Tree-structured Parzen estimator: Discrete param, Easy config; hyperopt lib
* GP may fail
* -> use Random Forest

== Sound processing

* Fourier transform, MFCC (Mel frequency cepstral coefficients), Low pass filters

== AA
https://www.youtube.com/watch?v=ek9jwRA2Jio&spfreload=10
* Empirical risk minimization consistent iff VC finite
* Reichenbach: X and Y dependent if X<-Z->Y; or directly X->Y
* X indep of non-descendants given parents
* functional causal model
* can we recover causal graph if prob known
* -> we can infer consistent graph with independences
* may contain accidental independencies
* Markov condition: X indep of non-descendants given parents
* Functional Causal Model:
** more specific information than Graphical model -> predict effect of interventions
** can argue about counter-factuals
** observables connected by arrows; determined by parents and unexplained (noises random; indep)
** Graphical model in this notations wouldn't be unique
* Question:
** by conditional independence testing you can infer class of graphs consistent with observed independencies; and will also contain correct graph
** track how noise spreads in graph
** problems:
    * distribution may contain accidental conditional independencies
    * if functions are complex, test for cond indep becomes arbitrarily hard
    * cant solve problem for 2 vars (since no conditional indep possible)
* with additive noise can find direction of causality for two variables
* semisupervised learning fails where prediction causal

== OpenCV
* Object detection:
** Haar cascades: more reliable looks for certain rectangular transitions
** Local binary pattern: faster; detect transition dark to light in any direction
* look at different scales
* use data stored in XML
* neither can detect rotated or flipped objects; need standardization
* gray=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
* cv2.equalizeHist first
* detectMultiScale; neighbors: same face detected some pixels shifted
* CV_HAAR_SCALE_IMAGE f or flags good
* eyes need to be standard height
* rectangle of cats should include much more than cat; no background -> no ears included
* for face recognition:
** Eigenfaces
** Fisherfaces: build average face plus differences
** local binary pattern histogram: fast; good for realtime training

== Intro to Gaussian processes
* Williams is bible
* http://videolectures.net/gpip06_mackay_gpb/
* similar application to ANN
* solve just with Gaussian distribution
* parallel coordinate plot of multiple variables (Gaussian)
* variables are high dim Gaussian
* marginalized variables: can just remove cols/rows in covariance matrix
* -> think of covariance matrix in Gaussian processes
* covariance matrix like modes; inverse covariance like forces
* like non-linear regression
* cov(yi,yj)=Gaussian(xi-xj)+diagonal term
* -> use for non-linear regression
* GP: collection of random variable s.t. joint distr. of any finite subset is Gaussian
* Bayes theorem for hyperparam
* have like infinite number of parameters
* automatic relevance determination: large sigma for that variable
* can do prediction of mean/var with (N+N^2)N* instead of (N+N*)^3 operations for inverting matrix [N number of training points; ]; full computations only for all covariances
* -> or even faster approximate answer
* need to chose covariance function
* linear regression possible; Brownian; can do 1 layer infinite ANN
* classification by sigmoid link of Gaussian
* special cases: RBF; Splines; ANN 1-layer
* problems:
** ill-conditioned
** N^3 hard for N>1000

== LeCun deep learning
http://videolectures.net/sahd2014_lecun_deep_learning/
* many effective paths through network
* almost like random matrix theory or high-order spherical spin glasses
* use rectification, contrast normalization,
* convnet on sliding window is cheap


* Regularization indep of data
* -> large alpha decreases dependence on data
* balance data loss + regularization loss -> should be tangent when sum is minimized
* Lp; p<1 non-convex -> only L1 sparse and convex

* gradient descent iteration like "fitting on error" in GBT

2014 Google Imagenet 7% error on top-5

NMF = finding simplex which contains all points (with many points at vertex)

Linear models in sklearn
*

== AA
https://www.youtube.com/watch?v=TB3axpIpCiQ
explain:
* what if change variable? sensitivity
* feature contribution; path approach
** feature importances
* compare subsets

== AA
https://www.youtube.com/watch?v=9Zag7uhjdYo
Learned from Kaggle

== Probabilistic programming in quantative finance
https://www.youtube.com/watch?v=MeKucat_gw8
* MCMC works for all right away
* approx posterior with MCMC
* -> PyMC3 package (uses Theano which compiles to C code)
* use NUTS() sampler
* t-distribution more stable and can give totally diff result; it's in the tails
* t-distribution more robust to outliers

== Winning data science competitions
https://www.youtube.com/watch?v=spYNo8TU0fs
* tuning GBM:
** 1000 trees and tune learning rate
** how many obs in leaf
** interaction depth 10+ ok; roughly number of leave nodes
* help GBM:
** for high cardinality features (zip codes, ...); slow if all one-hot or overfit
** convert into numerical with preprocessing - out-of-fold average, counts, ...
** use ridge regression and out-of-golf predication as input to GBM or blend
** use N-way interactions (e.g. even 7-way)
* dont use in sample prediction for stacking (overfitting first steps get high weights later) - since first model (early in stack) already used target -> use different data (e.g. half/half and swap around)
* strong interactions benefit from being explicitely defined for GBM (esp. ratio, sum, diff....)
* GLMnet:
** complements GBM well (since GLM global model)
** need work for missing, outliers, transformations, interactions
** work with small number of rows
* tricks text mining:
** tau package (R)
** L2 penalty
** N-grams
** many text-mining dominated by structured fields
** include trivial features (length, number of words, ...)
* encode categorical:
** use avg. response of level
** use counts of that category
** leave-one-out encoding: avg(Y) for same category rows apart from current row; for testing data just avg without leave out -> A0,A1,A1,A0 -> 0.6,0.3,0.3,0.6
** add random noise (factor 0.98-1.02 to features)
* VW good
* Allstate for 7 labels:
** baseline last quoted hard to beat
** model chain of models (assume you know one label part - predict next one)
** one model to decide whether to use baseline
* Time series:
** linear approach
** or translate to GBM

== Frequentism and Bayesianism
https://www.youtube.com/watch?v=KhAUfqhLakw
* multiple measurements as gaussian
* freq=weighted average with 1/sigma_i^2
* differences when:
** handling nuisance parameters (intermediate latent param)
** interpreting uncertainty
** incorporating prior info
** comparison and evaluation of models

Taking Humans out of the Deep Learning Loop
* https://www.youtube.com/watch?v=VG2uCpKJkSg

== Price elasticity

Usually price elasticity is defined as

asciimath:[epsilon=(d ln N)/(d ln P)]

If there are no fixed costs per item solf the the profit is

asciimath:[S=N P]

and the elasticity

asciimath:[dS/dP=N(epsilon+1)]

If there are fixed costs and the profit is

asciimath:[S=N(P-Delta)]

then one could write

asciimath:[dS/dP=N(epsilon (1-Delta/P)+1)]

or alternatively use a shifted price asciimath:[P*=P-Delta] and do the same analysis. Note that the plots with asciimath:[ln(P-Delta)] are different.


== Introduction to information theory
http://videolectures.net/mlss09uk_mackay_it/

=== Error correction
* Shannon: reliable communication over unreliable channel
* care about error correct ratio and transmission ratio
* can do much better than repetition codes
* some of the first: (7,4) Hamming -> 3 parity bits, linear block code; corrects 1/7 errors, transmits 4 bits
* BCH(1013,101)
* !below the capacity of channel (rate): an arbitrary small error is theoretically possible
* for channel which flips with probability f:
** C_CSB(f)=1-H_2(f)
** H_2(f)=-f ld(f)-(1-f)ld(1-f)
** e.g. C(f=0.1)=0.53..
* since 1993 we know how to make good codes
* DVBS2 - sparse graph codes
* Fountain codes, ...
* Hamming: H*t=[0,0,0]' mod 2 (H is 7x3 matrix)
* !sphere packing never got to shannon limit of error correction
* Shannon averaged over all codes and shows that average probability of error is small
* Factor graph:
** bipartite graph (for each matrix)
** spare, only few connections
** parity constraints
** loopy belief propagation, need iterations to settle code word (sum-product algorithm) -> not good to get marginal probabilities
** can get arbitrary close to Shannon limit

=== Compression
* Huffmann within 1 bit of optimal (entropy)
* symbol codes (single character) bad if Markov chain (since +1 bit is very large for extreme distributions)
* arithmetic coding:
** learning probabilities on the fly
** identical twin learning
** encode binary
** within 2 bits of optimal of whole file

== Linear algebra
* Tr log A=log det A
* SVD of nxp matrix: O(max(n,p)min^2(n,p)) time
* positive definite: xTAx>=0, invertable, all eigenvalues positive

Due to Cauchy:
df/dx=Im(f(x+i*h))/h   # much less floating round-off errors
only if analytic

== Tribes of machine learning
https://www.youtube.com/watch?v=UPsYGzln-Ys
* Pedro Domingos
* Symbolists
** Induction/Deduction
** -> composable rules
** master algoritm: inverse deduction
* Connectionist:
** deep learning, Bengio/Hinton/LeCun
** assignment to which part is important
** -> emulate brain
** master algorithm: backprop
* Evolutionaries
** John Koza, John Holland! (dead), Hod Lipson
** -> discover structure
** master algorithm: genetic programming
* Bayesians:
** David Heckerman, Judea Pearl, Michael Jordan
** -> model uncertainty
** master algorithm: probabilistic inference
* Analogizers:
** Peter Hart, Vladimir Vapnik, Douglas Hofstadter
** -> generalize to very different situations from very little examples
** nearest neighbor, analogies
** SVM: only consider border points, also amooth border
** master algorithm: kernel machines
* Recommender system: 1/3 of Amazon business, 3/4 of Netflix business
* Universl learner to enable:
** home robots
** world wide brain
** cure cancer
** recommendation on every data available about you

== Deep learning
https://www.youtube.com/watch?v=kxp7eWZa-2M
* recommenders: Matrix factorization or RBM (in Netflix both)
* ReLU: derivatives are such as to make same optimal learning rate in each layer due to weight convergence
* machine translation: run RNN; hidden state is then thought vector (on very big data not as good as google though)
* brain and backprop? -> explanation why still possible
* ImageNet SOTA: ResNet, AmoebaNet; Top1 84.3% (2018), Top5 97%

* Searching for optimal activation functions: https://arxiv.org/pdf/1710.05941v2.pdf
** best is simple functions; often b(x, g(x))
** working sightly better than ReLu: x*sig(beta*x) [Swish, beta fixed or trainable, non-monotonic], max(x, sig(x))

== Machine learning that changes behavior in wild
https://www.youtube.com/watch?v=QWCSxAKR-h0

== LeCun keynote
https://www.youtube.com/watch?v=fe-uxTUnoCs
* memory in networks?

== Deep learning
* optimization RMSProp w Mom.m ADaM (easiest to tune); SGD w/ Mom. (harder)
* BatchNorm: speeds up convergence; allows higher learning rates

== Gaussian Processes

=== Introduction to Gaussian Processes
https://www.youtube.com/watch?v=JSY2rha7qOw
* sum of Gaussians is Gaussian
* scaling Gaussian also Gaussian
* multivariate Gaussian to put prior directly on function
* ???

https://www.youtube.com/watch?v=rrBhHDzmgUA
* Taken theorem: just single coordinate x(t), but together with x(t-D), x(t-2D) can reconstruct
Lyapunov coef and topology if dynamics (e.g. Lorenz attractor)
* -> could use convergent cross-mapping by comparing x(t),x(t-D) - y(t),y(t-D)

=== Distill article
https://distill.pub/2019/visual-exploration-gaussian-processes/
* cov. matrix symmetric and positive semi-definite
* Gaussian closed under conditioning and marginalization
* marginalization (with conv.matrix): simply take the block on diagonal
** new mean will depend on value of conditioned variable, but not cov.matrix



== Tensorflow
* Python and C++ frontend to specify computations graph
* all data flows are tensors (N-dim arrays)
* distributed; multiple devices possibly simultaneously
* useful for many ML algorithms
* abstracts away hardware
* extensible
* auto differentiation
* stateful nodes; no parameter server

== Tensor methods
https://www.youtube.com/watch?v=KmvZu9qJNzg
* Spectral methods
* spectral on tensors: higher order relations (not just pairwise Cov)
* Orthogonal Tensor Power Method
* tensor decomposition

https://www.youtube.com/watch?v=TFIMqt0yT2I
* Invariance by neural activities bad -> better equivariant in shape
* you notice if right angles slightly off in square but not diamond (turned square)
* capture affine tranformation between parts; they should chain as "multiplication"
* capsules find: x,y,prob of some object part
* factor analysis to pose vectors

HTM
http://numenta.com/learn/principles-of-hierarchical-temporal-memory.html
* cortex 75% of brain; 2.5mm think; uniform functionally
* ~4 layers; mini-columns
* 1000 synapses
* only 10% are close to cell body, rest far away
* new synapses formed -> most learning
* all regions memory of sequencesM sequence unfolding
* 2 layers forward, 2 backward
* stable, if predict whats next
* connection: in layer and also next layer
* synapses dont always work; fail often
* very few active, 2%
* SDR overlap -> similar

== Stable conservative clustering for exploration
* not k-means
* DBSCAN OK
* Robust Single Linkage: hierarchical, robust to outliers
* HDBSCAN even better

=== Sparse distributed representations
https://www.youtube.com/watch?v=LbZtc_zWBS4
* w bits on of n -> (n over k) representations
* overlap by AND and count
* failure tolerant

== Tensor methods
https://www.youtube.com/watch?v=B4YvhcGaafw
* beat local optima by tensor factorization
* discriminative model: learn conditional distr of output given input
* generative model: learn joint distr of input and output
* generative models make NN tractable
* transform input
* XOR has local optima
* multivariate moments methods
** use E(x*y), E(x*x*y), E(phi(x)*y)
** here use the transform
** class of score function: S=-grad log pdf(x)
** -> look at correlation with label
** also higher order score functions
** derivatives nicely related to parameters
** with matrix not enough constraints; with tensors more coverery possible

== Recent advanced in deep learning
* LSTM better than RNN; gating mechanism; long-range "memory" due to forget gates
* seq2seq: for let it run with only input; generate representation vector
* also possible to take image features and output text description
* Sequences are now first class citizens
* Sets as features?

== RNN
* trick to avoid vanishing/exploding gradients: clip gradients to prevent instability
* but too "powerful": model become too complex
* ht=o(A*xt+R*h(t-1)); yt=f(U*ht)
* Structurally constraint recurrent networks: diagonal layer keep state, has single constant for self-connections of nodes; accumulates long history
* less parameters than LSTM; computationally cheaper
* cannot do well:
** variable-length patterns
** algorithmic paterns
** a^nb^n

== Survival analysis in Python
https://www.youtube.com/watch?v=XQfxndJH4UA
* lifelines package
* -> measure durations
* know "when does someone die?" (without waiting for too long)
* censorship problem; don't see end of all patients
* survival curve S(t)=P(T>t)
* KaplanMeier curve for fitting

== Spark
* DataFrame: same performance for all languages due to Catalyst optimizer
* Tungsen:
** today often CPU bound (only network and disk speed [SSD] increased)
** -> improve CPU efficiency
** dynamic/runtime code generation
** exploit more cache locality (L1, L2)
** off-heap memory management (do manual memory; otherwise JVM messes up GC)
** easier with DataFrame -> Tungsten enabled only there yet
* Dataset API:
** in Scala want more type safety
** typed interface over dataframes/Tungsten
** define arbit Java objects; convert any object to DataSet object
** tells how to map to Tungsten types, SPARK-9999
* Streaming DataFrames
** dataframe concept to streaming also
* Amazon 2TB bytes soon
* new memory 3D XPoint (large as SSD, last as RAM)
* could optimize for more than JVM: LLVM, SIMD (vectorized), 3D XPoint, ...

== Learning Theory - Domingos
* from Coursera
* VC generalizes PAC to continuous spaces
* Gibbs learner: pick random hypothesis by probability from data; use that to classify; is <= 2*bayesoptimal
* NFLT: since there is always and exactly opposite concept which give accuracy 1-x instead of x for a particular learner
* anti-concepts is basically one where all test samples are assigned the opposite label
* -> proved with function bool->bool; accuracy on unseen examples relevant
* <Acc(Learner)>_concepts=0.5
* loss=bias+variance+noise; if noise->need more attributes
* only noise unavoidable
* bias&variance is about random variations in training set
* bias: deviation of avg predicted model from reality
* variance: oscillation of predicted model around avg predicted model when training set changes
* variance has nothing to do with truth; diff between avg model and data
* bias: truth-avg
* variance: avg-data
* general: main prediction y-bar = argmin E(loss(y,y')); for squared loss argmin is mean
* bias/variance measured at particular point; but could also avg
* in classification variance can be good; unstable classifier gives good results; unlike in regression
* PAC learning:
** learn about generalization error from training error
** how much data is enough?
** hypothesis space H finite; m indep instances
  -> prob. that version space (training consistent hypothesis) contains hypoth.
  with error greater eps, goes down as |H| exp(-eps*m) [probabilistic statement]
  -> prob goes down exponentially with instance numbers [bad hypothesis won't get all right; prob. decreases fast]
  -> to bound error by delta: m>=1/eps*(ln|H|+ln(1/delta))
** making delta small not so costly; but also |H|=2^2^d
* Agnostic learning:
** dont assume that concept is in hypothesis space
** write 2*eps, if care about bound on difference between train/test error
* for infinite hypothesis space -> VC dimension
** replace ln|H| by VC
* shattering:
** relation between set of instances and hypothesis space
** if for every dichotomy, there is always a consistent hypothesis
* VC = largest finite subset of instance space that shattered by H
*! adversary makes label; but you choose any point positions
* hyperplane in d-dim: VC=d+1
* 1 param could also have infinite VC
* SVM: E(error)<=E(#sup.vec)/(#instances)

== Random project ensemble classifiers - Schclar, Rokach
* general johnson-lindenstrass: any metric with N points can be embedded by a bi-Lipschitz map into an Euclidean space of logN dimension with a bi-Lipschitz constant of logN [4]
* single run of random projections unstable -> use ensemble [10]

== The separation plot - a new visual meothd for evaluation the fit of binary models - Greenhill
* best cutoff and implications unclear
* ROC curve tells little about actual model fit
* evaluation calibration:
  * Brier score
  * Expected PCP [Herron99]: (sum_(y=1) p_i + sum_(y=0) (1-p_i))/N
* plot rag-plot on probability as x-axis for all classes (mb with cumul too)

== On multi-class cost-sensitive learning - Zhou, Liu
* example dependent cost-learning: [Zadrozny01]...[Maloof03]
* cost-sensitive good for imbalanced: [Chawla02], [Weiss04]
* rebalancing not helpful for multiclass [ZhouLiu06]
* Elkan theorem: to make target prob threshold p* correspond to p0 -> number of 2nd class examples multiplied by p*/(1-p*)*(1-p0)/p0

== Cost-sensitive learning and the class imbalance problem - Ling, Sheng
* theory of cost-sensitive learnign [Elkan01][ZadroznyElkan01]
=> check ICET [Turney95], cost-sensitive decision tree [DummondHolte00, Ling04]

== Multiclass cost-sensitive classification
* see refs [_1]
=> check regs [4][5]

== Feature-weighted linear stacking - Sill, Lin
* stack by linear model, where coef are linear of raw features again (-> actually linear model with interaction terms again)
* sum (sum a_ij f_j)*p_i ; where p_i is prediction of model i and a_ij is raw feature j of model i
* use non-negative weights for stacking [6]
* netflix features: number of movies rated by user, number of users rated for a movie, log/binary versions, mean user rating with bayesian shrink to overall mean, norm of 10-factor SVD trained on residuals of global effects,
  correlations, ... (see table 1)

== Class imbalance
* different reason for problem possible [_2]
* SVM-ensemble good if low imbalance; SVM-THR good if high imbalance and correlated features [_2]
=> check [19] for effect of high dimensions with imbalance
=> check [37] for threshold method (threshold change leaves accuracy same, but adjusts precision/recall)
* one-class SVM good for high imbalance [27]
* Meta Imbalanced Classification Ensemble (MICE) [40], good but requires algorithmic modification
* feature selection described in [_3]
* many sampling-based references in [_3]; many are similar, SMOTE good for large training, some ideas on imbalanced rules
=> [_3]: [38-39] WE and RUS good
* cost-sensitive references in [_3]
=> cost-sensitive RF by sampling+thresholding [_3:61]
* empirical thresholding [_4:ShenLing06]
* references in [_5]; some comparisons
* SMOTE-ENN (in sklearn/imbalanced-learn) good (?)
* cost-sensitive SVM not so successful
* C5 cost-sensitive was same as oversampling (both better than under-sampling(?))
* some references in [_6]
* balanced random forest and weighted random forest in [_6]

[_1] Cost-sensitive classification - Status and beyond - Lin
[_2] Class-imbalanced classifiers for high-dimensional data - Lin, Chen
[_3] A review of class imbalance problem - Elrahman
[_4] Cost-sensitive learning and the class imbalance problem - Ling, Sheng
[_5] Analysis of preprocessing vs cost-sensitive learning for imbalanced classification - Lopz, Herrera
[_6] Using random forest to learn imbalanced data - Chen

== Decision tree
* http://de.slideshare.net/pierluca.lanzi/machine-learning-and-data-mining-11-decision-trees
* impurity measure should:
  * zero when pure node
  * maximal when all classes equally likely
  * multistage property (decision can be made in several stages)
* -> satisfied by entropy; but biased towards attributes with large number of values
* -> Gain ratio reduces this bias
* GainRatio=Gain / IntrinsicInfo
  IntrinsicInfo(S,A) = sum |S_i|/S * log |S_i|/|S|
* GainRatio may overcompensate (chose attribute just because IntrinsicInfo low) -> consider attributes only with greater than average information gain
* Biases:
  * Information gain: towards multivalued attributes
  * Gain ratio: prefers when one partition much smaller than others
  * Gini index: prefers multivalued attributes; problems when number of classes large; favors equal-sized partitions and purity in both

* latest C4.5: J4.8 in Weka (or commercial C5.0 from Rulequest)

== Graph DBs and Python
* http://de.slideshare.net/MaxKlymyshyn/odessapy2013-pdf
* ArangoDB; Bulbflow, py4neo
* Arango:
  * AQL: Arango query language
  * support Gremlin graph query

== Pomegranade
* https://youtu.be/YBknijEiABA?t=8m15s
* fast and intuitive
* combine HMM, GMM, NB, Graphical, ...
* even faster than numpy, scipy, sklearn... for prob tasks

== Decreasing uncertainty
* https://www.youtube.com/watch?v=jtkSaHC6Hy0
* curse of dim -> use weakly informative priors or penalized regression
* Weakly informative priors:
  * Y~N(X*b,sigma); likes Cauchy-0.25(l,s) prior for coef b
  * credible interval: chance of being in limit right now
  * confidence interval: chance of being in limit if repeated experiment
  * weakly informative prior: confidence intervals much smaller
  * -> Andrew Gelmans secret weapon
  * "Stan" package for MCMC (faster than BUGS)
* Penalized regression:
  * penalty term added to regression
  * glmnet: gives coef: which minimize; and which minimize when error still within 1std error
  * penalized regression can't do confidence intervals easily
* Bayesian interpretation of penalized regression:
  * look at posterior mode
  * -> ridge prior: normal; lasso prior: laplace prior (exp.)
  * -> tighter confidence intervals; greater interpretability

== Hochreiter Neural Network Notes
Solution to vanishing gradient:
* pre-train network
* ReLU
* LSTM
* Highway net
* ResNet
* Ky Fan: showed that if activation=activation-old+term then solved

* SGD actually good since creates entropy and reaches flat minima

=== Self-Normalizating NN
* ANN Kaggle successes (apart from images) actually only HIGGS, Tox21, Merck Activation
* ReLU+norm best at 2-3 layers only
* !usually ANN unstable on inhomogeneous data (cannot do too much regularization either), but Self-Normalizing NN work
* best slope 1.05 (>1)
* only SeLU start to become better than RF
* SeLU can work with 8 layers

=== GANs

* use two-time-scale to avoid oscillations
* Coulomb GANs: go along high field direction

== Hinton at Stanford - Backprop in Brain
https://www.youtube.com/watch?v=VIRCybGgHts
* neuro scientists have arguments why backprop cannot be
* -< show that there is a way
* Wake-Sleep algo (Hinton'95): double direction connection; try to reconstruct; no need for backprop
* Goodfellow Image generation: 2nd net tries to detect whether image real -> internals can be used in generative net
* -> don't have to inject label for supervision
* stochastic 0/1 fine
* regularizer: let models share information (dropout -> softmax = geometric mean ensemble); parameter want to be like in other model (better than zero)
* there is a way to represent derivatives

== McKinney
* Feather:
** build on columnar Arrow
** minimal structure to include Python and R dataframes
** libfeather C++
** Rcpp for R and Cython for Python dataframe
** not as fast as native though
* with Blueyonder: Arrow based adapter to parquet

== Yamal by Blueyonder
https://www.youtube.com/watch?v=R1em4C0oXo8
* splitter: split data into chunks and execute following pipeline on all chunks in parallel
* fork: fork same data stream to different functions
* reduce: combine different streams
* scope: assign labels to data; case-when-label structure
* all functions single argument
* different execution backends: local parallel, remote parallel (self-made cluster backend)
* pipelines are lists of functions
* pickled with dill
* scheduled with asyncio
* pipeline observers: e.g. graph observer, performance observer
* top-level Airflow to schedule tasks; yamal in between as pipelines

== What's new in deep learning
https://www.youtube.com/watch?v=mw-NfRO1jv0
* different weight initialization guidelines; but simple theory doesnt work
-> Batch Normalization from Google'15: adapative re-parametrization of data
* mean and variance calc during training -> normalized data to mean 0, variance 1
-> gradient won't harm; don't need penalization to cost function anymore
* now also need to increase learning rate
* also remove dropout
* accelerate annealing
* shuffle training data better
* better to train on real data only (not artifical/augmented)
* currently 152 layers, ResNet 2015
* ResNet: shortcut, residual connections (?)
* ResNet'16: no max pool, no hidden fully connected, no dropout, simple network

== Pragmatic data scientist
* https://www.youtube.com/watch?v=HS7mObQttxU
 Optimize NN search for cosine distance
* cosine distance not metric
** but cos(X,A)>cos(X,B) -> can also look at euclidean distance of normalized vectors -> use space partitioning tree for nearest neighbor search
 Missing value in X of test set
* marginalized over missing value, once you have full classifier

== Survival analysis
https://www.youtube.com/watch?v=fli-yE5grtY
* covariates <-> time of event
* birth event: start of observation period; death event: end of o.p.; censorship: don't always see death
* Kaplan-Meier estimator: MLE estimate survival curve from empirics
* Nelson-Aalen: same for cumulative durvival function
* Cox model: proportional hazard hi(t)=h0(t)exp(sum beta x)
** -> partial likelihood
* Additive Hazards model: when covariates time-dep and Cox cant be used
** lambda(t)=Y(t)*alpha(t)
* evaluate: concordance index (generalization of ROC AUC)
* beyond Cox:
** GAM (mgcv in R)
** Random Survival Forest (randomForestSRC in R): only 3 parameters to fix and no assumptions about covariates

== Hyperparam search
* Gaussian processes: only for not too many continuous parameters
* Random Forest Based (SMAC): discretize all
* Non-parametric TPE: for condition param; need prior distr.
* Spearmint seems best
* Random search at 2x eval is better
* auto-sklearn; SMAC search; explicit parameter list
* Autoweka; SMAC or TPE; no meta-learning; no pipeline selection
* Hyperopt-sklearn; TPE; list of parameters
* TPot; genetic algorithm for pipelines
* Spearmint; only model based optimization; can use for own search
* Scikit-optimize; GP
* in sklearn soon: BayesianSearchCV, Pipeline search, parameter lists
* hyperband: subsample data

== Gaussian processes II
https://www.youtube.com/watch?v=KcB8c3a4LYU

=== Kernel choices
* exponentiated-square: smooth functions
* rational quadratic: like weighted sum of exp-sqr; mixed long/short range corr
* periodic: exp(sin^2|dt|)
* non-stationary kernels: not k(ti-tj)
** Wiener process: k=min(ti,tj); prior growing like random walk; draws are random walks
** Linear regression: k=ti*tj
* own kernels:
** kernels linear (for positive coef); or integrate or differentiate
** product of kernels
** warping: k(h(ti),h(tj))
* stationary semi pos-def iff Fourier positive
* GP gives distibution over functions

=== Other likelihood choice
* not noisy Gaussian cloud anymore; e.g. classification
* still need to approximate by Gaussian for tractability
* GP & SVM related

=== Shortcomings
* not all Gaussian (or use approx inference)
* non-parametric flexibility, but we have to compute all data (invert matrix)

=== Possible
* temporal linear Gaussian
* ARMA

== Deep learning and physics
https://www.youtube.com/watch?v=5MdSE-N0bxs
* multiplication gate with sigmoids:
** input u,v
** node weights: (a,a) (-a,-a) (a,-a) (-a,a)
** node weights: b,b,-b,-b
** result: uv(1+O(u^2+v^2)

== Unbalanced dataset
https://www.youtube.com/watch?v=-Z1PaqYKC1w
* here between class imbalance only
* weighted loss in LogReg (class_weight="balanced")
* kNN-based "NearMiss" undersampling to retain useful examples
** keep negatives where avg distance to positives small -> close to boundary
** (usually best) keep negatives that avg distance far from positives -> negatives far from boundary -> keep points that close to all positives
** keep nearest neighbors to all positives
** condensed near.neigh: remove negatives where n.n. same anyway; may be slow
** (can be good) edited n.n.: remove samples which unlike n.n.
* tomek link removal: n.n. pair with different class labels
* random oversampling: may lead to overfitting
* SMOTE: add synthetic minority points (line between positive and n.n.)
* SMOTE+Tomek
* SMOTE+ENN
* EasyEnsemble: Adaboost on 1:1 undersampled
* BalanceCascade: similar, but following subset only take what's incorrectly classified

== Getting best performance with Pyspark
https://www.youtube.com/watch?v=V6DkTVvy9vk
* Py4J + pickling + magic
* Python memory not controlled by JVM -> YARN kills memory
* checkpoint: writes out; loses graph
* key skew with groupbykey
* partition explosion? -> add part to key
* shuffle files no garbage collected -> may need manual GC trigger
* Python UDF slow
* can call Scala UDFs easily (22m30s)
* downsides of Python DF:
** schema serialized as JSON and passed
* future UDFs: better; translate simple UDFs to SQL
* Arrow?
*

== Neighbourhood components analysis
http://videolectures.net/mlss06tw_roweis_nca/
* allround often simple classifiers good, e.g. kNN
* but kNN: needs distance metric and algorithm to find neighbours
* LOO-CV to find best metric? (see if nearest neighbors predict each point)
* but NN depend only on nearest neighbours -> discontinuos over metric
* -> randomized neighbour selection; probability is softmax on distances
* -> avg LOO error smooth
* -> optimize metric with continuous parameter by GD
* e.g. quadratic (Mahalanobis); like linear transf on data + Euclidean
* complicating gradient, but you can subsample and also truncate
* = Neighbourhood Component Analysis
* scale of distance learned; is like learning K
* can also make transf matrix projecting into *lower* space; do same optimization
* -> learn projection where Euclidean kNN works
** good for dim. reduction with many noisy dimensions; better than LDA, PCA
> learns Mahalanobis distance for kNN, by SG on softmax-NN LOO test
=> can also reduce dimensionality (into where kNN L2 works)

== Julia
https://www.youtube.com/watch?v=Cj6bjqS5otM
\  -> backsolve
myfunc.(X)  -> map(myfunc,X)
inbuilt matrices, arrays, frctions,...
type hierarchy Any
borrows a lot from lisp, matlab, ...
many functional ideas
macros
multiple dispatch, types dont have methods, autom compiles version when needed
no multiple inheritance
only leaves can be instantiated -> tree
default args also compile to extra functions
for performance concrete types

== Order statistics
f_order(x1,x2,...)=n!f(x1)f(x2)... if x1<x2<...
0 otherwise

fXj(xj)=(n over j-1,1,n-j) f(xj)F(xj)^ (j-1)*(1-F(xj))^(n-j)

min(exp(l))=exp(n*l)

== QQ Plot

* Interpretation: https://stats.stackexchange.com/questions/101274/how-to-interpret-a-qq-plot
* Visual test: https://stats.stackexchange.com/questions/111010/interpreting-qqplot-is-there-any-rule-of-thumb-to-decide-for-non-normality/111013#111013

== Polynomial fitting
* Savitzky-Golay filter
* windows polynomial in frequeny domain (linear time-invariant system) interpretation facilitates free parameter interpretation
! any LTI system can be described by single impulse response
* using impulse response with convolution same as smoothing
* = pointwise multiplication in frequency domain
* -> degree increase low-pass threshold
* -> window length also increases low-pass threshold, but also cutoff less sharp

== Diagnosing ML models
https://www.youtube.com/watch?v=ZD8LA3n6YvI
Find multicollin
* yes/no from statsmodels condition number
* clustered correlation plot with affinity propagation (no need to num cluster)
* could take ratios if corr var

== Scalable and modular machine learning
https://www.youtube.com/watch?v=XBQzhjiaqhA
* XGBoost:
** could place monotonic contraints target/feature
** new histogram based speed-up (like FastBDT and LightBGM) -> 10x single thread, 5x multithread
** GPU based speed-up
** Python, Scala, R, YARN, Flink, Spark
** XGBoost4J
* declarative: Theano, Tensorflow, (maybe more optimized)
* imperative: PyTorch, Chainer, (more flexible)
* MXNet:
** declarative + imperative possible
** auto-parallelization: checks dependencies
** can scale to multi machine or GPU
** memory optimization -> can train bigger models; may be re-computation for memory
* NNVM:
** different frontend (TF, MXNet, ..) and backends (CPU, GPU, ARM, Torch) possible
** challenges: new optimizations, new operators, ...

== Principle Component Analysis -  PCA

* http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/[Its Neuronal blog post]
* max variance of loadings over instances (projections onto a principal component) -> loadings spread a lot
* same as minimizing reconstruction loss
* roughly since recon.loss^2+proj.var^2=const.
* for a centered matrix asciimath:[X] and unit vector asciimath:[w] you can show https://stats.stackexchange.com/questions/32174/pca-objective-function-what-is-the-connection-between-maximizing-variance-and-m/136072#136072[that] asciimath:[Var(Xw) propto -|X-Xww^T|^2], hence maximizing the variance is the same as minimizing reconstruction error.
* usually found by SVD (but doesnt work for tensors, missing data, ...)
* first principal vector -> rank-1 reconstruction = outer product of 2 vectors (principal component and loadings)
** loadings of rank-1 = how much of that 1 feature vector you want for each instance
** loadings of rank-k = k feature vectors, how much of each you want for each instance
* inconsistent when more features than instances (due to noise; http://dx.doi.org/10.1198/jasa.2009.0121[Johnston 2009])
* sklearn PCA.noise_variance_ seems to estimate the noise which was not fitted

=== Estimating how many components to keep
* threshold on singular value (from estimating noise; http://dx.doi.org/10.1109/TIT.2014.2323359[Gavish 2014]): asciimath:[lambda = 4 sigma sqrt(n)/sqrt(3)]
* Bayesian alternative http://hd.media.mit.edu/tech-reports/TR-514.pdf[Minka 2000]

=== Quadratically regularized PCA
* similar to Ridge regression
* quadratic penalty on both weights and loadings
* similar to classical PCA (https://courses2.cit.cornell.edu/mru8/doc/udell15_thesis.pdf[Thesis])

=== Sparse PCA
* penalty of sum of L1 norm on both weights and loadings
* often sparse result
* similar results to PCA, but simpler components
* no but guarantee that sparse is correct (http://statweb.stanford.edu/~candes/papers/LassoFDR.pdf[Su 2015], http://arxiv.org/abs/1601.04650[Avani 2016])

=== Non-negative matrix factorization - NMF
* exactly same optimization term as PCA (with asciimath:[W=XC]), but non-negativity contraint

=== Logistic PCA
* logistic loss; e.g. when data [0,1]
* http://dx.doi.org/10.1109/TPAMI.2008.114[Kwak 2008], http://statweb.stanford.edu/~candes/papers/RobustPCA.pdf[Candes 2009], http://papers.nips.cc/paper/5430-non-convex-robust-pca.pdf[Netrapalli 2014]

=== Robust PCA
* when outliers; L1 or Huber loss

=== Poisson PCA
* http://qwone.com/~jason/papers/ijcai05-preference.pdf[Rennie 2005]

=== Zero-inflated dimensionality reduction
* http://dx.doi.org/10.1186/s13059-015-0805-z[Pierson 2015]

=== With missing values
* difficult http://www.jmlr.org/papers/volume11/ilin10a/ilin10a.pdf[Ilin 2010]
* no analytic solution for any part, local minima, not even mean centering (bias) is clear

== Statistical tests

=== F-test

* usually from decomposition of variability in terms of sums of squares
* ratio of two scaled sums of squares (need to be independent; each scaled chi^2 distribution)

==== Whether multiple quantitative variables different expected value
* whether one of multiple quantitative variables has different expected value
* "Omnibus" test since test for multiple
* avoids specifying which treatments to comparison and no need to multiple comparison adjustment

asciimath:[F=bb"explained variance"/bb"unexplained variance"=bb"between-group variability"/bb"within-group variability"]

* F-statistic with degrees of freedom
* large -> population means different
* for only 2 groups same as t test (since asciimath:[F_2=t^2])

==== Whether nested models useful

* is a Wald-test
* for structural break test: Chow test

==== Fit quality

=== Chi-Square test
* compare measured values against a known distribution
* for discrete data (unlike Kolmogorov-Smirnov and Anderson-Darling)
* for binned data (need cumulative)
* may depend on bin size (optimal bin width depends on distribution)
* needs sufficient size for chi^2 to be valid (expected >5 counts)
* https://www.itl.nist.gov/div898/handbook/eda/section3/eda35f.htm

=== Kolmogorov-Smirnov test
* compare measured values against continuous, known distribution
* use (empirical) cumulative distribution
* exact test
* critical values indep of distribution
* only for continuous (but there are adjustments for discrete case)
* more sensitive in center than tails
* more powerful alternatives: Anderson-Darling, Cramer Von-Mises
* the statistics (max diff between cumulative distr) is a distance metric!
* -> but "saturates" at 1 -> better Earth moves distance

=== Earth movers distance
* distance between distributions (better than KS for anomalous distances)
* in 1D: area between CDFs; also on quantiles
* same as Wasserstein 1-distance
* -> can also do p-th Wasserstein distances (power p)
* CramÃ©r-von Mises distance: p-th power of CDF distance

=== Distances
* divergence: only non-negative and 0 for equal
* MLE = min. KL-divergence


=== Anderson-Darling test
* test if sample comes from specific distribution
* distribution specific; need to calculate critical values per distribution
* but more sensitive
* more weight to tails than K-S

=== Shapiro-Wilk test for normality
* good
* thresholds from MC

== Fast template periodogram
https://www.youtube.com/watch?v=7STeeVnfYFM
* Fourier-like fitting to signals
* Lomb-Scargle periodidigram:
** for evenly sampled equiv to Fourier
** other convenient propereties -> chi-squares distributed -> hypothesis testing
** equiv to least-squares fitting of (single?) sinusoids to data
* fitting explained well in video
* template periodigram: template function
* tricks to run FFT on unevenly sampled data

== Robust ML
https://www.youtube.com/watch?v=tu0TKlizuos
* Kruskal-Wallis: non-param F-test, diff in variance
* Mann-Whitney U: non-param t-test, diff in mean, non-Gaussian
* Kendal tau, Spearman rank: non-param r-squared, association
* t-test: Gaussian, equal variance
* MWU: prob that random item bigger
* F-test: Gaussian, compare variances
* KW: variance of ranks, approx chi^2 if n>5
* Kolmogorov-Smirnov: cumu distr on ranked data, deviation from expected distr, really good test
* parametric tests often work even if distr incorrect; unless black swan
* sign up/down robust but low power

== t-SNE
https://www.youtube.com/watch?v=aStvaXMhGGs
* PCA conserves large distance (variance) -> local structure bad
* SNE: preserve neighbours
* perplexity eff. number of neighbors; often 5-50 or just default
* early_exaggeration: higher if want more space betw cluster in result
* learning rate to avoid local min
* t-SNE assumes local linearity
* -> bad if noisy data -> PCA to smooth data (see sklearn); do PCA 50 first
* -> if data too complex -> use auto-encoder first
* perplexity:
* early_exaggeration: if too small (e.g. below default 12), then cluster parts might not meet each other
* if you have too few examples in one area, it may split into little islands

Method(?):
* t-SNE on "interesting columns" (e.g. ones which predictive of a key state measure)
* color by each feature and see which creates a cluster
* redo t-SNE with only these features

Uncertainty in neural networks
* noise (maybe or not be constant per point)
* model uncertainty

== Entropy

* difference of differential entropy and entropy not important when only differences of entropy considered
* max entropy distribution:
** given variance -> Gaussian
** positive and given mean -> Exponential

=== Entropy estimation
http://techtalks.tv/talks/entropy-estimation-and-streaming-data-sebastian/59433/
* Plugin estimator (naive): consistent, but biased
* need adjustment: some address bias/variance/infinite classes
* decision trees with Grassberger estimator in trees: slightly better (ICML2012 Improved information gain estimates)
* more difficult for differential entropy since not parametric
* best unbiased estimator for continuous normal distribution exists: UMVUE (IEEE TIT'89 Entropy expressions...)
* but does not work if non-normal (e.g. t-distr)
* non-parametric estimation video12:50
* e.g. compute all nearest neighbor distance and use equation
* in streaming: how many samples do I need to wait?, before I accept tree split
* -> Hoeffding Trees '00 Mining high-speed data streams
* basically about variance in entropy gain for split

== Tensorflow lattice
https://www.youtube.com/watch?v=kaPheQxIsPY
* interpolated lookup table
* can include constraints like monotonicity (for interpolation values)

== Smaller ANN
https://www.youtube.com/watch?v=AgpmDOsdTIA
* SqueezeNet has FireModules (like bottleneck); smaller net at good accuracy; with DeepCompression even smaller (overall 510x smaller)
* MobileNet: Convs on subsets of channel
* smaller nets:
** replace last fully connected by convs
** replace 3x3xChannel -> 1x1xChannel; re-organise channels only; not much loss
** channel reduction: convert to fewer channels
** downsampling *gradually* from beg to end
** depthwise separable convs; each conv to only some channels
* -> most params will be in 1x1 convs
* but need Shuffle to exchange info between 1x1; shuffle channels
* compression and distillation; train on high accuracy bigger NN

== Features
* DL may need help with ratios

== Multiarmed bandit
https://www.youtube.com/watch?v=o6HBIGzQfJs
* you cannot play suboptimal arms few than log(T) times
* worst case regret sqrt(NT)
* !Upper Confidence Bound algorithm achieves near optimal upper bounds
* Thompson Sampling
** still better
** converges and achieves instance-wise and worst-case regret lower bounds
** start with uniform believe about payout (Beta(1,1))
** play arm with it's probability to be best (this is optimal amount of "doubt")
** or: sample theta from Beta(a+1,b+1) and pick highest theta
** Bayesian update of belief (e.g. Beta(a+1,b) or Beta(a,b+1))
** Regret <= O(sqrt(NT log(T)))
** with Gaussian priors possible
** best if you don't make assumptions about reward function
* Contextual Bandits
** too many products/customers -> utilize similarity
** similar features mean similar preferences
** parametric models, learn parameters (e.g. Linear Contextual Bandits)

== Ranking
* XGBoost: YetiRank, PairLogit, QueryCrossEntropy, QuerySoftMax

== CatBoost
* classif, regr, ranking
* feat imp, feat interactions
* SHAP values for per obj feat imp
* influential documents
* fast
* different plots
* is feat influence significant
* overfitting detector
* training monitoring
* missing features
* tune: lr, iters, depth, l2_reg, bagging_temp, random_strength


== Quantile regression
* predict v such that P(Y<v|X)=q
* instead of mean
* utility function
* minimize asymmetric penalities for over-/underprediction (1-q) vs q)

== Random Kitchen Sinks
https://www.youtube.com/watch?v=Nqi2iU7kbD0
* f(x)=sum^N a_i k(x,x_i)
* same as linear f(x)=<w, phi(x)> with <phi(x),phi(y)>=k(x,y) [Representer theorem]
* but large k(i,j) matrices
* -> or just take dot product of suitable random mappings; such that result classifier very similar
* Fourier for shift invariant kernels: Fourier transform is distribution and we only sample finite number of terms -> take a few sampled Fourier frequencies as features (vector)
* !Random binning features [19:50]: Make grid in x and indicator features on it -> to get grid sizes randomly sample from hat transform of kernel
* !need to define kernel -> can find a few random features which work with a linear classifier (train kernel machines faster)
* greedy approx decays as 1/sqrt(num_terms)
* instead of greey and sequence, just pick random terms (frequencies)
* !weights are important, but not frequencies (param of functions)

== Neural surface loss function
http://www.ipam.ucla.edu/abstract/?tid=14548&pcode=DLT2018
* too slow to calc loss function for many points
* flat generalize better (?)
* but also opposite possible (Dinh'17)
* flat when: small batch, SGD
* sharp when: big batch, ADAM
* but weight decay can change batch size effect around
* but actually you only plot size of weights -> smal weights look sharp on scale
* ReLU: only layer *10 and other layer /10 yields same
* -> sharpness wrt to weights not meanigful
* filter normalization captures better flatness; plot over 2 random dims
*! skip connections make loss surface much simpler (even for deep nets)
*! wider nets simpler surface
* GAN: when trying to find saddle, min/max collapse, one wins
*! there is a fix to get stable saddle (change minmax optmization slightlx)
* Successful nets: Inception, ResNet, FractalNet, DenseNet

== Best practices for Random Forest
* factor out linear dependence (otherwise linear may overpower rest); for example use transformed target
* still add interaction effects
* check trees for directions of top features
* vary one parameter with synthetic pts

== Bengio Deep Learning
* https://www.youtube.com/watch?v=exhdfIPzj24
* AI:
** need knowledge
** need learning (prior, optimization, efficient computation)
** need generalization (guess probability mass)
** need fight curse of dim
** need to disentangle underlying explanatory factors (make sense of data)
* Curse of dim: need smooth assumption
* most data concentrates on manifold -> map to new space
* GG Image search (Bengio/Weston NIPS2010, Bengio NIPS 2000):
** map images to (100 dim) space
** map keyword to space, too
* Machine translation improvements (encoding/decoding problem -> parametrization grows linearly with languages, not quadratic); like universal language
* some functions a lot more efficiently if multiple hierarchies (would need exponential size with 2 layers)
* deep learning: can split input space in may more not-independent linear regions with constraints (e.g. mirror responses by folding)
* 2006: unsupervised pretraining (RBM, auto-encoder, sparse coding)
* each parameter can influence many regions
* Bengio: gradual disentangling manifolds
* limitation of backprop: relies on infinitesimal representations; but very deep nets yield too sharp non-linearities; also not biologically plausible
* idea: neurons try to predict future value and also match past; observation also clamped


== Reinforcement learning

* https://www.alexirpan.com/2018/02/14/rl-hard.html
* DeepMind rainbox method needs 18 Mio frames (83h of play) to beat humans at some Atari games
* generally very sample inefficient
* most often alternative methods yield better results (e.g. easy to beat DQN in Atari with MC tree search on Atari simulator)
* Boston dynamics uses just classical robotis techniques
* need reward function which captures exactly what you need; may overfit very easily
* hard with "sparse rewards" (only goal); compare to "shaped rewards" (also indicate right direction)
* some bad behaviors can burn-in (local optima)
* often bad generalization to other (supposedly bad) agents
* https://arxiv.org/abs/1709.06560
** multiply reward by a constant can cause significant differences
** need more than 5 random seeds
** diff implementations of algorithms have different performance on same task
* hard to find real world value (maybe only https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/ and https://cloud.google.com/automl/)
* maybe initialize with some supervised learning; and RL as fine-tuning
* shaped reward better
* model-based approach is more sample efficient, but how to do it?
* more complex tasks can be easier, since less overfitting

== Causality

=== Summary
* treatment assignment must be non-factor
* -> need to adjust for confounders (affect both treatment and outcome)

==== Instrumental variables
* when unmeasured confounders exist, but IV can be found
* IV Z does affect treatment A, but not outcome Y (in a direct way); "exclusion"
* Z randomized enough to be uncorrelated with noise/errors
* 2-Stage-Least-Squares:
* 1. regress A~Z+X
* 2. regress Y~hatA+X
* 3. coef of hatA is effect size
* (hatA unconfounded)
* monotonicity important(?)

=== Assumptions
* outcome Y, treatment A, pre-treatment covariates X

==== Stable Unit Treatment Value Assumption (SUTVA)
* no interference
** units do not interfere with each other
** no spillover or contagion
* one version of treatment
-> write potential outcomes of i-th subject only in terms of his treatment

==== Consistency
* Y^a is actually the outcomes with treatment A=a

==== Ignorability!
* most important
* no unmeasured confounders
* Y0, Y1 \perp A | X
* among subjects with same X, treatment is *randomly* assigned
* treatment assignment becomes non-factor
* but usually need too many variables -> could lead to many empty cells
-> alternatives: matching, inverse probability of treatment weighting, propensity score methods, instrumental variables (natural experiments with variables as randomizer)
* need that (within known group) the prob. of treatment is random
* causal graphs can show which variables needed to control for confounding

==== Positivity
* everybody had some chance of get either treatment
* P(A=a|X=x)>0 forall a,x
* variability in treatment assignment

=== Observed data
* E(Y|A=a,X=x) involves only observed data
* =E(Y^a|A=a,X=x) by consistency
* =E(Y^a|X=x) by ignorability
* marginal, causal effect -> average over X

=== Stratification
* to estimate causal effects
* E(Y^a) = sum_X E(Y|A=a,X=x) P(X=x)  -> E(Y|..) only some, P(X=x) over all
* standardization = stratifying + averaging
* -> split by confounder -> calculate rates within those subgroups
* mean potential outcome: what if everybody had the treatment (transfer rate to untreated as well); subtract what if no-one had treatment

=== Experiment design
* try to compare only subject in more same situations

==== Incident user design
* compare at equal conditions by fixed time after start
* incident user design, new user design
* look at outcome after some fixed time after starting a treatment
* not looking at only right now and whether they are treated; might include people who were treated in past
* only unclear at what time to measure no non-treated people

==== Active comparator design
* have equal conditions by having some kind of treatment in all groups; no difference before
* compare to other treatment -> no problem with "untreated" and time question
* but causal question then more narrow

=== Confounding
* variables that affect treatment *and* outcome

=== Graphical models
* tell which variables independent, cond. indep, factor/simplify, ...
* used to derived nonparametric causal effects
* ...->A->X  :  P(X|A)=P(X|A,...)  or B\perp D,C|A

Prob. factorization does not determine DAG uniquely (e.g. A->B or B->A)

Inverted fork/Collider do not pass information between branches

==== Blocking
* Chains and Forks block by conditioning on middle
* Colliders get unblocked by conditioning on middle
* A and B d-separated by nodes C if all paths blocked

-> "Chains and Forks are of metal and conduct electricity! Conditioning reverses effect on all. (Only colliders stop)"

=== Confounder
* only need to control for backdoor paths
* front-door path: link from start outwards
* exact front-door paths interesting only when Causal mediation analysis (how much through intermediate variables)
* !back-door paths need to be controlled for (link goes into start) -> confounder which affects both start and end of chain -> identify variables which block all back-door paths
* -> would have ignorability Y0,Y1 \perp A|X
* if DAG slightly different, you need to control for other
* !sometimes not possible since controlling for some observed would at best unblock a path (then all attempts may fail) ->everything (even below approaches) wrong
* !controlling for can be done with: matching and inverse probability of treatment weighting

==== Backdoor path criterion
* blocks all backdoor paths; does not include any descendants of treatment
* possible sets to block not unique
* need to know DAG for this here
* controlling for one var may however unblock a path if unobserved missing

==== Disjunctive cause criterion
* do not need to know full DAG
* control (only) for all (observed) *causes* of exposure and/or outcome (i.e. need only direct relations)

=== Randomized trials
* no link between possible confounders and treatment -> distribution of pretreatment variables identical in all treatment groups ("covariate balance")

=== Observational studies
* as opposed to randomized trial

==== Matching
* select pre-treatment covariates which will satisfy ignorability assumption
* match individual from treatment and control group on some covariate (need to drop of no matches found)
* would also see exceptional people who are only in one group
* ->treat like randomized trial
* but will be effect on selected subgroup only (treated, when they are smaller)
* fine balance: only approximate matches fine, if in the end still *same marginal distribution* of covariates for treated and control
* sometimes multi-matches ok

===== Matching on confounders - Finding distance
* distance matrix (e.g. [robust] Mahalanobis distance)
* covariance matrix as scaling
* possible to use ranks of all covariates (to dismiss scaling effects): robust Mah. dist.

=====  Greedy matching
* fast, but not great
* sometimes second iteration with more matches possible
* "caliper": max acceptable distance

===== Optimal matching
* slow, but good
* network flow optimization problem
* constraints possible to make it feasible: match with one covariate exactly equal ("sparse matching")

===== Assess balance
* check covariate balance
*e.g. use standardized differences (similar means): (mu1-mu2)/sqrt(mean(variance))
      * <0.2 just about ok
* or use hypothesis tests to compare means (but with many samples there is always a difference)
* create "Table 1": summary statistics (e.g. mean) of covariates for matched groups (overall mean)

==== Analyse matched data
* test for treatment effect, estimate confidence interval

===== Permutations tests, Exact tests
* choose test statistic; permute and compute test -> see if it was unusual
* reassign treated/control label
* for 0/1 in treat/control: only 0-1 pairs can actually change; like McNemar test for paired data
* paired t-test for continous
* other outcome models:
  * conditional logistic regression: matched binary outcome data
  * stratified cox model:time-to-event (survival); baseline hazard stratified on matched sets
  * generalized estimating equations (GEE): match ID variable used to specify clusters; estimate causal risk difference

https://cran.r-project.org/web/packages/rcbalance/index.html for matching

=== Sensitivity analysis
* need balance on observed covariates ("overt bias")
* "hidden bias": bias on unobserved variables; even worse if they are confounders
* sensitivity analysis: how much hidden bias before conclusion would change (e.g. direction of determined causality)
* no hidden bias: when prob. of treatment the same for matched persons
* need odds-ratio=Gamma=1
* how much can we increase Gamma in analysis before evidence of treatment disappears
* R packages sensitivity2x2xk, sensitivityfull; a bit complex to explain here

=== Propensity score
* = probability of receiving treatment given covariates, pi_i=P(A=1|X)
* balancing score: same propensity score (even if different covariates)
* P(X|pi(X)=p, A=1)=P(X|pi(X)=p, A=0)
* -> could use propensity score to stratify by this for causality analysis
* could estimate propensity score from logistic regression X->A (we want the predicted probs)

==== Matching on propensity score
* look at prop.scores or control and treated (2 histograms for binary treatment)
* -> having overlap to do matching would be good
* -> trimming tails if extreme prop.scores only in control XOR treated
* after trimming use nearest neighbor matching
* Caliper: max distance to prevent too bad matches
* e.g. Caliper=0.2*stddev(logit(prop.score))

=== Inverse Probability of Treatment weighting (IPTW)
* lets assume only one confounder X
* prop.score(X=1) = 0.1, prop.score(X=0)=0.8
* prop.score matching would select only few people (min(x=1,treated vs x=1,control))
* !weigh by 1/P(A=a|X)   (a=[0,1])
* ->instead matching you now average over all
* creates un-confounded pseudo-populations
* https://en.wikipedia.org/wiki/Horvitz%E2%80%93Thompson_estimator (weight data
to account for oversampling)
* assuming that prop.score model on X fully captures confounding

* ! E[Y^1]=<Y>_(weight 1/pi, only where A=1)

=== Marginal structural models
* type of causal models
* IPTW can be used to estimate params from model params
* MSM: model for means of potential outcomes
* Marginal: not conditional on confounders; averaging over whole population
* Structural: model for potential outcomes, not observed outcomes
* -> for all subjects and all outcomes simultaneously
* e.g. E[Y^a]=psi0+psi1*a
* or use logit(E[Y^a]) if binary -> exp(psi1)=causal odds ratio=odds(Y^1=1)/odds(Y^0=1)
* MSM with effect modification: something that might have effect on treatment
* -> include V (with interactions with A) on linear model; but usually just average out confounders; split causal effects by V

 E(Y^a)=g^-1(...) not same as E(Y|A)=g^-1(...)
since first version is "setting A" and second is conditioning on "A"
(only same for randomized trials)
-> need IPTW to create pseudo-populations
->introduce weights (Video Week 4 IPTW Estimation 6:40)
W=1/P(A=a_i|X_i)
-> estimate prop.score first; fit GLM with weights
-> use asymptotic (sandwich) variance estimator (or usw bootstrapping) since weighting might give incorrect standard errors (due to pseudo-population)

=== Assessing balance
* after prop.score model and created weights
* diff. between A=1 and A=0
* with standardized mean (smd) with pooled stddev -> for every covariate
* weighted mean, weighted pooled variance -> smd.
* svydesign package (R)
* want smd<0.1

* if still imbalanced after prop.score -> include interactions, non-linearity (better prop.score model)
* larger weights (positivity assumption shaky) might lead to larger noise
* ->bootstrapping for std errors: sample with replacement and estimate parameters
* -> single instances with large weight will represent larger stddev in bootstrap

=== Remedies for large weights
* check why weights are large; what is unusual about them (problem with data? data extremes/anomalies? problem with prop.score model?)
* maybe remove subjects which have extreme prop.scores (people who were likely or very unlikely be treated); e.g. trim above <2 and >98 percentiles
*maybe just weight truncation: max allowable weight of 100 (clip weight); creates a bit of bias, but less variance

R IPW package

=== Doubly robust estimation; Augmented IPTW (AIPTW)
* could also model m1(X)=E(Y|A=1,X)
* average over: Y value if A=1 and m1(X) if A=0 (hence predict for untreated)
* ! unbiased if outcome model correct OR(!) prop.score model correct
-> augmentation <A/pi*Y+(1-A/pi)*m1(X)>
* AIPTW can be more efficient than IPTW (smaller variance)
* some theory says which are most efficient

=== Instrumental variables (IV)
* useful when *unmeasured* confounding -> cannot average over confounders
* no ignorability
* !Z (IV) affects treatment, but not (directly) the outcome (only through treatment; "exclusion restriction"; do not know from data)
* e.g. randomly encourage smoker to stop smoking before experiment
* e.g. assign people to groups but they may not comply
* causal effect of encouragment (but not real causal effect): E[Y^ (Z=1)]-E[Y^(Z=0)]
* IV in nature: quarter of birth, geographic distance to something
* Z randomization to treatment, A actually treatment received (non-compliance)
* Z -> A -> Y; X -> A; X ->Y
* A^(Z=1), A^(Z=0)   ; A^1, A^0
* causal effect of assignment on receipt = E[A^1 - A^0]
* =1 if perfect compliance
* randomization and consistency -> E[A^1]=E[A|Z=1], E[A^0]=E[A|Z=0]
* causal effect of assignment on outcome = E[Y^(Z=1)-Y^(Z=0)]
* = causal effect of treatment if perfect compliance
* E[Y^(Z=1)]=E[Y|Z=1), E[Y^(Z=0)]=E[Y|Z=0]
* but real causal effect of treatment?!
* IV: use randomization of Z
* subpopulations:
** never-takers (cannot learn causal)
** compliers (can learn since randomized)
** defiers (always opposite than Z suggested; could learn; usually small population)
** always-takers (cannot learn causal)
* IV focus on local average treatment effect; not for whole population!
* IV: E[Y^(Z=1) | A^0=0, A^1=1) - E[Y^(Z=0) | A^0=0, A^1=1) = E[Y^(Z=1)-Y^(Z=0)|compliers]; same subpopulation
* !IV only makes statement about compliers -> Complier Average Causal Effect (CACE)
* =E[Y^(A=1)-Y^(A=0)|compliers] since compliers
* but we only know A and Z and not A^0 and A^1
* !monotonicity assumption: there is no defiers (no systematic opposite; probability of treatment increases with encouragement) -> now can find causal effect among compliers
* Intention to treat (ITT) = E[Y^(Z=1)-Y^(Z=0)]=E[Y|Z=1]-E[Y|Z=0]
* expand in subgroups "always taker", "never taker", "complier"
* for "always takers" and "never taker" indep. of Z
* -> E[Y|Z=1]-E[Y|Z=0]=(E[Y|Z=1,compliers]-E[Y|Z=0,compliers])*P(compliers)   # rest cancels
* CACE=E[Y^(A=1)|compliers]-E[Y^(A=0)|compliers]
* CACE=(E[Y|Z=1)-E[Y|Z=0])/P(compliers)
* P(compliers)=E[A|Z=1]-E[A|Z=0]
* ! CACE=(E[Y|Z=1)-E[Y|Z=0]) / (E[A|Z=1]-E[A|Z=0])
* for perfect compliance denominator is 1
* IV:
** strongest assumption: exclusion restriction
** monotonicity makes calc possible

=== IV in observational studies
* Z binary or continuous ("dose of encouragement")
* need to check "exclusion restriction" by subject matter knowledge
* Z has to affect treatment
* examples of IV:
** time when there is change of treatment preferences over time (other drug become popular); "time randomizes"
** distance to something (i.e. travel time)
** mendelian randomization: some genes encourages alcohol, but not final outcome

=== Two-stage least squares with IV
* Y=beta0+A*beta1+eps
* when confounding: A and eps correlated -> OLS fails
1) A_i=alpha0+Z_i*alpha1+eps_i      # Z_i and eps_i indep. by randomization
-> get predicted hatA_i=hatalpha0+Z_i*hatalpha1
(predicted A_i based on Z_i alone)
2) Y_i=beta0+hatA_i*beta1+eps_i
exclusion restriction -> Z indep of Y given A; hatA projection of A onto space spanned by Z; hatA unconfounded (just determined from Z)
-> beta1 estimate of causal effect CACE
* 2SLS also works for non-binary: A ~ Z + X; Y ~ hatA + X
* Biaocchi'14: Tutorial in Biostatistics: Instrumental variable methods for causal inference
* do sensitivity analysis (what if assumptions violated; what if there are defiers? what if Z directly affects Y?)

=== Weak instruments
* measure strength of instrumental variable (how much encouragement works)
* estimate proportion of compliers E[A|Z=1]-E[A|Z=0]
* causal effect estimation unstable of only few are compliers
* use other IV if too weak
* strengthen IV: "near/far matching" make similar on confounder, but diff in IV (Baiocchi'12)
* R package: ivpack
* 8:40 complier average causal effect or alternatively 2SLS (same result)
* when more variables: use 2SLS
* use robust std errors

== Correlation

=== Maximum Information Criterion (MIC)

* seems bad https://arxiv.org/abs/1401.7645

=== Maximal Correlation: max(Corr(f(X), g(Y)))
* -> Alternativ Conditional Expectations (ACE)
* iteratively, cubic spline modelling
* not good if data correlates with high discontinous place
* R: acepack
* https://www.quora.com/Correlation-can-measure-only-the-linear-relationship-between-variables-What-are-the-methods-for-measuring-non-linear-relationships-between-two-variables[Quora]

=== Non-linear correlation information entropy (NCIE)
* Wang 2005: "A nonlinear correlation measure for multivariable data set"

=== CANOVA
* Wang 2015: "Efficient test for nonlinear dependence of two continuous variables"
* https://sourceforge.net/projects/canova/

=== Mutual Information
* KL[p(x,y)||p(x)p(y)]
* H(X)-H(X|Y)=H(Y)-H(Y|X)=H(X,Y)-H(X|Y)-H(Y|X)

=== Total Correlation
* one generalization of Mutual Information for more than 2 inputs
* sum H(Xi) - H(X1..Xn)

=== Dual Total Correlation, Excess Entropy, Binding Information
* https://en.wikipedia.org/wiki/Dual_total_correlation[Wikipedia]
* another generalization of Mutual Information
* H(X1..Xn)-sum H(Xi|X\Xi)

=== Randomized Dependence Coefficient

=== Coefficient of determination R^2

=== Distance Correlation
* Proportion of variance in dependent variable that is predictable from indep. var

=== Spearman's rank
* for monotonic


== Random number generator
https://en.wikipedia.org/wiki/Blum_Blum_Shub
* Blum Blum Shub: x <- x^2 mod M
* M=pq and some certain restrictions
* could calculate n-th number direction without recursion

== Model comparison
https://sebastianraschka.com/blog/2018/model-evaluation-selection-part4.html

== Principal Component Analysis
* R1-PCA is a rotationally invariant L1 norm (Ding, Zha)
* for mean centered: |Xc|->max, |c|=1
* SVD method only if not tensor or no missing data
* w=Xc -> X ~ w c^T (like simple outer product; every observation is some fixed base vector times a scaling [loadings])
* for d principal components: d base vectors, d loadings for each instance; X ~ W C^T
* same objective: minimize squared residuals or projection
* PCA bad if p>n; overfits to noise, inconsistent
* number of components to pick:
** estimate noise in data sigma
** min. singular value 4*sigma*sqrt(n)/sqrt(3) [for square matrix nxn]
* hard if missing values (no analytics, local minima, no analytical even for bias term)

=== Variations
http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/
* Quadratically regularized PCA: penalized W and C elements by square; solved analytically with SVD
* PCA and QR-PCA non-convex but solved exactly
* other methods only bi-convex: can alternate between finding W and C
* Sparse PCA: L1 norm on W and C elements
* Non-negative Matrix Factorization: same as PCA, but W and C elements >=0 (ofc only when Xij>=0)
* Logistic PCA: logistic loss (when Xij={-1,1})
* Robust PCA: L1 of residuals or Huber
* Poisson PCA
* PCA on ordinal data
* Zero-inflated dimensionality reduction
=======
* Maximum Information Criterion (MIC): seems bad https://arxiv.org/abs/1401.7645

== ML Strategies
* know what to tune to get some effect ("orthogonalization")
* maybe also some minimum extra metrics
* cross-val and test set should be same distribution
* if a lot of data: use more percent in train data (10000 in test set?)
* not having test set may be ok, if not final performance needed
* Bayes optimal error is limit
* Compare to human error performance (proxy for Bayes error) to judge train error (need bias or variance?)
* DL quite robust to random label noise
* count incorrect labels (remember that test set also may need treatment)
* if train and test have to be different (semi-unsupervised): may draw conclusions about bias/variance to predictions of part of train set
* multi-task learning: e.g. car needs to detect many things
* sometimes breaking algorithm into steps helps (as compared to full end-to-end; may need to much data)

== Decision trees
* Gini more for continuous, Entropy for classes (http://paginas.fe.up.pt/~ec/files_1011/week%2008%20-%20Decision%20Trees.pdf)
* Gini tends to find largest class, Entropy groups that make ~50%
* Gini to minimize misclassification, Entropy for exploratory analysis
* usually Gini and Entropy the same within 2% (https://rapid-i.com/rapidforum/index.php?topic=3060.0)
* Entropy slightly slower to compute

== Rule learner

* PRIM: https://ir.library.louisville.edu/cgi/viewcontent.cgi?referer=&httpsredir=1&article=2454&context=etd
* FOIL: needs boolean input

== Visualization

* Mountain plot: Cumulative plot (F vs x), but mirrored down at y=0.5 -> easier to compare 5%-95%

== Metrics

=== ROC AUC
* is also equal to the probability, that a random Positive/Negative pair has probabilities assigned which are ordered correctly (https://www.alexejgossmann.com/auc/)
* invariance to prior class probabilities or class prevalence in the data.
* can choose/change a decision threshold based on cost-benefit analysis after model training.
* http://dl.acm.org/citation.cfm?id=1143874
** domination in ROC same as domination in PR curve

== Hyperparameter Tuning

=== Good parameter ranges
* experiments on OpenML:
** https://arxiv.org/pdf/1802.09596.pdf
** https://arxiv.org/pdf/1710.04725.pdf

== Extrinsic Curvature

* Cauchy: inverse of distance of intersection of two infinitely close normals
* unique "osculating circle"
* also |d vec(Tangent)/ds |
* magnitude of acceleration if moving with unit speed
* kappa = det(vecx', vecx'')/|vecx'|^3
* 3D: kappa= |vecx' x vecx''|/|vecx'|^3 = a_perp / v^2
* in 3D also need torsion (helical) to describe path (-> related by Frenet-Serret formulas)
* also related to ratio of arc length and line distance

== Approximate Median
* to find k-th largest element you need <=5.43N comparisons (Blum, Floyd, Pratt)
* you need 2N...2.94N comparison to find exact median (for sure O(N))
* survey of Mike Paterson
* to get exact median in 1 pass you need to store >=N/2 elements (for p passes O(N^1/p))
* http://www.mathcs.emory.edu/~cheung/Courses/584-StreamDB/Syllabus/08-Quantile/Greenwald.html


== Neural Nets just Bags?
https://medium.com/bethgelab/neural-networks-seem-to-follow-a-puzzlingly-simple-strategy-to-classify-images-f4229317261f
* NN with a 33x33 vision and sum is almost as good as best
* heavy texture focus
* insensitive to shuffling of image
* CNNs use the many weak statistical regularities present in natural images for classification

== Projection Pursuit
* optimize some "projection index"
* e.g. kurtosis -> minimize
** https://towardsdatascience.com/interesting-projections-where-pca-fails-fe64ddca73e6
** "Fast and simple methods for the optimization of kurtosis used as a projection pursuit index" (Hou, Wentzell)
** however only when class sizes about equal
** better when number of classes 2^n
** need about 10 samples per variable

== Information loss in dimensionality reduction
https://www.youtube.com/watch?v=yN64YMNNEdc
* precision ~ injectivity
* recall ~ continuity
* prec + recall < 2
*

== Decay of memory model

asciimath:[A exp(-at)+B exp(-bt)]
* "The universal decay of collective memory and attention" (Candia)


== Anomaly Detection

=== PyOD methods

Tested on 2D data with simple outliers with default parameters and 0.01 contamination:
* Good:
** KNN (tune: contamination, n_neighbors [30], p [maybe], method [largest (default)]; leaf_size just for speed)
* Bad:
** ABOD (seemed ok, but needs tuning and still did not do what was needed)
** CBLOF (Exception?)
** FeatureBagging
** HBOS
** IForest (some weird extrapolation)
** LOF (cannot contain an obvious high density corner)
** MCD (useless ellipse)
** OCSVM (cannot contain an obvious high density corner)
** PCA
** SOS (did not find an envelope)
* Failed:
** LOCI (did not finish in any time)

== Simple linear regression

y = a * x + b

a = (n* sum (x*y) - sum x * sum y) / (n*sum (x**2) - (sum x)**2)
b = (sum y * sum (x**2) - sum x * sum (x*y)) / (n*sum(x**2)-(sum x)**2)


