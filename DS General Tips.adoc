== GENERAL TIPS

* Regression might be too noisy -> predicts mean only -> use classification (or multiclass)
* Maybe use same class for extreme deviation (both sides)
* signals, multiply sin^2 window
* cluster on 32 sample long; sparse coding or kmeans
* see where reconstruction doesnt match signal
* glmnet from R uses and approximation which makes it faster but less accurate
* You can try this: In one of the 5 folds, train the models, then use the results of the models as "variables" in logistic regression over the validation data of that fold. Weights obtained in this manner will be more accurate, and I bet the SVM, elastic net and LR will have negative weights or close to zero.
* San Francisco housing prediction features: Num jobs, salary jobs, num places to live -> very good performance
* try log(x+1) scaling
* thing to try:
** common sense for features
** look at logreg feature weights
** visualize decision tree
** cluster data and observe
** train simple classifier and see where mistakes
** monitor: accuracy on test data, input feature distribution, output score distribution, output classification distribution
* Methods like ElasticNet, Lasso, LogReg, ... can fit multiple parameters incrementally, reducing computation
* LassoLarsIC can choose parameters by AIC or BIC
* just clip predictions (esp. when averaging from multiple models)
* visualize by pixel maps
* predict only when certain: train model with mean and confidence predictions, mask predictions below some confidence threshold such that mean predictions achieve the needed quality, evaluate models by their number of predictions made
* try abs(x-x0) to enable tree to catch the middle?
* create normalized versions of string fields; or parts of it
* for large data: test on smaller part first and then maybe on all data
* Weibull distribution: time to event/failure; scale/shape
* Voting Classifier: for 4 classes ties become less likely starting from 11 models
* if overfitting -> Bagging; maybe tune params such that less spread betwen train/val
* feat generation var name: prefixtoken__...  (by knowing how many parts a token expects you can decipher again)

=== Eslami - Patterns for Research in Machine Learning [[http://arkitus.com/PRML/ Link]] =
* Use version control (to collaborate, replicate, work from multiple computers)
* Separate code from data (/data, /code; to share code, swap data; hide data from version control)
* Separate input/working/output data (input: never change; working: always change, can safely be deleted; output: for sharing)
* Always keep data raw (keep note where from and licensing; write script to convert data; never clean by hand, or document thoroughly)
* Safe to disk frequently; store output of different days to different folders
* Separate Options (how algorithm should run) and Parameters (fitted model, output of algorithm); unaffected parameters can be initialized by options
* Don't use global variables; communicate through function arguments (easier to debug and parallize)
* Record options used at each run (set random number seed; save copy of code for important runs)
* Make it easy to sweep options (ranges returned by ``get_config``)
* Make it easy to execute only portions of code (specify which parts; store intermediate results, ``run_experiment('dataset_1_options', '|preprocess_data|initialise_model|train_model|')``)
* Use checkpoints; experiments fail occationally; store entire state (counters etc.) at suitable intervals; code should be able to continue from latest saved state (write message that it restarts)
* Write demos and tests (/code, /data, /demos, /tests)
* Estimate how long experiment will run
* Keep journal that explains why you ran experiment and what finding are
* Do coding and debugging on subset of data that runs in 10sec
* Make it easy to swap in and out different models
* Record Git revision number of code
* Use GNU make for pipeline? (download, traning, evaluating)


== Data tips
* event data:
** https://www.kaggle.com/olistbr/brazilian-ecommerce
** https://www.kaggle.com/c/instacart-market-basket-analysis
** https://www.kaggle.com/c/data-science-bowl-2019

== Kaggle
* test-time augmentation; augment test set (e.g. flip image and better get same label)
* folds -> could overfit each a bit (otherwise folds too similar)
* data cleaning (remove outliers)
* "distillation" when you happen to know labels
* Stacking


== Kaggle
* check for overfitting (train/val gap); esp. when adding feats
* check patterns: 2D + color (target)
* LB should correlate with CV
* start with subsample=0.7
* increase min_child_weight if train/val large
* regularization of LB way below CV
* single great model better than ensemble (?)
* adversarial most imp feat? (maybe only one LB visible)
* may be bias in train data
* learn features from autoencoder
* more data augmentation
* SMAPE close to MAE when log
* plot value distr, cat val counts, distr to target, difference train/test (shared features)
* feature: can also related to info from column or row
* bin numeric feats
* residues of linear fit
* SVD
* xgbfir for interactions?
* first make sure than CV correlates with Public LB (not tuning before)
* first test ideas with 1-fold
* save full run and parameters
* logbook of ideas and CV/LB scores
* include non-linear cat vars to OHE