:stem:

== Linear regression

From https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf

asciimath:[y=X beta+epsilon]

=== Least squares solution

asciimath:[epsilon'epsilon=y'y-2beta'X'y+beta'X'X beta]

asciimath:[(del epsilon'epsilon)/(del beta)=-2X'(y-X hat beta)=0]

asciimath:[X'X hat beta=X' y]

asciimath:[hat beta=(X'X)^-1 X'y]

=== Properties of the least squares solution

Properties which do not depend on any assumption, but just follow from the LS solution.

* Observed values X uncorrelated with residuals
asciimath:[X'epsilon=0]
* If X includes a bias term 1, then also:
* Mean und sum of residuals is zero asciimath:[sum epsilon_i=0]
* Regression hyperplane through means asciimath:[bar y=bar x hat{beta}]
* Predicted values of y uncorrelated with residuals asciimath:[hat y epsilon=0]
* Mean of predicted y same as mean of observed asciimath:[hat bar y=bar y]

All of this is just because the coefficients were chosen to minimize the squared error.

== Gauss-Markov theorem

=== Assumptions
* there is a linear relation asciimath:[y=X beta+epsilon]
* X has no perfect multicollinearity (full rank)
* asciimath:[E\[epsilon|X\]=0]. Implies that we get mean right asciimath:[E\[y\]=X beta]
* asciimath:[E\[epsilon epsilon'|X\]=sigma^2 I] homoskedasticity. Variance independent of X and no autocorrelation
* X generated by a mechanism unrelated to asciimath:[epsilon]

=== Theorem

The OLS is the Best Linear, Unbiased and Efficient estimator (BLUE). No other linear and unbiased estimator of asciimath:[beta] has smaller variance.

== Covariance Matrix of beta

asciimath:[E\[(hat beta-beta)(hat beta-beta)'\]=sigma^2(X'X)^-1]

asciimath:[sf"cov"(beta_i,beta_j)]

Estimate noise from

asciimath:[hat sigma^2=(epsilon'epsilon)/(n-k)]

== Hypothesis testing

For hypothesis testing it is also often additionally assumed that asciimath:[epsilon|X ~ N(0,sigma^2 I)]

By assuming that we have a multi-variate normal, we can conclude that

asciimath:[hat beta ~ N(beta, sigma^2(X'X)^-1)]

== Heteroskedasticity

Without heteroskedasticity you can estimate parameter means, but not standard errors.

To compensate, you could

* use weighted least squares (if know something proportional to standard errors)
* or use robust standard errors (White 1980)

== Covariance

asciimath:[sf"cov"(X,Y)=E\[(X-E\[X\])(Y-E\[Y\])\]]

asciimath:[=E\[XY\]-E\[X\]E\[Y\]]

asciimath:[=E\[bb"X"bb"Y"^T\]-E\[bb"X"\]E\[bb"Y"\]^T]

but last equation is not numerically stable.

asciimath:[sf"cov"(X,a)=0]

asciimath:[sf"cov"(aX)=a cdot sf"cov"(X)]

asciimath:[sigma^2(sum a_i X_i)=sum_{i,j} sf"cov"(X_i, X_j)]

asciimath:[Sigma(AX)=A Sigma(X) A^T]

asciimath:[Sigma=E\[XX^T\]-mu mu^T]

Must be positive-semidefinite matrix and any psm can be a covariance matrix.

=== Independence

Independent variables have zero covariance. But zero covariance does not imply independence (e.g. for an X where asciimath:[E\[X\]=0] and asciimath:[E\[X^3\]=0] you have asciimath:[ss"cov"(X,X^2)=0])

=== Covariance matrix
For a vector X
asciimath:[Sigma(X)=sf"cov"(X,X)]

== Spatial data analysis

* Thiessen polygon: polygon where a point is the nearest

=== Aggregation to metrics

* 1-step would be find the ratio between supply and demand in a catchment region of radius (or travel time) around each point
* 2-Step-Floating-Catchment-Analysis:
** Find measure (e.g. ratio supply to sum demand) in radius around each supply point
** Sum measures (of supply point) in radius around each demand point to get final metric
* gravity model with inverse power weights, usually causes more trouble than this simple method

=== Spatial autocorrelation

* standard: Moran's I (basically spatial autocorrelation with weights)
* known form of expectation and variance can be used to set up a z-score for hypothesis testing

=== Interpolation / Kriging

* Kriging method is Best Linear Unbiased Estimator (BLUE) and recommended (but need correct variogram, other non-linear or bias methods might be better)
* other methods not as good: Trend Surface Analysis (just fit a [polynomial] function); Inverse Distance Weighting (inverse distance power)
* Interpolation is weighted mean of surrounding points; weights have to be determined
* first step is to calculate a variogram (relation between variance and distance): mean of asciimath:[(Z_i-Z_j)^2] within given radius (?)
* need to fit one of a certain class of functional forms to the variogram (this choice requires expertise); spherical, gaussian, linear, exponential
* version:
** ordinary kriging: mean is constant (this is the same as Gaussian Processes[?])
** universal kriging: mean is position dependent (usually polynomial trend; then identical to GLS polynomial curve fitting)
** co-kriging: dependence on additional features
** block kriging: made to blocks of areas (instead of points)
* can be used to estimate error of estimations
* "nugget": y-intercept of variogram
* honors observed values (there are matched exactly)

=== Hotspot analysis

* can be polygon or point based
* most popular method: Getis-Ord Gl* (simple weighted sum?)

=== Location coding

* GeoHash: uses z-scores (interleaved bits), Base32 encoded
+ C-Squares

=== Map matching

* matching objects to objects on match; e.g. coordinates of GPS to road (i.e. line) where you are on

== Confidence Intervals

* quite some explanation in "The fallacy of placing confidence in confidence intervals" (Morey et. al.)
* Definition: An X% confidence interval for a parameter theta is an interval (L, U) generated by a procedure (!) that in repeated sampling has an X% probability of containing the true value of theta, for all possible values of theta
* confidence procedure is a random process; confidence interval if observed and fixed
* frequentist CI theory says nothing about the probability of the value being in the interval
* frequentist evaluation: based on "power" of procedures, which is a frequency with which false values of a parameter are excluded
* confidence procedures closely related to hypothesis testing (control rate of including true value; more power if exluding false values)
* intervals based on Uniformly Most-Powerful test are optimal for the goal of CIs
* many different CI procedures
* UMP may still lose information (i.e. beyond 1D summary)
* (!) UMP based CI better than Bayesian at excluding false values
* only Bayesian credible intervals actually contain the true value X%
* frequentist pre-data; bayesian post-data
* when estimating mean of Gaussian, frequentist and bayesian coincide
* (!) always include procedure and statistic used when reporting CI
* CI width means nothing
* for normal data, for each CI procedure there is an equivalent Bayesian with a certain prior (Jeffrey, Lindley)
* CI have difference shape in result parameter space (even 100% CI may be nested in some 50% CI)
* checking whether a parameter is included in credible interval is wrong