= Kaggle Winner solution summary

== Facebook V - 1st place

http://blog.kaggle.com/2016/08/16/facebook-v-predicting-check-ins-winners-interview-1st-place-tom-van-de-wiele/

* methods: kNN, XGBoost
* separate classifiers for x-y rectangular grids
* >100K classes! -> only model probability for 20 chosen candidates
* features:
  * nearest neighbor calc for chunks
  * place accuracy summary features
  * spatial summary features
  * weekly summary features
* many batches on 48GB workstation
* average 15 XGBoost models to select 20 place candidates from features
* accuracy (variation) showed pattern
* yearly patterns
* kNN features performed really well

== Facebook V - 2nd place

http://blog.kaggle.com/2016/08/02/facebook-v-predicting-check-ins-winners-interview-2nd-place-markus-kliegl/

* GNU parallel to automate parallel runs

== Avito duplicate Ads - 3rd place

http://blog.kaggle.com/2016/07/27/avito-duplicate-ads-detection-winners-interview-3rd-place-mario-gerard-kele-praveen-gilberto/

* methods: XGBoost, RF, LogReg, MinHash, TFIDF + SVD, Follow-the-Leader-Regularized-Learning (FTRL)

== Home depot product search - 1st place

http://blog.kaggle.com/2016/05/18/home-depot-product-search-relevance-winners-interview-1st-place-alex-andreas-nurlan/

* methods: word2vec, Glove, many models, LSA, NMF, Gaussian projections, Random Tree Embedding, Bayesian Ridge, Lasso, XGBoost
* ensemble multiple sampling schemes, since many terms only in test set (average of disjoint terms/disjoint IDs)
* sklearn, keras, hyperopt

== BNP Paribas cardif claims management - 1st place

http://blog.kaggle.com/2016/05/13/bnp-paribas-cardif-claims-management-winners-interview-1st-place-team-dexters-lab-darius-davut-song/

* many XGBoost, RF, ElNet, ANN, SVM, LogReg, ...

== Home depot product search - 3rd place

http://blog.kaggle.com/2016/06/01/home-depot-product-search-relevance-winners-interview-3rd-place-team-turing-test-igor-kostia-chenglong/
https://github.com/ChenglongChen/Kaggle_HomeDepot

* 2-level stacking
* XGBoost, GBT, ExtraTrees, RF, Ridge, KerasDNN, RGF
* doc2vec, word2vec, wordnet sim, ...
* SVD dim red
* XGBoost, TSNE, Keras, Hyperopt, Gensim, ...
* splitting for CV important, so that terms appear in train/test

== Home depot product search - 2nd place

http://blog.kaggle.com/2016/06/15/home-depot-product-search-relevance-winners-interview-2nd-place-thomas-sean-qingchen-nima/

* xgboost + ridge
* put words into category and count
* counting features
* co-occurence, e.g. LSA
* semantic similarity

== Yelp photo classification - 2nd place

http://blog.kaggle.com/2016/05/04/yelp-restaurant-photo-classification-winners-interview-2rd-place-thuyen-ngo/

* pre-trained convnet (first inception-v3, later resnet-152)
* multi-layer perceptron for multi-label and multi instance; network learns how to combine multiple instances; attention units
* tensorflow, torch, lasagne

== March machine learning mania 2016 - 1st place

http://blog.kaggle.com/2016/05/10/march-machine-learning-mania-2016-winners-interview-1st-place-miguel-alomar/

* logaritmic regression, RF

== Homesite quote conversion - 2nd place

http://blog.kaggle.com/2016/05/02/homesite-quote-conversion-winners-interview-2nd-place-team-frenchies-nicolas-florian-pierre/

* features: counts, PCA top, tSNE, IDs from k-means, golden features
* LR+Reg Greedy Forest+NN+ET+XGB -> LogReg + XGBoost + ANN -> avg
* bag NN

== Homesite quote conversion - 1st place

http://blog.kaggle.com/2016/04/08/homesite-quote-conversion-winners-write-up-1st-place-kazanova-faron-clobber/

* XGB, Keras, Lasagne, NoLearn, LibFM, ...
* Categories: IDs, value count, out-of-fold likelihood, one-hot
* Numerical: as is, percentile
* differences of highly-correlation features
* interaction candidates from XGB or LogReg
* feature select: forward, backward, AUC
* added noise to features
* sub-models trained on partitions by categories
* neural networks best stackers; XBG and LR good, but sensitive to feature selection; best to train on subsets of features
* XGB from RF
* XGB num_parallel_tree>1
* XGB interaction embeddings as inputs to NN
* different seeded folds
* tips:
  * use stratified k-fold
  * usually should have only one peak in hyperparameters

== Yelp photo classification - 1st place

http://blog.kaggle.com/2016/04/28/yelp-restaurant-photo-classification-winners-interview-1st-place-dmitrii-tsybulevskii/

* features from Inception-V3, Inception-BN (best), ResNet; use second to last layer (last layer too overfit to ImageNet -> but need Truncated SVD for dimred)
* feature pooling (mean, median), Fisher vectors, VLAD descriptor
* XGB, LR
* multi-label: binary relevance (BR), ensemble of classifier chians (ECC)
* tools: mxnet, torch, vlfeat, xgb, caffe

== Diagnosing heart diseases - 2nd place

http://blog.kaggle.com/2016/04/13/diagnosing-heart-diseases-with-deep-neural-networks-2nd-place-ira-korshunova/
https://github.com/317070/kaggle-heart

* complicating image processing

== Allen AI science challenge - 3rd place

http://blog.kaggle.com/2016/04/09/the-allen-ai-science-challenge-winners-interview-3rd-place-alejandro-mosquera/

* LR 3-class (yes/no/other to account for low confidence predictions)
* gensim word2vec
* expand word2vec with fuzzy string matching
* treat as pair ranking problem

== Telstra network disruption - 1st place

http://blog.kaggle.com/2016/03/23/telstra-network-disruption-winners-interview-1st-place-mario-filho/

* XGB, Keras

== Airbnb new user bookings - 2nd place

http://blog.kaggle.com/2016/03/17/airbnb-new-user-bookings-winners-interview-2nd-place-keiichi-kuroyanagi-keiku/

* XGB, glmnet, R DescTools

== Prudential life insurance assessment - 2nd place

http://blog.kaggle.com/2016/03/14/prudential-life-insurance-assessment-winners-interview-2nd-place-bogdan-zhurakovskyi/

* linear models enough (no worse than XGB)

== Airbnb new user bookings - 3rd place

http://blog.kaggle.com/2016/03/07/airbnb-new-user-bookings-winners-interview-3rd-place-sandro-vega-pons/
https://www.kaggle.com/svpons/airbnb-recruiting-new-user-bookings/three-level-classification-architecture

* XGB, Keras
* own blending (better than LR and XGB if many classes)
* XGB + Keras usually good

== Homesite quote conversion winners - 3rd place

http://blog.kaggle.com/2016/02/29/homesite-quote-conversion-winners-interview-3rd-place-team-new-model-army-cad-quy/

* categorical features: response-rate encoding
* differences of high correlated features
* XGB, h2o RF, Keras
* no feature selection since all variables relevant

== Genentech cervical cancer screening - 1st place

http://blog.kaggle.com/2016/02/26/genentech-cervical-cancer-screening-winners-interview-1st-place-michael-giulio/

* autoencoder
* SQL feature engineering
* XGB + NN

== NOAA right whale recognition - 2nd place

http://blog.kaggle.com/2016/02/04/noaa-right-whale-recognition-winners-interview-2nd-place-felix-lau/
http://felixlaumon.github.io/2015/01/08/kaggle-right-whale.html

* predict parts of whale and crop image to let NN refocus
* VGG-net, ResNet
* Theano, Lasagne, Nolearn

== Winton stock market challenge - 3rd place

http://blog.kaggle.com/2016/02/12/winton-stock-market-challenge-winners-interview-3rd-place-mendrika-ramarlina/

* nu-SVMs, Ridge LR
* peak-to-valley distance

== Rossmann store sales - 2nd place

http://blog.kaggle.com/2016/02/03/rossmann-store-sales-winners-interview-2nd-place-nima-shahbazi/

== NOAA right whale recognition - 1st place

http://blog.kaggle.com/2016/01/29/noaa-right-whale-recognition-winners-interview-1st-place-deepsense-io/
http://deepsense.io/deep-learning-right-whale-recognition-kaggle/

* train NN to find whale head, align on head and crop
* spatial transform networ, triplet training

== Santa's stolen sleigh - 2nd place

http://blog.kaggle.com/2016/01/28/santas-stolen-sleigh-winners-interview-2nd-place-woshialex-weezy/
https://github.com/woshialex/SantaStolenSleigh

* optimization, simulated annealing

== Rossmann store sales - 3rd place

http://blog.kaggle.com/2016/01/22/rossmann-store-sales-winners-interview-3rd-place-cheng-gui/
https://www.kaggle.com/c/rossmann-store-sales/forums/t/17974/code-sharing-3rd-place-category-embedding-with-deep-neural-network

* "entity embedding" for categorical features (similar to semantic embedding)
* Keras

== Avito duplicate ads - 2nd place

http://blog.kaggle.com/2016/08/31/avito-duplicate-ads-detection-winners-interview-2nd-place-team-the-quants-mikel-peter-marios-sonny/

* made script to monitor whether XGBoost overfits on features (histograms, split purity) -> remove those features (which are too specific to training set)
* many feature descriptions
* cluster rows -> add their size as feature
* non-overlapping items validation
* XGB: max depth 20, min child 10, eta 0.1 (high)
* RF and XGB combination of all models

== Avito duplicate ads - 1st place

http://blog.kaggle.com/2016/08/24/avito-duplicate-ads-detection-winners-interview-1st-place-team-devil-team-stanislav-dmitrii/

* only XGB (top 3 with single; averaged with diff seeds for 1st place)
* NCD distance (normalized compression distance)
* text: stemming, lemmatization, transliteration
* LSI of ads union/XOR
* ratios of lengths
* image: BRIEF, HOG, MXNet, AKAZE local visual features
* tools: XGB, VLFeat, OpenCV, MXNet

== Prediction check ins - 3rd place

http://blog.kaggle.com/2016/08/18/facebook-v-predicting-check-ins-winners-interview-3rd-place-ryuji-sakata/

* Naive Bayes

== Dato - 1st place

http://blog.kaggle.com/2015/12/03/dato-winners-interview-1st-place-mad-professors/

== Kazanova solutions

* https://www.kaggle.com/c/malware-classification/discussion/13863
* http://blog.kaggle.com/2015/05/11/microsoft-malware-winners-interview-2nd-place-gert-marios-aka-kazanova/
* https://github.com/kaz-Anova/ensemble_amazon
* http://blog.kaggle.com/2015/12/03/dato-winners-interview-1st-place-mad-professors/
* http://blog.kaggle.com/2016/04/08/homesite-quote-conversion-winners-write-up-1st-place-kazanova-faron-clobber/
* https://mlwave.com/how-we-won-3rd-prize-in-crowdanalytix-copd-competition/
* http://blog.kaggle.com/2016/08/31/avito-duplicate-ads-detection-winners-interview-2nd-place-team-the-quants-mikel-peter-marios-sonny/
* http://blog.kaggle.com/2016/12/15/bosch-production-line-performance-competition-winners-interview-3rd-place-team-data-property-avengers-darragh-marios-mathias-stanislav/

<<<<<<< HEAD

== 2019 Data Science Bowl 1st place
https://www.kaggle.com/c/data-science-bowl-2019/discussion/127469
* 1st place
* many useful agg feats
* just 5 seed x 5 GroupKFold LGBM
* use RMSE instead of actual QWK metric
* include test for training
* feat select by Null importance https://www.kaggle.com/ogrellier/feature-selection-with-null-importances

=======
== LANL Earth quake predictions

https://www.linkedin.com/pulse/my-team-won-20000-1st-place-kaggles-earthquake-corey-levinson/?trackingId=yAsezLzPTDyCuR6z0e4a1g%3D%3D
https://www.kaggle.com/c/LANL-Earthquake-Prediction/discussion/94390
https://www.kaggle.com/dkaraflos/1-geomean-nn-and-6featlgbm-2-259-private-lb
LGB only: https://www.kaggle.com/ilu000/1-private-lb-kernel-lanl-lgbm/
* predict time to a laboratory earthquake (shear stress on rock)
* continuous training data; only few point for time to event prediction
* segment of vibration data for features
* `base_score` parameter tricky to set right (esp. with MAE; e.g. guess base score without many tree; OR iterate long with worse performance)
* LOO CV not working (on LB) -> better Leave out something that resembles test set (feature distribution); here (at first) leave out 1 long, 1 medium, 2 small earthquakes
* test set had more cycles than train set that lasted longer 
* remove median from features (after adding noise), since increasing over time
* noise since(?): e.g. z->z-mean(z); now median(abs(z)) would be skewed unless you first add noise to z?!
https://www.kaggle.com/c/LANL-Earthquake-Prediction/discussion/94390
* posting all zeros showed that test and LB were different on target
* -> before validation, remove short cycles (remove data)
* features on peak and volatility (like other participants)
* 4 features in LGB very good:
** number of peak of support >=2 (in denoised signal)
** 20% percentile of rolling std window 50
** 4th and 18th Mel-frequency spectral coef mean
** a rolling standard deviation, two Mel-frequency cepstral coefficients, number of peaks, autocorrelation, and the rate of the sum of the Hanning Window Fast Fourier Transformation between the raw and denoised signal
** The neural network model used more features. It used the number of times the signal crossed the x-axis, lots of Mel-frequency cepstral coefficients, number of peaks features, rolling standard deviation features, 95th and 99th quantiles, and some FFT features. Some of these were fit on denoised versions of the segments.
* other features only if p>0.05 on KS of train vs test
* sub-select only some earthquakes (out of all 17):
** calculate 9 good features
** sample 10 out of 17 earthquakes 10k times
** compare KS statistic of all selected features (sampled 10 vs test)
** for target>=6 feature distr. look similar (p<=0.05), for all but one feature (-> disregard)
** -> use only 10 specific earthquakes (do 3 fold CV on it); best avg. KS across features
* final models: LGB (objective="fair"; better than optimizing MAE itself), SVR, NN
* multi-task learning by specifying additional losses; other loss for target<0.5; new MAE loss on time-since-failure
* some outliers at ends
* NN best single model; LGB+NN better; SVR helps a bit
* distribution of test preds very similar to OOF preds
* denoise:
def denoise_signal_simple(x, wavelet='db4', level=1):
    coeff = pywt.wavedec(x, wavelet, mode="per")
    #univeral threshold
    uthresh = 10
    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])
    # Reconstruct the signal using the thresholded coefficients
    return pywt.waverec(coeff, wavelet, mode='per')
    
Mel features:
mfcc = librosa.feature.mfcc(z)
mfcc_mean = mfcc.mean(axis=1)
mfcc[18], mfcc[4]
* Anyway, the architecture of this network is fascinating. It is essentially feed-forward (even though it uses LSTM and Conv1D layers). The unusual technique however is that it is fit to multiple objectives: Time to Failure, Time since Failure, and binary indicator TTF < 0.5 seconds. The model is trained to fit to all three of these objectives at once with objectives weights of 8, 1, and 1 respectively.

Because of the multi-objective, the model is forced to create weights that optimize for Time Since Failure and the binary indicator, while focusing mostly on TTF. I think this allowed the model to generalize better to the signal. The performance was better using multi-objective than just fitting to TTF alone.
>>>>>>> e483949a526c922669cbbc8b219ac3f6cd03ed30
