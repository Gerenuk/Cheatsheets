== Summary

* mostly from https://otexts.org/fpp2/[Forecasting: Principles and Practice (Hyndman, Athanasopoulos)]
* centered moving average will smooth out seasons (e.g. 1/8,1/4,1/4,1/4,1/8)
* maybe transform data if want to restrict to range (e.g. log for positive values) or variances are changing (e.g. Box-Cox transform)
** -> remember to do correct back transformation, since with the naive way the mean is biased
* you may consider modelling effects as additive or multiplicative
* forecasting with co-integrated features may require special treatment(?)
* include spike dummy variables for special events
* you need stationary time series for ARIMA-like models -> use KPSS test -> use difference if small p-value (otherwise variance/interval estimates are incorrect)
* you can include features (rather than target alone); in FPP "Dynamic regression models"; basically error term in normal linear regression modelled as ARIMA
** need stationary, and if differencing ideally difference all the same time
** do linear regression first and see if residuals really follow an ARIMA process
* could include lagged features, if lagged effects reasonable
** check lagged cross-correlation (but see whitening below)
* use AICc or CV to choose models (but differencing cannot be found with this since changes data)
* for ARIMA(p,d,q), if p=0 or q=0: can detect model from behaviour of ACF/PACF plots
* test residuals:
** use Ljung-Box test to see if residuals are white noise
** use AR(1) or Cochrane-Orcutt test to check for autocorrelation in residuals
** -> if not, you are missing information and may need a better model (more features, transformations, ...)
* usually prediction intervals are too narrow since many choices not accounted for
* bagging: do many predictions and average
* permanent shocks cannot be estimated from time-series model
* monthly (ideally seasonal) points can improve forecast a lot, even if you actually want to optimize the yearly sum forecast

=== Whitening

* https://onlinecourses.science.psu.edu/stat510/node/75/
* determine a model for asciimath:[hat x(t)]
* apply filter with coef from above model on asciimath:[y] now (filter is operator on left of asciimath:[f(bb"L")(x_t-mu)=epsilon_t])
* check cross-correlation between asciimath:[x-hat x] and filtered asciimath:[y] to determine good lags

=== Weak stationarity

* properties do not depend on absolute time (e.g. fixed time seasons; but may still have cycles due to dependence on past values)
* mean, variance and covariances time-independent

=== Feature preparation summary

* include lagged values
* transform (e.g. Box-Cox) to make variance stable
* transform (e.g. Log) if want to enforce certain range for target
* for back-transforms need special correction to make means unbiased!
* feature selection with AICc or CV possible

=== ARIMA summary

* find out needed order of differencing asciimath:[d]
** with KPSS test (difference more as long small p-value; AIC not suitable here since data different)
** KPSS doesn't seem enough -> test with autocorrelation test
* use AIC or CV to find ARIMAA asciimath:[p] and asciimath:[q] orders; find if including constant
* test residuals for remaining information (Ljung-Box test)
* ARIMA seems to work better than Prophet, but is more sensitive to "level shifts" (will think level shift jumps will repeat after one occurence)

==== Seasonal ARIMA summary

* usually multiplicative version (lag operators multiplied)
* use asciimath:[d_s=1] and asciimath:[d<=1] (?) (for asciimath:[d_s=0] seasonal would face away(?))
* suggested asciimath:[sAR + sMA <= 2] (?)

=== Holt-Winters summary

* make sure to use version with trend and dampening (0.8-0.98)
* parameter estimation may be tricky/unstable

=== ARIMA with features

* difference target and features equally

=== Successful models

* SARIMAX: but does not work if period large (e.g. yearly seasons on weekly data); weaker of irregular/sparse data
* damped Holt-Winters
* auto.arima (R): may need to force to search more exhaustively
* Pyramid (Python auto-arima)
* ETS (Error-Trend-Season): non-linear ETS is not ARIMA
* Seasons could be handled with Fourier terms asciimath:[([[cos],[sin]] (2*pi*k*t/T)] "harmonic regression"
* TBATS
* LSTM, RNN
* XGB, ANN
* GARCH (when variance follows ARMA process)
* CausalImpact (Google)
* Shallow ANN

==== Prophet (Facebook)

* `pip install fbprophet`; needs `cython` and `pystan`
* package is mainly for the masses who cannot tune more complex models
* seems to model seasonality with yearly, monthly, weekly patterns
* usually models addition of
** piece-wise linear trend or saturating growth trend
** seasonality (multiple scales; modelled with Fourier)
** optionally explicit holidays
* can include holidays as explicit input
* modelled in Stan (few lines only)
* resistant to outliers
* no need for regular spacing; fast
* for saturating growth model: can specify total scaling manually
* can explictely define change points
* Prophet has more interpretable parameters (you can include manual changepoints etc.)
* there is a paper describing the approach
* documentation: https://facebook.github.io/prophet/docs/quick_start.html
* https://github.com/facebook/prophet/blob/master/python/fbprophet/forecaster.py[parameters Prophet]
* robust; for normal seasonality ARIMA seems better
* can fit changepoints well - after they occured - but may output too many changepoints (and unstable to tune to just 1 changepoint in a hockey stick curve)


==== Model choice

* linear ETS is special case of ARIMA
* non-linear ETS no equivalent ARIMA
* some ARIMA not a ETS
* all ETS non-stationary, some ARIMA stationary
* harmonic regression or other packages if multiple periods
* special methods for low-integer counts (e.g. Croston's method; https://otexts.org/fpp2/counts.html[FPP Counts])

=== M4 Forecasting competition summary

* Winner: Smyl from Uber https://www.reddit.com/r/MachineLearning/comments/8tajvh/d_hybrid_rnn_model_wins_m4_forecasting_competition/
** combination of ETS and RNN
** excellent prediction interval calibration
* pure ML was bad; hard to beat baseline
* other best models mostly classical techniques (after winner 2-6 places only small differences); they often underestimate prediction interval (apart from top 2 winners)
* helpful to include information from individual series and *whole dataset* (like hierarchical; mb like global trend)
* damped forecast better than naive linear extrapolation
* seasonal time series less noisy
* traditional techniques not so sensitive to amount of data


=== Special models

* Hierarchical/grouped models: when want to model parts and whole
* Vector Autoregression (VAR): when interactions between multiple targets
* Low integer count prediction: there are special methods for this; e.g. predict non-zero values and time between them

== State space models

* http://www.statsmodels.org/dev/statespace.html#module-statsmodels.tsa.statespace

asciimath:[y_t=Z_t alpha_t+d_t+epsilon_t]

asciimath:[alpha_t=T_t alpha_(t-1) +c_t +R_t eta_t]

* Z: endog x states x n_obs (if time-invariant, then no n_obs)

=== AR model as state space models

asciimath:[y_t=[1, 0] alpha_t]

asciimath:[alpha_t=[[phi_1, phi_2],[1 0]] alpha_(t-1) +[[1],[0]] eta_t]

asciimath:[alpha_t=[[y_t],[y_(t-1)]]]

=== Multiplicative seasonal

asciimath:[phi_p(bb"L")bar phi_bar p(bb"L"^s) Delta^d Delta_s^(bar d) * y_t=A(t)+theta_q(bb"L")bar theta_(bar q)(bb"L"^s)*epsilon_t]

where

* asciimath:[phi] are polynomials on the lag operator
* asciimath:[Delta=1-bb"L"] is differencing operator
* variables with "bar" are for seasonal component
* asciimath:[s] is length of season
* e.g. lag polynomial is asciimath:[phi_(p=2)(bb"L")=1-phi_1 bb"L"-phi_2 bb"L"^2]
* a asciimath:[p=2] and asciimath:[bar p=1] would after expansion mean asciimath:[y_t=c+phi_1 y_(t-1)+phi_2 y_(t-2)+bar phi_1 y_(t-12)-phi_1 bar phi_1 y_(t-13) - phi_2 bar phi_1 y_(t-14)+epsilon_t]

=== Other

* http://www.statsmodels.org/dev/statespace.html#unobserved-components[Unobserved components]
* http://www.statsmodels.org/dev/statespace.html#vector-autoregressive-moving-average-with-exogenous-regressors-varmax[VARMAX]
* http://www.statsmodels.org/dev/statespace.html#dynamic-factor-models[Dynamic factor models]

== Plots

* x-axis: month; within month show year development (an total mean for this month, but over years)
* scatter plot (matrix) of y's
* autocorrelation with diff lags
* ACF of residuals

== Tests

* Portmanteau test: test residuals whether they could be white noise
** Box-Pierce test: sum of h residuals squared; use h=10 for non-seasonal, h=2*season for seasonal, but h<=T/5
** Ljung-Box test: more accurate than Box-Pierce; chi^2 test
* Breusch-Godfrey test (Lagrange Multiplier) test for serial correlation: joint hypothesis that there is no autocorrelation in the residuals up to a certain specified order; small p-value -> significant autocorrelation remaining in the residuals.

=== Unit root test for stationarity

* unit root: problem in model estimation (?)
* determine whether differencing needed for stationarity
* KPSS Kwiatkowski-Phillips-Schmid-Shin test: differencing if small p-value
* also check if seasonal differencing needed
* with autocorrelation:
** while coef estimate is still unbiased, it may be off because a feature with predictive power is not in the model
** (hidden) variance of estimates increased
** estimated std errors to narrow; all statistics too optimistic

== Autocorrelation

* trend -> ACF drops slowly
* white noise -> 95% within -+ 2/sqrt(T) [T is length of time series]; if large deviations or too many -> not white noise
* model still unbiased if autocorrelated residuals, but prediction intervals may be wrong
* (Durbin-Watson tests for first order autocorrelation only; also does not work if target is reused as future feature; 0: pos 1st autocorr, 2: no 1st autocorr, 4: neg 1st autocorr)
* AR(1) more powerful test; Cochrane-Orcutt almost same result
* to fix:
** add feature
** change functional form
* if after 1st difference no significant correlations -> random walk
* PACF: last coef of AR model
* AR: ACF damped exp + sines; PACF zero after p lags
* MA: ACF zero after q lags, PACF damped exp + sines
* ARMA: ACF eventually dominated by AR and then dies out; PACF eventually dominated by MA and the dies out

=== AR autocorrelation

asciimath:[x_t=delta+phi_1 x_(t-1) +w_t]

asciimath:[rho_h=phi_1^h]

* mean and variance finite only when asciimath:[phi_1<1]

=== MA autocorrelation

* ACF has non-zero only at order of MA
* MA model best seen in ACF
* invertible if algebraically equivalent to converging (decaying coef) of infinite order AR model
** from multiple models matching a ACF, choose invertible (http://iacs-courses.seas.harvard.edu/courses/am207/blog/lecture-17.html)
* invertibility is programmed into time-series software to enable solution


==== Partial autocorrelation

* conditional correlation
* 1st order PACF same as 1st order ACF

== Box-Cox transform

* asciimath:[(y^lambda-1)/lambda] (or asciimath:[ln(y)] if asciimath:[lambda=0])
* best if seasonal variation about equal sizes
* does not work if asciimath:[y<0]
* forecasts not sensitive on asciimath:[lambda]; but may have large effects on prediction intervals
*! not the the back transform gets the mean biased -> needs correction if want to add up results (e.g. aggregate); need forecast variance

asciimath:[exp(w_t) (1+sigma_h^2/2)]

asciimath:[(lambda w_t+1)^(1/lambda) (1+(sigma_h^2 (1-lambda))/(2(lambda w_t +1)^2))]

* you could pick 3 points from the QQ curve and optimize asciimath:[((x+alpha)^lambda-1)/lambda] to make it linear

== Metrics

* MAE for median
* RMSE for mean
* MAPE: mean of percentage error (only makes sense if meaningful zero); heavier weight on negative error
* (sMAPE: asciimath:[mean(|r_i|*2/(y+hat y)]; not recommended!)
* MASE mean absolute scaled error: scale errors based on training MAE from a simple forecast (e.g. naive forecasts)
* Hyndman, R. J., & Koehler, A. B. (2006). Another look at measures of forecast accuracy. International Journal of Forecasting, 22, 679–688.

== Prediction intervals

* https://otexts.org/fpp2/prediction-intervals.html
* 1 step ahead: stddev about same as that of residuals
* for basic methods, there are equations (e.g. asciimath:[sigma*sqrt(h)] for naive method)
* bootstrap method: iteratively cumulate errors

== Selecting features

* https://otexts.org/fpp2/selecting-predictors.html
* try multiple model versions; e.g. all feature subsets
* or do backward feature selection; not necessarily best model, but always good model
* (forward selection if all features are too many)
* (Adjusted R^2: equivalent to minimizing stderr; selects too many)
* LOO-CV (fast methods)
* AIC (for large T same as CV method)
* AICc: corrected, since for small T selects too many features
* (BIC: for large T similar to leave-v-out where asciimath:[v=T(1-1/(ln(T)-1)])
*-> use AIC/CV, in FPP AICc is used

== Trend

* estimate trend by smoothing or regression
* additive (substract trend) or multiplicative (divide trend)

== Exponential smoothing

asciimath:[hat x:(t+1)=alpha x_t + (1-alpha) hat x_t]

asciimath:[hat x_(t+1)=alpha x_t+alpha (1-alpha) x_(t-1)+ alpha (1-alpha)^2 x_(t-2)+...]

* average between last observation and previous forecast
* same as ARIMA(0,1,1) without a constant -> could fit this and then determine asciimath:[alpha=1+theta_1]
* double exponential smoothing when trend
** combine predictions of trend (differences) and level (values)
** same as ARIMA(0,2,2) without a constant
* capture trend and seasonality
* asciimath:[y_(T+1)=alpha*y_T+alpha(1-alpha)*y_(T-1)+alpha(1-alpha)^2*y_(T-2)+...]
* need to choose smoothing and initial parameters
* Holt extended method to allow for trend: forecast/level/trend equation
* Gardner & McKenzie: added dampening to a flat line (parameter 0.8...0.98)
* can include seasons
* forecast by iterating model (and setting unknown errors to zero)
* data can be non-stationary; actually Exponential smoothing sub-optimal if data is already stationary

* to fit linear trends (but allow for outliers on turns), i.e. log(abs(error)), one could use smoothing_level=0.95, smoothing_slope=0.6, trend="mul", damped=False
* for sqr(error): smoothing_level=0.95, smoothing_slope=0.95, trend="mul", damped=True
* M4 concluded that damped=True is more successful
* often multiplicative trend more stable; additive trend only slightly easier to understand
* curve usually looks like shadowed one month later (due to unpredictability of changes)

* Simple exponential smoothing: just one level equation and smoothing parameter asciimath:[alpha] (how strongly reacts to newest observation); all real future forecasts are the same ("flat forecast"); may look like shifted to right
* Holt linear trend: additionally one trend equation with a new parameter asciimath:[beta]; optionally another damping parameter asciimath:[phi]
* Holt-Winters method: additaionyll seasonality equation with parameter asciimath:[gamma]

== ARIMA

* capture autocorrelation
* needs to be stationary
* AR: model dependence on past values; momentum, mean reversion
* AM: model shocks in error; unexpected temporary events
* ARMA: ignore volatility clustering effects
* cyclic maybe be stationary if depends on on past values and not absolute time
* ACF:
** non-stationary: drop slowly, r1 often large and positive
** stationary: drops quickly
* stationary -> do difference and Ljung-Box test (want large p value)
* https://otexts.org/fpp2/MA.html
* AR: autoregression of target
* MA: regression on past forecast errors
* AR theoretically same as MA with large order
* with some constraints MA model is invertible and also same as AR model with large order; invertible when recent observations more predictive than past(?)
* for stationary data we coefs are restricted:
** AR(1): asciimath:[-1<phi_1<1]
** AR(2): asciimath:[-1<phi_2<1], asciimath:[phi_1+phi_2<1]
** AR(3): complex restrictions
* -> linear model on (differenced) target and past prediction errors
* also includes a constant term
* ARIMA(p,d,q): order of autoregr.,  degree of differencing,  order of moving average
* with c=0: differencing 0/1/2 will model zero/constant/line
* with c!=0: differencing 0/1/2 will model mean/line/quadratic
* for cycles need p>=2; then asciimath:[cos omega = -phi_1(1-phi_2)/(4*phi_2)]
* sometime possible to tell p and q from ACF/PACF plots
* PACF is like last coef in AR(k) model
* finding p/q from ACF/PACF plot:
**! ACF/PACF plot only help if only one of p>0 or q>0
** ARIMA(p,d,0): ACF exp decay or sine; PACF spike p, but none beyond (sharp drop)
** ARIMA(0,d,q): PACF exp decay or sine; ACF spike q, but none beyond (sharp drop)
* estimation methods complex and different software can give different results
* AIC can be used to find parameters (has term asciimath:[p+q+bool(c!=0)])
*! d parameter cannot be found from AIC since it changes data and AIC not comparable
* prediction intervals complicating; for stationary models (d=0) they will converge
*! prediction errors too narrow since variation in parameters, model order, etc not accounted
* seasonal ARIMA:
** (0,1): single spike in ACF, decay in PACF
** (1,0): decay in ACF, single spike in PACF

=== Books

* Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). Time series analysis: Forecasting and control (5th ed). Hoboken, New Jersey: John Wiley & Sons.
* math background: Brockwell, P. J., & Davis, R. A. (2016). Introduction to time series and forecasting (3rd ed). New York, USA: Springer.
* alternatie autoarima: Peña, D., Tiao, G. C., & Tsay, R. S. (Eds.). (2001). A course in time series analysis. New York, USA: John Wiley & Sons.
* long memory ARMA models: Section 5.1 of Shumway and Stoffer

=== auto.arima from R

* https://otexts.org/fpp2/arima-r.html
* unit root test, minimize AIC, fit MLE
* d<=2 determined from KPSS test
* initial (0,0) (2,2) (1,0) (0,1); if d<2: use constant, but also fit (0,0) without constant
* start from best of initial and vary parameters to find better models
* `approximation=FALSE` and `stepwise=FALSE` for more search
* need to be force to search more parameters, if want more brute force

=== Identifying ARIMA models

* http://people.duke.edu/~rnau/arimrule.htm

==== Differencing
* positive autocorr. to high order -> difference more
* lag-1 small -> no more differencing
* lag-1 < -0.5 -> overdifferenced
* best differencing order often when stddev smallest
* no-order differencing usually includes constant term to allow for non-zero mean
* one-order differencing usually with constant term of series has trend
* two-order differencing usally no constant term

==== Identifying AR and MA terms
* PACF sharp cutoff or lag-1 positive (looks underdiff) -> one more AR term (lag beyond which PACF cuts off is indicated number of terms)
* ACF sharp cutoff or lag-1 negative (looks overdiff) -> add MA term
* sometimes AR and MA terms cancel -> try one fewer AR and MA term (esp. if converged after only more than 10 steps)
* beware of multiple AR _and_ multiple MA terms in same model
* unit root in AR (sum of AR coef ~ 1) -> reduce num. AR terms by one and increase differencing
* unit root in MA (sum of MA coef ~ 1) -> reduce num. MA terms by one and reduce order of differening
* if long term forecasts erratic or unstable -> probably unit root in AR or MA
* term too high -> model may explode on short outlier patterns

==== Identify seasonal part
* if seasons -> always use seasonal differencing (otherwise seasonal pattern fades out over time)
* but seasonal diff <= 1
* seasonal diff + nonseasonal diff <= 2
* if seasonal: use asciimath:[D=1] and asciimath:[d=1] (if trend)
* autocorr positive at lag S -> add sAR term
* autocorr negative at lag S -> add sMA term
* sAR + sMA <= 2 (or overfitting and estimation problems)

== VAR/VARMA models

* "Although the model allows estimating VARMA(p,q) specifications, these models are not identified without additional restrictions on the representation matrices, which are not built-in" (?)
* VARMA: includes MA
* special cases of ARMAX

=== ARIMA vs ETS

* linear ETS is special case of ARIMA
* non-linear ETS no equivalent ARIMA
* some ARIMA not a ETS
* all ETS non-stationary, some ARIMA stationary
* AIC cannot be used to compare ARIMA and ETS because diff model class and diff MLE
* but could use time series CV to compare

== ARFIMA Fractional differences model

asciimath:[(1-bb"L")^d x_t=epsilon_t]

asciimath:[|d|<0.5]

* when ACF slowly goes to 0
* hard to interpret

== Threshold models

* https://onlinecourses.science.psu.edu/stat510/node/82/
* different behavior when value of a variable beyond some threshold
* special case of RSM (regime switching models)
* TAR threshold autoregressive models

== Dynamic regression

* https://otexts.org/fpp2/estimation.html
* fit linear model and see if residuals follow AR process
* normal linear regression, but let (auto-correlated) error asciimath:[nu] follow an ARIMA model (with white noise error asciimath:[epsilon])
* cannot just minimize (auto-correlated) asciimath:[nu] since would be biased, statistical tests would be incorrect, AIC would be incorrect, p-values would be too small -> minimize asciimath:[epsilon] from ARIMA model
*! require that target and all of features are stationary (otherwise coefs will not be consistent)
* exception for stationarity requirement: co-integrated (linear combination of non-stationary is actually stationary)
* to keep variable nature, apply same differencing to all of them at once ("model in differences")
* now only need ARMA model (not ARIMA)
* to forecast, you also need to predict features (e.g. by assuming past averages)
* Pankratz, A. E. (1991): "Forecasting with dynamic regression models."
* Generalization of dynamic regression models, "transfer function models": Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). "Time series analysis: Forecasting and control" (5th ed)

=== Trend modeling

* options:
** deterministic trend: constant term in target prediction
** stochastic trend: using differencing (asciimath:[d=1]) in error model (will have wider prediction intervals; but that may be good because it considers changing trends)

== Spectral density

* https://onlinecourses.science.psu.edu/stat510/node/80/

== Seasons with Harmonic regression
* https://otexts.org/fpp2/dhr.html
* for long seasonal trends: dynamic regression with Fourier terms often better; especially since ARIMA does not like large periods (24 for hourly just about fine)
* also differencing daily data on a year ago is just noisy
* -> use harmonic regression for seasons and ARMA for short-term
* only drawback is that seasons are assumed to be constant
* 10/365 for yearly and 3/7 for weekly components works well (?)

== Lagged features

* include lagged features if it makes sense (due to delayed effects)
* choose number of lags by AIC

== Hierarchical/grouped time series

* when target can be split into (hierarchical) categories (e.g. type or region)
* want "coherent" add up of series
* there is always: [all-intermediate-level-targets] = [Summing matrix] * [Bottom-level-targets]
* Wickramasuriya, S. L., Athanasopoulos, G., & Hyndman, R. J. (2018). Optimal forecast reconciliation for hierarchical and grouped time series through trace minimization
* "Grouped time series": Also independent (crossed) categorisation and not just a single tree
* bottom-up approach: just predict bottom level and sum
* top-down approach: predict top level and then predict proportions of categories; may perform well
* check https://otexts.org/fpp2/top-down.html for more
* middle-out approach: predict from middle and expand by summing or using proportions
* generally incoherent forecasts can be made coherent with a "reconciliation matrix"
* optimal reconciliation matrix: https://otexts.org/fpp2/reconciliation.html; "MinT" approach
** no top-down approach can be unbiased
** need to estimate forecast error variance -> use approximations
* further reading:
** Gross & Sohl (1990) provide a good introduction to the top-down approaches.
** The reconciliation methods were developed in a series of papers, which are best read in the following order: Hyndman et al. (2011), Athanasopoulos et al. (2009), Hyndman, Lee, & Wang (2016), Wickramasuriya et al. (2018).
** Athanasopoulos, Hyndman, Kourentzes, & Petropoulos (2017) extends the reconciliation approach to deal with temporal hierarchies.

== Complex, multiple seasonality

* https://otexts.org/fpp2/complexseasonality.html
* `msts`, `mstl` classes in R: specify all frequencies
* or use harmonic regression (include Fourier terms for each frequency asciimath:[T]): asciimath:[sin/cos ((2*pi*k*t/T))]; optimize number of terms by AICc
* or TBATS model by Livera, Hyndman, Synder: combination of Fourier, exp. smoothing, Box-Cox, automated
** however prediction intervals often too wide
** do not allow for features
* more sophisticated:
*+ Hyndman, R. J., & Fan, S. (2010). Density forecasting for long-term peak electricity demand. IEEE Transactions on Power Systems, 25(2), 1142–1153.
** Fan, S., & Hyndman, R. J. (2012). Short-term load forecasting based on a semi-parametric additive model. IEEE Transactions on Power Systems, 27(1), 134–141.

== Vector autoregression (VAR) where bi-directional influence

* target influences features
* influence cross-terms
* choose how many variables (choose only few by information criterion) and how many lags to use
* for co-integrated features use "vector error correction model"
* predict recursively with 1-step predictions
* `VARselect()` from R `vars` package select number of lags; Hyndmann suggests BIC (SC) criterions since AIC chooses too many features
* making cross-influence possible seems raw, but VAR useful when:
** forecasting a collection of related variables where no explicit interpretation is required
** testing whether one variable is useful in forecasting another (the basis of Granger causality tests);
** impulse response analysis, where the response of one variable to a sudden but temporary change in another variable is analysed;
** forecast error variance decomposition, where the proportion of the forecast variance of each variable is attributed to the effects of the other variables
* Vector ARMA generalization but also more complicating

== Neural networks

* use asciimath:[p] lags and asciimath:[k] nodes in a simple (autoregressive) neural network NNAR(p,k)
* `nnetar()`
* since not a stochastic model, no clear prediction intervals -> pick random errors from a distribution while predicting and simulate possible future paths

== Bootstrapping and bagging

* "blocked bootstrap" random sections of remainder time series (i.e. without trend ans season) select and joined together
* -> measure of forecast uncertainty

== Change detection

* Chow test: test whether fitting two linear regression for each time period would give different slopes/offsets
* Package Prophet (Facebook) can fit various seasonalities and underlying piece-wise linear trends with PyStan

== General

* all can be additive or multiplicative
* remove calendar effects (e.g. number of days in month); could be seen as negative autocorrelation before correction(!)
* forecast variable y: dependent, explained variables
* predictor variable x: independent, explanatory variable
* forecast inefficient if residuals autocorrelated or correlated with target
* could use spike or step function for interventions or changes
* lagged values can be useful
* for seasons of period asciimath:[m] could use first Fourier features asciimath:[[[sin],[cos]] (2*pi*t*k/m)]
* regression with Fourier terms: "harmonic regression"
* https://otexts.org/fpp2/regression-matrices.html
* CV statistics can be calculated directly from hat matrix asciimath:[H=X(X'X)^(-1)X'] with asciimath:[CV=1/T sum (e_t/(1-h_t))^2] where asciimath:[h_t] are diagonal elements of hat matrix [hence not needed to fit many models]
* when multi-collinearity: coefficient estimates unreliable, relation between features unreliable; for good software not a problem [unless perfect collinearity?]
* forecast unreliable of new features outside previous range
* "seasonally adjusted": remove seasonal
* if season order 4: 2x4 MA (moving average, first 2 then 4) will give weights (1/8,1/4,1/4,1/4,1/8) and hence remove quarterly season; MA is centered here
* classical decomposition: detrend with MA (m for odd, 2xm for even order season) -> remove averaged curve from original; average out months
* for monthly/quarterly seasons, some institutions have developed simple averagine methods: https://otexts.org/fpp2/x11.html
* https://otexts.org/fpp2/stl.html[STL Seasonal and Trend decomposition using Loess]
* FPP prefers to use AICc for minimization
* one could plot complex root with unit circle (https://otexts.org/fpp2/arima-r.html) to see how stationary and invertible the model is -> roots close to unit circle may make the model unstable (but many implementations only return well-behaved models)
* for forecasting: use predicted values as features; use zero for future errors; https://otexts.org/fpp2/arima-forecasting.html
* forecasting with co-integrated: "Applied time series modelling and forecasting" (Richard Harris and Robert Sollis)
* almost all model give too narrow(!) prediction intervals since some uncertainty not considered (e.g. ETS 95% may cover only 70%)
* Bagging: predict many bootstrapped versions and average
* one could need 13 Fourier terms with asciimath:[2*pi*j*t/52.18] if you have weekly data and yearly seasons
* for holiday effects use dummy variable (but STL, ETC, TBATS do not allow features) -> rather use dynamic regression models
* for predicting low integer counts: https://otexts.org/fpp2/counts.html
** Croston's method: often used, even though not great; predict non-zero values and also length of zero count runs
** better methods for low count data: Christou, V., & Fokianos, K. (2015). On count time series prediction. Journal of Statistical Computation and Simulation, 85(2), 357–373.
* to limit forecasts to a range, one could transform them https://otexts.org/fpp2/limits.html (e.g. log for positive counts)
* if you want to predict aggregates (e.g. 3 month sum when having monthly data), you can just sum, but may need to correct prediction intervals (possible correlations)
* ETS and ARIMA with differencing allow for condition changes, but dynamic regression models have no evolution
* think of if you want 1 or multi step forecasts in train/test data
* missing values can be estimated
* you could replace outliers with missing values
* ideally do not use the future information to decide on missing or outlier values
* more possible time series issues: last chapter of "Principles of business forecasting" (Ord, J. K., Fildes, R., & Kourentzes, N. (2017))

* hard to beat SARIMAX (e.g. Python https://github.com/tgsmith61591/pyramid)

https://vimeo.com/274421819
* multi-variate time series
* to model effects from y on x: use vector for y -> components affect each other
* Vector Autoregressive Models
** only for stationary; but sometimes differencing makes stationary
** if non-stat. but linear combination is stat (co-integration): can do "Vector error correction model")
** sm.tsa.VARMAX
* explore: time series itself, histogram of y, ACF, PACF(!)
* weak stat: first two moments constant (mean, [co]var)
* LSTM: does not need stationarity

== GARCH model (Autoregressive Conditionally Heteroskedastic Model)

* http://www.blackarbs.com/blog/time-series-analysis-in-python-linear-models-to-garch/11/1/2016
* most often rather variance bursts instead of trends
* ARCH: AR(p) model for Variance
* GARCH(n,m): ARMA for variance
* check PACF of squared values
* https://onlinecourses.science.psu.edu/stat510/node/85/

asciimath:[Var(y_t|y_(t-1))=sigma_t^2=alpha_0+alpha_1 y_(t-1)^2]

After centering (remove mean or trend) to zero mean:

asciimath:[y_t=sigma_t epsilon_t]
asciimath:[sigma_t^2=alpha_0+alpha_1 y_(t-1)^2]

* this model is causal (can be converted to MA model) only when asciimath:[alpha_1^2<1/3]
* asciimath:[y_t] white noise when asciimath:[0 <= alpha_1 <= 1]

== Transfer function

* model y in terms of lags of y and x

== Intervention quantification

* https://onlinecourses.science.psu.edu/stat510/node/76/
* determine how much mean jumped after a known intervention

== Cookbook

* check ACF of residuals
** see tests above
** helpful if normal, for easier prediction intervals
* residuals correlate with feature -> may need non-linear feature transformation
* any feature which is not in model correlates with residuals? -> include
* residuals vs fitted values -> if pattern, then heteroscedacticity and maybe transform target
* outliers which are "influential observations"?
* for ARIMA:
** https://otexts.org/fpp2/arima-r.html
** plot
** stabilize variance if needed
** check stationary with ACF (needs to drop quickly)
** make stationary by differencing if needed
** check ACF/PACF if (p,0) or (0,q) model appropriate -> use AICc to find better models
** check residuals by ACF and Portmanteau test to verify that they are white noise
* for CV test set you can use business metric again and also different differencing parameters

    from statsmodels.tsa.stattools import kpss
    p_values = kpss(s)[1]
    if p_value > 0.05:
        print("Stationary. Good!")

* alternatively `adfuller(s)[1] < 0.05` wanted (but this test is weaker)

== Stationarity

* need for models which rely on a process of constant conditions
* could be due to global trend, seasonality
* could have variation in: mean, variance, autocorrelation (i.e. frequency)
* could test by plot of moving average
* Dickey-Fuller test: null hyp. is the TS is non-stationary
** `statsmodels.tsa.stattools.adfuller(ts, autolag="AIC")`
* for details wee "Brockwell and Davis"
* make stationary:
** differencing
** or decomposition (subtract trend)
** scaling (e.g. log) to equalize variance
* http://people.duke.edu/~rnau/whatuse.htm


== Trend estimation in time series signals
https://www.youtube.com/watch?v=likDxYXhNQY
* median filtering: more robust to outliers, non-linear; could shadow mid-term if large window
* exponential weighted moving average: not robust to outliers; pd.stats
* bandpass filtering: extract mid-term (high=noise; low=bias); similar to Hodrick-Prescott filter; scipy.signal
* hodrick-prescott filter:
** one of best for trends
** decompose into trend (mid-term growth) and cyclical component
** minimize optimization function with smoothness constraint
** good when noise Gaussian
** bandpass at heart
** cycle: short term; trend: medium term
** linear
** Python statsmodels
* L1 trend filtering:
** L1 error
** more robust
** piecewise linear!
** nonlinear
** good when noise exponentially distr.
** computationally expensive
** library from Bugra

== Other packages
* https://github.com/blue-yonder/pydse

== Tips

* GARCH, ARCH, regression, ARIMA
* Theta method: Theta method was equivalent to an average of a linear regression and simple exponential smoothing with drift
* outlier removal (e.g. once-off events)
** local smoother; use residuals to determine seasonality; compare double smoothed to original data and drop outside 3 sigma
* MASE can be very sensitive to a few series; worth concentrating on these if you really have to optimize for MASE
** maybe then use only linear growth (not exponential); additive seasonality (not multiplicative) - otherwise small weights
* include a global trend (e.g. from across all series; if data from one industry)

== Individual solutions

* http://mabrek.github.io/blog/kaggle-forecasting/

=== Kaggle Competitions about Time Series

* https://www.kaggle.com/c/tourism2
* https://www.kaggle.com/c/web-traffic-time-series-forecasting[Web traffic time series forecasting]
** http://blog.kaggle.com/2010/11/24/how-we-did-it-jeremy-howard-on-winning-the-tourism-forecasting-competitoin/[Winner solution]; LSTM
** similar with CNN: https://github.com/sjvasquez/web-traffic-forecasting
** median of last week a benchmark that was hard to beat
** used https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error[SMAPE] metric which is https://www.kaggle.com/cpmpml/smape-weirdness[weird] (discontinuous at 0 and non-convex)
* https://www.kaggle.com/c/how-much-did-it-rain[How much did it rain?], https://www.kaggle.com/c/how-much-did-it-rain-ii[How much did it rain? (II)]
** http://blog.kaggle.com/tag/how-much-did-it-rain/[Winners]; winner had large RNN
* https://www.kaggle.com/c/online-sales[Online Product Sales]
* https://www.kaggle.com/c/rossmann-store-sales[Rossmann store sales]
** http://blog.kaggle.com/tag/rossmann-store-sales/[Winners]
* https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting[Walmart Recruiting - Store Sales Forecasting]
** https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/discussion/8125[1st place]; https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/discussion/8023[2nd place]
* (list from https://machinelearningmastery.com/challenging-machine-learning-time-series-forecasting-problems/)
* https://www.kaggle.com/c/ams-2014-solar-energy-prediction-contest[AMS 2013-2014 Solar Energy Prediction Contest]
** https://www.kaggle.com/c/ams-2014-solar-energy-prediction-contest/discussion/6321#33783[1st place]; ensemble of GBM
* https://www.kaggle.com/c/GEF2012-wind-forecasting[Global Energy Forecasting Competition 2012 - Wind Forecasting]
* https://www.kaggle.com/c/dsg-hackathon[EMC Data Science Global Hackathon (Air Quality Prediction)]
** http://blog.kaggle.com/2012/05/01/chucking-everything-into-a-random-forest-ben-hamner-on-winning-the-air-quality-prediction-hackathon/[1st place]; ensemble of RFs on lagged data

=== Kaggle Tourism Forecasting

I then fitted a weighted regression (weighted the most recent observations the most heavily) combined with weighted additive seasonality (again weighting the most recent observations the most heavily) on all but the last 2 years of each series. A simple optimiser found the optimal weighting of each in order to predict the final 2 years. This weighted model was then applied to the full data set to create predictions. The intercept of the weighted regression was adjusted such that the residual on the final observation was always zero - this was important for ensuring that the series with a low denominator in the MASE metric were forecast as accurately as possible.

=== Kaggle Air Quality predition

I took the lagging N components from the full time series (N=8 for the winning submission, which was selected arbitrarily) as features, then each of the 10 prediction times and 39 pollutant measures as targets. I then trained 390 Random Forests over the entire training data, one for each predicted offset time-pollutant combination. The Random Forest parameters were selected so that the models would be quick to train. https://github.com/benhamner/Air-Quality-Prediction-Hackathon-Winning-Model[code]
