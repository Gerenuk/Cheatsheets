== ML Strategies
* know what to tune to get some effect ("orthogonalization")
* maybe also some minimum extra metrics
* cross-val and test set should be same distribution
* if a lot of data: use more percent in train data (10000 in test set?)
* not having test set may be ok, if not final performance needed
* Bayes optimal error is limit
* Compare to human error performance (proxy for Bayes error) to judge train error (need bias or variance?)
* DL quite robust to random label noise
* count incorrect labels (remember that test set also may need treatment)
* if train and test have to be different (semi-unsupervised): may draw conclusions about bias/variance to predictions of part of train set
* multi-task learning: e.g. car needs to detect many things
* sometimes breaking algorithm into steps helps (as compared to full end-to-end; may need to much data)

