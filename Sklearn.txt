////
Sklearn 0.18
http://scikit-learn.org/dev/whats_new.html
////

= Scikit-learn

:toc:

== Sklearn Guide

* Support for Python 3.3 dropped in sklearn 0.20
* Use https://pypi.python.org/pypi/ibex[Ibex] for Scikit-learn and Pandas interoperability:
** Named columns throughout pipeline (e.g. SelectKBest, Union, ...)
** Easy pipeline and union with `|` and `+`
** Pandas operations during pipeline
* install:
** `pyamg` for SpectralClustering
* `model.decision_function()`: When models gives "score" (e.g. with threshold 0) instead of a probability
* `model.score()`: between 0 and 1; probability of X for unsupervised

== Supervised learning

* LinearRegression: SVD with O(np^2) (n>=p)

=== Ridge Regression

* built-in support for multi-variate regression (y is [n_samples, n_targets])
* RidgeCV:
** built-in CV; similar to GridSearchCV but efficient form of LOO with GeneralizedCV
* RidgeClassifierCV:
** currently only n_features > n_samples efficiently

=== SVM

* using libsvm and liblinear libraries
* (+) effective in highdim
* (+) effective of dim>samples
* (+) subset of training set for decision
* (+) kernels
* (-) if features >> samples: poor performance
* (-) sklearn.svm supports dens (np.ndarray) and sparse (scipy.sparse)
* (-) for best performance use C-ordered np.ndarray or scipy.sparse.csr_matrix with float64
* sklearn.svm.l1_min_c: calc lower bound for C to get a non null model
* sample_weight: SVC, NuSVC, SVR, NuSVR, OneClassSVM
* multiclass: SVC, NuSVC (different param and diff mathematical formulation), LinearSVC (other implementation for linear case only)
** multiclass by 1-vs-1 ( c(c-1)/2 classifiers for c classes constructed) for SVC and NuSVC
** 1-vs-rest for LinearSVC
** LinearSVC can also do `crammer_singer` multiclass which is consistent (unlike the faster 1-vs-rest)
* NuSVC: mathematically equiv to SVC (nu if upper bound for training errors and lower bound for supp vec)
* OneClassSVM: for novelty detection, no labels needed
* compute and storage requirements increase rapidly with num of training vectors O(n_features*n_samples^(2...3))
* LinearSVC (by liblinear) much faster than SVC (libsvm)
* probabilities if `probabilies=True` (for binary Platt calibrated)
** but CV of Platt scaling slow and probability might be inconsistent with argmax
** better with `decision_function` instead of `predict_proba`
* `class_weight`: in `SVC`; not in `NuSVC`
* `sample_weight`: SVC, NuSVC, SVR, NuSVR, OneClassSVM
* Tips (http://scikit-learn.org/dev/modules/svm.html#tips-on-practical-use):
** for (Nu)SV(C/R) use C-ordered float64 (otherwise copies); check `flags`
** for Liblinear copied to internal sparse repr anyway (use SGDClassifier if you need to avoid copy)
** if large RAM: increase cache_size (for kernel)
** `C=1` good start; decrease if many noisy instances
** need scaling
** `nu` approximates fraction of training errors and support vectors
** class_weights="balanced" for unbalanced data
** LinearSVC uses random numbers (decrease tol to avoid that)
** LinearSVC(loss='l2', penalty='l1', dual=False) for sparse model
* own kernels by python function or pre-computing gram-matrix (but support_vectors_ will be empty; only indices stored in suppprt_)

=== LASSO

* Least Absolute Shrinkage and Selection Operator
* Lasso sklearn: uses coordinate descent
* similar to forward stepwise regression, but instead of including variables at each step, the estimated parameters are increased
equiangular to each one's corr with the residual
* `lasso_path` to compute full path (http://scikit-learn.org/dev/modules/generated/sklearn.linear_model.lasso_path.html#sklearn.linear_model.lasso_path)
* LassoLars sklearn: when weight vector estimated is very sparse
** exact solution unlike method by coordinate descent -> piecewise linear as function of norm of coef
* LassoCV: for high-dim with many collinear regressors
* LassoLarsCV: explores more relevant values of alpha; faster if samples much fewer than observations
* LassoLarsIC: uses AIC or BIC, faster; but needs estimation of degrees of freedom, only works asymptotically (large samples), assumes model is correct; break if badly conditioned (more features than samples)
* MultiTaskLasso: sparse model for multiple `y` jointly (http://scikit-learn.org/dev/modules/generated/sklearn.linear_model.MultiTaskLasso.html#sklearn.linear_model.MultiTaskLasso)
** Example on http://scikit-learn.org/dev/auto_examples/linear_model/plot_multi_task_lasso_support.html#sphx-glr-auto-examples-linear-model-plot-multi-task-lasso-support-py
* LARS:
** (+) efficient if p>>n (dim > num points)
** (+) as fast as forward selection
** (+) produces full piecewise linear solution path (good for CV)
** (+) if two variables have same correlation with response, their coef increase at similar rate (intuitive)
** (+) easy to modify for lasso
** (-) maybe sensitive to noice since iterative refitting of residuals (see Efron et al 2004 Annals of Statistics)
** estimator `Lars`; low-level `lars_path`

=== ElasticNet

* inherts stability under rotation from Ridge
* ElasticNetCV: find `alpha` and `l1_ratio` by CV
* MultiTaskElasticNet: estimate sparse coef of multiple targets

=== Orthogonal Matching Pursuit

* OrthogonalMatchingPursuit
* L0 pseudo-norm: number of non-zero coef limited
* greedy algorithm, includes at each step the atom most highly correlated with the current residual
* linear least-squares regression, with limited number of non-zero coef

=== Logistic Regression

* binary, OvA; L1, L2
* multiple solvers:
** liblinear solver:
*** shipped with sklearn
*** does OvA for multiclass
*** sklearn.svm.l1_min_C: calc min C for non-zero model
** other solvers:
*** only L2
*** converge faster for high dim
*** true multiclass with `multinomial`
* use-cases:
** small data or L1: `liblinear`
** multiclass or large data: `lbfgs`, `sag`, `newton-cg`
** very large data (`n` and `p`): `sag`
* `LogisticRegressionCV`: builtin CV for `C`
** `newton-cg`, `sag`, `lbfgs` faster for high dim dense data due to warm start
** for `multi_class="ovr"`, optimal `C` obtained for each class
** for `multi_class="multinomial"`, optimal `C` obtained by min cross-entropy


=== Stochastic Gradient Descent (SGD)

* (+) efficient
* (+) ease to implement; many tunings possible
* (-) need hyperparameters (regularization, num of iterations)
* (-) sensitive to feature scaling -> `StandardScaler`
* `SGDClassifier` and `SGDRegressor`
* use `shuffle=True`
* `class_wright` and `sample_weight`
* averaging with `average=True` (for LogReg also SGD averaging in `LogisticRegression`)
* SGDRegressor: for samples>10000 (otherwise better Ridge, Lasso or ElasticNet)
* theory: runtime to get desired accuracy does not increase with training set size
* O(n*p*iter)
* often `n_iter=np.ceil(10**6 / n)`
* GridSearchCV `alpha=10.0**np.arange(1,7)`
* if PCA before -> scale feature by constant C (from average L2 norm)
* averaged SGD best with larger number of features and higher `eta0`
* multiclass with OvA
* `.decision_function([x])` to get signed distance to hyperplane
* for sparse data best to use `scipy.sparse.csr_matrix` (sparse results slightly different from dense due to shrunk intercept coef)
* learning rate 1/(alpha*(t0+t))



=== Nearest Neighbours Classifier

* RadiusNeighborsClassifier:
** (+) better if data non-uniform
** (-) weaker if curse of dim
* KDTree: partitions data by cartesian coordinates
** (-) inefficient for D>20
* Ball Trees: partition data by nested hyperplanes
* choice guides: http://scikit-learn.org/dev/modules/neighbors.html#choice-of-nearest-neighbors-algorithm
* `leaf_size`: when to switch to brute-force
* `NearestCentroid`: very simple; represent classes by centroid
** `shrink_thresold`: feature values divided by within-class variance
* LSHForest: approx nearest neighbor search
** `n_estimators`: should be large enough (10-50)
** `n_candidates`: adjusted by query time
** sublinear; When more than 20000 samples?
** hash with multiple functions

=== Perceptron

* by default no learning rate, not regularized
* updates only on mistakes -> slightly faster than SGD with hinge loss; models sparser

=== Passive Aggressive Algorithms

* classifier and regressor for large-scale learning
* do not require learning rate (same as Perceptron)
* include regularization `C` (unlike Perceptron)

=== Gaussian Processes

* http://scikit-learn.org/dev/modules/gaussian_process.html
* generic supervised learning to solve regression problems
* also extend to probabilistic classification
* (+) prediction interpolates observations
* (+) prediction probabilistic so that one can compute empirical confidence intervals (useful for refiting)
* (+) difference linear regression models and correlation models can be specified (arbit stationary models possible)
* (+) can do complex kernels
* (-) not sparse
* (-) loses efficiency in high dim if more than a few dozen features
* prior mean assumed to be zero for `normalize_y=False` and training data for `normalize_y=True`
* `n_restarts_optimizer` to restart due to local minima (first start from initial parameters in kernel)
* GPR vs KRR:
** GPR hyperparam learning (instead of gridsearch)
** GPR generative models can provide confidence intervals and posterior samples (instead of just predictions)
** GPR learns noise level in data
* `GaussianProcessClassifier`: logistic link function; easily approx in binary case; Laplace approx since not Gaussian
* Kernel API: http://scikit-learn.org/dev/modules/gaussian_process.html#kernels-for-gaussian-processes

=== Discriminant analysis

* LDA, QDA: does not assume equal variances such as nearest centroid classifer
* (+) closed-form solutions
* (+) multi-class
* (+) no hyper-parameters
* can be used for dimensionality reduction -> dimensionality less than number of classes (use for multiclass) (http://scikit-learn.org/dev/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform)
* LDA: all covariances the same
* QDA with covariances diagonal is equiv to Gaussian Naive Bayes
* `shrinkage` to improve covariance estimation when few samples
* default solver `svd`: but cannot so shrinkage
* `lsqr` solver only for classification but with shrinkage
* `eigensolver` solver optimizes between class scatter to within class scatter ratio
** for classification and transform and shrinkage
** needs to compute covariance matrix -> not for high number of features

=== Automatic Relevance Determination:

* similar to Bayesian Ridge Regression, but can lead to sparser signals
* different prior: not spherical Gaussians, but axis-parallel elliptical (each corrdinate has own standard deviation)

=== Bayesian Regression

* hyperparameters tuned to data with uninformative priors
* (+) adapts to data
* (+) more robust to ill-posed problems (that OLS)
* (-) inference can be time-consuming
* see Pattern Recognition and ML (Bishop)
* see MacKay: Bayesian Interpolation (http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.27.9072&rep=rep1&type=pdf)
* BayesianRidge: spherical Gaussian for coef, gamma distr for alpha and lambda
* Automatic Relevance Determination `ARDRegression`: elliptical Gaussian for coef (each coor own lambda)
** can lead to sparser weights (known as Sparse Bayesian Learning or Relevance Vector Machine)

=== Robust regression

* `RANSAC`: fit on random subsets (http://scikit-learn.org/dev/modules/linear_model.html#ransac-random-sample-consensus)
** can have functions to reject some models or data subsets
* `TheilSen`: Generalization of median to many dimensions
** for univariate breakdown 29% of data points
** complexity (n choose n_subsamples)
* `HuberRegressor`: squared loss until cutoff; Absolute deviation thereafter
** different from SGD with huber: scale invariant, more efficient on small data
* comparison
** `HuberRegressor` faster (and more robust) than others unless n>>p
** `RANSAC` faster then `TheilSen` and scales better with `n`
** `RANSAC` deals better with outliers in y direction (most common situation)
** `TheilSen` better for medium-sized outliers in X direction (if not too many dimensions)
* -> in doubt use `RANSAC`

=== Kernel Ridge Regression

* L2 loss with Kernel trick
* (+) can be done in closed-form; faster than SVR for medium sized data (~1000 samples)
* (-) models not sparse; prediction with SVR always faster

=== Decision Trees

* (+) simple to understand and visualize
* (+) little data prep
* (+) numerical and categorical data
* (+) possible to validate with statistical tests
* (-) overcomplex trees overfit
* (-) unstable (small changes changes whole tree)
* (-) NP hard -> heuristics used
* (-) XOR and parity hard to learn
* (-) biased trees if some classes dominate -> recommended to balance classes
* support multiclass by computing average split reduction over all classes
* Tips:
** not too many dim
** PCA/ICA/Feature selection before
** sklearn uses Fortran ordered `np.float32`
** sample required double with new level depth
** start with `min_samples_leaf`
** balance data beforehand
** if samples weighted, optimize tree structure with weight-based pre-pruning, e.g. `min_weight_fraction_leaf`
** for sparse data use `csc_matrix` for fit and `csr_matrix` for predict
* C4.5 (successor to ID3): also handles continuous attr
** converts to if-then rules and orders them by accuracy
** pruning by removing rule's precondition if accuracy improved this way
* C5.0: less memory, smaller rule sets; C4.5 more accurate
* CART: like C4.5 but also supports numerical target; doesnt not compute rule sets; binary splits only
* sklearn uses CART

dot_data=tree.export_graphviz(clf, out_file=None, feature_names=.., class_names.., filled=True, rounded=True, special_characters=True)
graph=pydotplus.graph_from_dot_data(dot_data)
IPython.display.Image(graph)


=== Naive Bayes

* When does NB work: H.Zhang 2004 The optimality of Naive Bayes Proc FLAIRS (http://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf)
* `predict_proba` generally bad
* very fast
* `GaussianNB`: Gaussian likelihood
* `MultinomialNB`
* `BernoulliNB`: each feature binary
* `partial_fit`: but use large chunks

=== Semi-supervised learning

* `sklearn.semi_supervised`
* use label `-1`
* label propagation:
  * classification and regression
  * kernel: rbf (dense matrix, can be slow), knn (more sparse)
  * construct similarity graph
  * `LabelPropagation`: uses data graph without modification 
  * `LabelSpreading`: regularized loss, more robust to noise; iterates over modified version of graph (spectral clustering)


=== Isotonic regression

* tries to find monotic regression (e.g. strictly increasing) curve fitting



== Ensemble methods

* Bagging:
** `BaggingClassifier`, `BaggingRegressor` meta-estimator
** reduce variance due to averaging
** best with strong and complex models
** Pasting (subsets), Bagging (subsets with replacement), Subspaces (feature subsets), Patches (feature and data subsets)
* Boosting:
** best with shallow models

* `RandomForestClassifier`, `RandomForestRegressor`:
  * bias increased, but variance decreased more
  * sklearn averages probabilistic predictions (instead of single class votes)
* `ExtraTreesClassifier`, `ExtraTreesRegressor`:
  * extremly randomized trees
  * also subset of features used at splits; but feature thresholds taken randomly instead of optimally; still best feature selected
* `max_features`:
  * subset of features used at split
  * for regression `max_features=n_features`
  * for classification `max_features=sqrt(n_features)`
* `n_estimators`: more is better, but usually slower; also some threshold
* `max_depth`: usually `None` best with ``min_smaples_split=1`
* feature importance:
  * relative rank in tree (expected fraction of the samples the contribute; top in tree is better)
  * `feature_importances_` sum to 1
    
=== Tree ensembles

* trees support multiclass (http://scikit-learn.org/dev/modules/tree.html#tree-multioutput)
* `RandomForestCla/Reg`
** average probs (instead of class voting)
* Extremly Randomized Trees `ExtraTreesCla/Reg`:
** best threshold from set of _randomly_ determined thresholds used

=== AdaBoost

* repeated modified version of data for weak learners; weighted majority vote
* changing point weights; higher weights for misclassified
* `AdaBoostClassifier`: Adaboost-SAMME and Adaboost-SAMME.R
* `AdaBoostRegressor`: Adaboost.R2
* main parameters to tune: `n_estimators`, complexity of base (`max_depth`, `min_samples_leaf`)

=== Gradient Tree Boosting

* (+) generalization of boosting to arbitrary differentiable loss functions
* (+) can handles mixed data
* (+) good predictive power
* (+) robust to outliers
* (-) hard to parallelize
* `GradientBoostingClassifier`: binary and multi-class via deviance loss function (negative binomial log-likelihood)
* `learning_rate` controls overvitting via shinkage (small usually better, e.g. <=0.1; interacts strongly with `n_estimators` -> chose by early stopping)
* `subsample` for bagging: subsample alone not good, but with shrinkage often an improvement
* multi-class needs a tree per class at each iteration -> rather random forest when many classes
* `warm_start=True` to add more estimators
* `GradientBoostingRegressor` different loss functions:
  * least squares `ls`
  * least absolute deviation for robust ``lad`
  * Huber loss `huber` which combines least squares and least absolute deviation; parameter `alpha` to control sensitivity to outliers
  * quantil loss with parameter `alpha` (can be used to create prediction intervals)
* train error at iterations `.train_score_`; test error at iterations `.staged_predict()` -> determine `n_estimators` for early stopping
* has `feature_importances_`
* some initial model used (`init` argument)
* solve iterative models by steepest descent
* `learning_rate` scale gradient down by factor
* steps trained on `subsample` of all data (use only with shrinkage)
* subsampled features by `max_features` (esp. faster)
* `oob_improvement_[i]` for OOB test estimates (usually pessimistic estimates; use CV is enough time)


=== Voting classifier

* `voting="hard"` for majority; `voting="soft"` for argmax of sum of predicted probabilities for classes
* optionally `weights=[..]` for custom classifier weights

		VotingClassifier(estimators=[("name1", clf1),..], voting=..)

    
== Multiclass and multilabel algorithms

* `sklearn.multiclass`
* meta-estimators that turn binary or regressor into multiclass
* multiclass: one label each
* multilabel: multiple labels allowed per sample
** use `MultiLabelBinarizer().fit_transform(y)`
* multioutput-multiclass: handle jointly several classification tasks; 2D array for y
* useful only if experiment with multiclass strategies
* inherent multiclass: NB, LDA/QDA, DT, RF, NN
* inherent multilabel: DT, RF, NN
* OvO: SVC
* OvA: linear (except SVC)
* multilabel+multiclass: DT, RF, NN
* currently no metrics in  `sklearn.metrics`
* wrapper:
** `OneVsRestClassifier`
** `OneVsOneClassifier`
*** n_cl*(n_cl-1)/2 -> slow, but good if underlying algo complex in n_instances
** `OutputCodeClassifier`: output code classifier;
*** `code_size` for number of classifiers (percent of total classes); in theory `log2(n_cl)/n_cl`, but in practice too small
*** `code_size>1` possible for error-correcting codes

    clf=OutputCodeClassifier(clf, code_size=..)
    clf=MultiOutputRegressor(clf)



== Feature selection

* http://scikit-learn.org/dev/modules/feature_selection.html
* transform methods for univariate feature selection:
  * `SelectKBest`
  * `SelectPercentile`
  * `SelectFpr` false positive rate, `SelectFdr` false discovery rate, ``SelectFwe` family-wise error
  * take input scoring function; return univariate p-values
  * regression: `f_regression`
  * classification: `chi2` (only this useful for sparse data), `f_classif`
* for sparse data `chi2`, `mutual_info_regression`, `mutual_info_classif` can keep sparsity
* recursive feature elimination:
  * `RFE`
  * `RFECV`: with cross validation
* you can use `Lasso`, `LogisticRegression` or `LinearSVC` with L1 norm
* `LassoCV` and `LassoLarsCV` tends to include too many features; `LassoLarsIC` too few
* randomized sparse models:
  * usually sparse model select only one of multiple correlated features
  * -> randomization (perturb design matrix, sub-sampling,...)
  * `RandomizedLasso` (reg), `RandomizedLogisticRegression` (cla)
  * to be better than standard F statistics at detecting non-zero features, the ground truth should be sparse (there should be only a small fraction of non zero)
  * `lasso_stability_path` to get full path of stability scores
* can use tree feature importances




== Calibration

* `CalibratedClassifierCV`
** `cv="prefit"` if already fit
** can do multiclass if base classifier can
* with isotonic or sigmoid
* calibration_curve
* LogisticRegression: good calibration
* GaussianNB: pushes to 0 or 1
* RandomForestClassifier: peaks at 0.2, 0.9 (0/1 rare)
* LinearSVC: even worse tha RF, since max margine focusses on margin




== Neural network models

* no GPU support; better other projects
* `MLPCla/Reg`: multi-layer perceptron
* standardize data before
* solvers:
** L-BFGS faster for small data
** Adam for large data robust
** momentum (SGD, nesterov) good if correct learning rate
* `warm_start=True` and `max_iter=1` for manual steps



== Unsupervised learning

* Agglomeration: merge similar features
* top-down bad if many clusters
* bottom-up good if interesting clusters are small
* Ward clustering: criterion like in k-Means
* message passing (AffinityPropagation)
* kNN: average knT (n=#samples, T=#iterations); worst n^(k+2/p) (p=#features)
* spectral (when highly non-convex) -> install pyamg to speed up!
* mean shift; use estimate_bandwidth
* biclustering: find checker-board structure
* Clustering comparison: http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html
* Clustering evaluation (against randomness): http://scikit-learn.org/stable/auto_examples/cluster/plot_adjusted_for_chance_measures.html
* Recursively merge features: http://scikit-learn.org/stable/modules/generated/sklearn.cluster.FeatureAgglomeration.html
* Silhouette: http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html

=== Clustering

* use-cases for methods: http://scikit-learn.org/dev/modules/clustering.html#overview-of-clustering-methods
* MiniBatchKMeans: almost as good as KMeans
* AffinityPropagation:
** chooses number of clusters
** complexity for time and memory n^2
* MeanShift: find blobs in smooth density of samples
* SpectralClustering: low-dim embedding of affinity matrix
** fast if affinity matrix sparse and pyamg module installed
** if distances not well-behaved distance, use transform like np.exp(-beta*d/d.std())
** `assign_labels="kmeans"`: finer details, but randomized results
** `assign_labels="discretize"`: even parcels, but deterministic
* Hierarchical clustering `AgglomerativeClustering`: http://scikit-learn.org/dev/modules/clustering.html#hierarchical-clustering
** rich get richer; but Ward gives most regular sizes, but only Euclidean; complete linkage worst
** `FeatureAgglomeration`: feature clustering
** connectivity constraints, to avoid merging of distant clusters http://scikit-learn.org/dev/modules/clustering.html#adding-connectivity-constraints
* DBSCAN
* Birch: builds Characteristic Feature Tree
** use rather MiniBatchKMeans if more than 20 features
** useful for reducing data to representors (better than KMeans)
* Cluster performance evaluation: http://scikit-learn.org/dev/modules/clustering.html#clustering-performance-evaluation
** Adjusted Rand Index: measures similarity of assignments http://scikit-learn.org/dev/modules/generated/sklearn.metrics.adjusted_rand_score.html#sklearn.metrics.adjusted_rand_score
*** how much better than random
*** but requires ground truth knowledge

=== RandomTreesEmbedding

* unsupervised transformation of data
* encodes data by indices of leaves the data point ends up in; encoded in one-of-K manner -> high dimensional sparse binary coding
* similar points likely in same leaf
* size of coding at most :math:`n_\mathrm{estimators}2^{\mathrm{max_depth}}`


=== PCA

* successive components that explain maximum variance
* n_samples * n_features^2
* can do MLE choice for number of components
* there can be randomness in sign
* `SparsePCA`: find sparse components; or faster `MiniBatchSparsePCA`
* `IncrementalPCA`: stores only estimates of components and noise variances

=== FactorAnalysis
* lower dimensional latent factors with Gaussian noise (PCA assumes equal noise variance)

=== SparseCoder

* match to given sparse dictionary

=== RandomTreesEmbedding

* unsupervised to sparse binary representation (by tree leaves)
* can transform into higher dimension (e.g. train linear model on that http://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html)

=== ICA

* distribution of loadings carriers maximum amount of independent information (only for non-Gaussian independent signals)

=== Gaussian Mixture Models (GMM)

* (+) fast
* (+) maximizes only likelihodd; will not bias to zero or cluster shape
* (-) with few points per cluster, might diverge without proper regularization
* (-) need held-out set to determine number of components
* DPGMM: Dirichlet for infinite
* VBGMM: Variational for little data
* BIC can estimate number of components
* Variational Bayesian Gaussian mixtures `BayesianGaussianMixture`:
** finds number of components
** maximiaze lower bound on model evidence instead of data likelihood
** adds regularization; avoids singularities
** a bit slower
** (+) with `weight_concentration_prior` small and `n_components` large enough, still can set some weights close to zero to automatically adjust number of components
** (+) `weight_concentration_prior_type="dirichlet_process"` less sensitive to exact parameters
** (+) less sensitive to pathological cases (regularization)
** (-) new parameter `weight-concentration_prior` needed; more components if large
** (-) some bias; may not work if data is different


=== Manifold learning

* complexities roughly n*(n+p)*log(n); also depend on number of nearest neighbors and output dimension
* standardize for better convergence speed
* reconstruction error be help determining correct output dimension
* Isomap: extension of multi-dimensional scaling or Kernel PCA
* `LocallyLinearEmbedding`: like series of local PCA which are globally compared to find best non-linear embedding
** with `method="modified"`: better regularization; needs `n_neighbors>n_components`
** with `method="hessian"`: better regularization; needs `n_neighbors>n_components*(n_components+3)/2`
** with `method="ltsa"`: Local Tanget Space Alignment; optimize tangent space
* Spectral Embedding (aka Laplacian Eigenmaps) `SpectralEmbedding`:
* Multi-dimensional scaling `MDS`:
** non-metric version tries to preserve order of distance (monotonic relationship)
* t-SNE: 
** (+) can unfold _multiple_ manifolds (unlike other methods)
** (-) slow
** (-) local minima
** (-) Barnes-Hut t-SNE limited to 2 or 3 dim
** (-) global structure not preserved (unless `init="pca"`)
** perplexity like number of nearest neighbors; larger perp. -> more nearest neoghbors and less sensitive to small structure
** larger data need larger perplexity
* if singular matrices: `solver="dense"` (instead of `"arpack"`) or if disjoint set, then increase `n_neighbors`
* also see Random Tree Embeddings


= Density estimation

* `sklearn.neighbors.KernelDensity`
* Haversine distince of geographical


== Model selection and evaluation

* StratifiedKFold: even classes across folds



== Cross-decomposition

* find latent variable linear relation between matrices X and Y:
** Partial Least Squares `PLSRegression`
*** better when predictors p>n and when multi-collinearity
** Canonical Correlation Analysis CCA: find unit vectors u,v to max corr(X*u,Y*v)
* `PLSRegression`, `PLSCanonical`, `CCA`, `PLSSVD`




== Data transformations

* RobustScaler: remove median and scale by IQR
* robust_scale: on sparse data you can at least do with_centering=False
* add_dummy_feature: add constant
* PolynomialFeature creation: http://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures



== Anomaly detection

* `covariance.EllipticEnvelope` (but not for n_samples > n_features^2): only for Gaussian
* `OneClassSVM`: overfits if multi-modal
* `sklearn.ensemble.IsolationForest`: Calculate isolation score of samples



== Analysis

=== Partial dependence plots

* http://scikit-learn.org/dev/modules/ensemble.html#partial-dependence
* `partial_dependence` for raw values
* marginalize out all but one or two features
* for multi-class also select the specific class
* here for decision trees:
  * if node involves target feature -> follow correct branch
  * otherwise follow both branches
  * in the end average weighted by fraction of samples -> weighted average of all visited leaves
  
    from sklearn.ensemble.partial_dependence import plot_partial_dependence
    clf=GradientBoostingClassifier(..).fit(X, y)
    fix, ax = plot_partial_dependence(clf, X, features, label=0)


* learning_curve: depending on diff training sizes
* validation_curve: depending of diff hyperparam
* sklearn.metrics.classification_report
* export_graphviz(decision_tree)

* Gaussian processes: Iso-curves of probability http://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gp_probabilistic_classification_after_regression.html
* Permutation test to see how far from luck http://scikit-learn.org/stable/auto_examples/feature_selection/plot_permutation_test_for_classification.html
* GradientBoostingRegressor(loss="quantile", alpha=0.95): can do prediction boundaries (http://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html)


== Feature selection

* SelectFromModel: based in importace weights
* Recursive Feature Elimination (RFE): recursive feature elimination
* RFECV: with CV to select best number
* VarianceThreshold: remove low variance
* chi2: top features from non-negative X (frequencies) to classes
* f_classif: ANOVE F-value
* check_increasing(x,y): sign of Spearman correlation
* johnson_lindenstrass_min_dim: find safe number of components needed

* Feature selection with LassoCV: http://scikit-learn.org/stable/auto_examples/feature_selection/plot_select_from_model_boston.html
* Joint feature selection with Multi-task Lasso: http://scikit-learn.org/stable/auto_examples/linear_model/plot_multi_task_lasso_support.html
** Multiple regression problems and force selected features to be same across problems



== Out-of-core learning

* easiest to get EC2 with 256GB RAM: 68Mio. point with 1000 features
* SGDClassifier, Naive Bayes (multiple), MinibatchKMeans, IncrementalPCA, MiniBatchDictionaryLearning
* make batches; .partial_fit()
* multiple updates if needed (but no use for Naive Bayes)
* maybe shuffle batches (at least once at beginning)
* but cannot use pipelines here -> use stateless transformers:
** don't need to learn data; just map
** Normalizer (divides by L2 norm)
** HashingVectorizer
*** takes upper limit
*** sparse matrix encoding
*** e.g. one text: CounterVectorizer, TfidfVectorizer); cannot reverse/can have collisions
*** .transform() on each batch
** RBFSampler (and other kernel approx)
*** make kernel instead of new features (kernel trick) for linear classifier
*** kernel SVM: #samples^3; linear SVM (primal): #sample * #features
*** undoing kernel trick: find feature functions (again) which approximate kernel product

-> does that for RBF kernel:

    kernel_approx=RBFSampler(..)
    kernel_approx.fit(batch[0])
    sgd=SGDClassifier()
    for all batches:
        X_trans = kernel_approx.transform(batch[i])
        sgd.partial_fit(X_trans, y, classes=..)
        
== Data sources

* mldata.org, mlcomp.org
* Gaussian blobs, concentric circles, moons, 
* make_classification: normally on hypercube
* different regression problems
* low-rank matrix
* make_regression (random linear), random regression with sparse uncorrelated
* sparse dictionary elements
* blockdiagonal for biclustering

== Other

* Saving models:
** save a model with pickle.dumps()
** joblib.dump() [more efficient version on bigdata but only to disk not string]
* fitted attributes end by underscore
* X: often best to have Fortran contiguous numpy array
* check_random_state(seed) # create numpy.random.RandomState instance
* check_estimator # check whether sklearn conventions
* utils: resample, shuffle
* lasso_stability_path

make_scorer:
* wrap function for GridSearchCV

BernoulliRBM:
* binary visible units and binary hidden units



np.array_split()
clf.fit(<train>).score(<test>)
np.logspace()
NearestCentroid shrink_threshold: remove noisy features

========================================

Tools:
* cross_val_score(.., cv=5)
* cv=ShuffleSplit(..), cv=LeaveOneOut(..)
* GridSearchCV(clf, param_grid={..}) -> fit/predict # meta-estimator
* make_pipeline(trans1, .., transN, estimator) # meta-estimator like last estimator
* log scale: 10.**np.arange(-3,3)
* in GridSearchCV after pipeline use name mangling for parameters, e.g.:{"svc__C": ..} (lower class of class name)
* make_union: horizontal stack of transformers # meta-transformer # param: "featureunion__tfidfvectorizer-1__.."
* randomized parameter search:
  * RandomizedSearchCV(pipe, param_distributions={..}, n_iter=50)
  * log scale: "linearsvc__C":expon()  # from scipy.stats
  * step-size free, robust to irrelevant parameters, constant complexity
  * not for low-dim spaces
  * Bayesian: hyperopt, spearmint, ...
* y=f+eps -> E[(y-hatf)^2]=E[(f-E[hatf])^2)+E[(hatf-E[[hatf])^2]+E[eps^2]=Bias+Var+IrreducibleError (noise in data)
* change capacity: validation_curve(clf, X, y, param_name="..", param_range=..) # check for complexity controlling parameters
* change data size: learning_curve(clf, Y, y, train_sizes=..) [but model capacity fixed] -> add more data?
* Scoring functions:
  * default: accuracy for classification; R2 for regression
  * for imbalanced data other scores
  * DummyClassifier("most_frequent") -> always says most frequent
  * better: cross_val_score(.., scoring="roc_auc") -> but badly calibrated if just use output; need to calibrate
  * SCORERS.keys(): prints all scoring string ids
  * own scoring function:
    * myscore(clf, X, y)
    * myyscorer(y_true, y_pred); make_scorer(myyscorer)


+++++++++++++++++++++++++++

= Scikit-learn userguide summary

== Supervised learning

* OLS, n*p^2 time
* RidgeCV uses Generalized Cross-Validation (efficient form of LOO CV)
* Lasso uses coordinate descent
* LassoCV vs LassoLarsCV:
  * LassoCV: for high-dimensional data with collinear
  * LassoLarsCV: explores more relevant alpha; faster if n<<p
* LassoLarsIC:
  * faster to find alpha
  * but need proper degree of freedom which is asymptotic result only
  * assumes model is correct
  * breaks when p>n
* ElasticNet:
  * when multiple features correlated to pick all
* Multi-task Lasso:
  * multiple outputs; pick same features for all Lasso simultaneously
  * mixed L12 prior: sum sqrt(sum w_ij^2)
* LARS:
  * numerically efficient where p>>n
  * complexity same as OLS
  * full piecewise linear solution path [when coef normalized by max] (for CV or tuning model)
  * same correlation variables increase the same
  * BUT might be sensitive to noise (since iterative refitting)
  * lasso_path function for full path
* Orthogonal Matching Pursuit (OMP):
  * linear model with L0 norm, i.e. number of non-zero coef
  * fix max number of coef or max error
  * greedy algorithm
  * compared to MP recomputes residuals
* Bayesian ridge regression (BRR):
  * regularization parameter tune to data
  * alpha as random variable to tune
  * w, alpha, lambda estimated jointly during fit
  * w spherical Gaussians
  * more robust to ill-posed than OLS
  * BUT model inference time consuming
* Automatic Relevance Determination (ARD):
  * similar to BRR, but leads to sparser weights
  * w axis-parallel elliptical Gaussians
* LogisticRegression:
  * can do multiclass
  * can do L1 or L2
  * has different solvers
    * lbfgs and newton-cg: for highdim and multinomial
    * liblinear: for small data or L1; no true multinomial
    * sag: for very large data; no multinomial and L2 only
  * svm.l1_min_c to get min C for not all zero (solver liblinear)
  * for large data also SGDClassifier with log loss
* SGD:
  * partial_fit()
* Perceptron:
  * needs no learning rate
  * not regularized
  * updates only on erros (-> faster and sparser model)
* PassiveAggressive:
  * like Perceptron no need for learning rate
  * has regularization C
  * different loss functions
* Robust regression
  * generally doesnt work for high dimensions
  * RANSAC: faster; better for large outliers in y; usually better
    * performance probabilistic and depends on iterations
    * extracts in-liers
  * Theil Sen: better with medium outliers in x (if dim not too large)
    * uses generalization of median in high dimension
    * breakpoint about 30%
    * time (n choose n_subsamples)
    
=== Discriminant analysis

* closed-form solutions
* multiclass
* hyper-parameters
* quadratic: quadratic surface
* models class-conditional prob.; select class that maximizes prob.
* prob. modelled as multi-variate Gaussian with own mean/cov.matrix (for LDA: all cov.matrix same; for QDA: general)
* if in QDA cov.matrix diagonal -> classes conditionally indep. -> Gaussian Naive Bayers (GaussianNB())
* dim. reduction since LDA like projecting to K-1 dim subspace (K num. classes)
* reduce dim even more by maximizing variance (use n_components)
* LDA:
  * for supervised dim reduction
  * strong dim reduction (dim output less than number of classes)
  * set n_components (for .transform() only)
  * shrinkage="auto" parameter when very few samples (and empirical cov.matrix would be poor)
    * shrinkage=1 means use diagonal estimates only of cov.mat.
    * only with solver lsqr or eigen and classification
  * default solver svd: doesn't need cov.mat.
  * solver eigen:
    * optimize between class scatter to within class scatter ratio
    * for classification, transform, shrinkage
    * needs cov.mat -> slow for many features
    
Features
* PolynomialFeatures()
* PolynomialFeatures(interaction_only=True)

Scoring functions:
http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter

=== Kernel Ridge regression

* combines L2 Ridge with kernel
* model same as SVR; but KRR uses squared loss (SVR uses esp-sensitive loss combined with L2)
* can be closed-form and faster for medium-sized data (e.g. 7x faster for <1000 samples)
* but model not sparse (slower for prediction)

sklearn.calibration.CalibratedClassifierCV: for probability calibration

=== Support vector machines

* still effective in high-dim. Even when p>n
* memory efficient in prediction
* versatile kernels
* BUT poor when p>>n
* BUT no probability estimates (in sklearn done with expensive cross-validation)
* can use dense and sparse data; but train on sparse too to predict on sparse
* fastest with C-ordered np.ndarray or scipy.sparse.csr_matrix with dytpe=float64

== MlXtend

* largely taken over to sklearn already -> not terribly useful
* `plot_decision_regions(X, y, clf)`
* `plot_learning_curves`
* `shuffle_arrays_unison`
* `find_filegroups` (task01.log, task02.log,...)
* `SequentialFeatureSelector`
* manual `ColumnSelector`
* `StackingClassifier`
* `StackingRegressor`
* `Adaline`
* `SoftmaxRegression`
* `TfMultilayerPerceptron`, and other Tensorflow models

++++++++++++++++++++++++++++++++++++++++++++++++

Supervised learning
+++++++++++++++++++

Linear model
============
* `linear_model.LinearRegression()`
* relies on independence; when features correlated, matrix becomes close to singular and this highly sensitive to noise
* `linear_model.Ridge()`
* `linear_model.Lasso()` (uses coordinate descent; note that :math:`\alpha` is different scale than Ridge due to factor :math:`1/(2n)`)
* `linear_model.LassoCV()` (preferable for high-dimensional with many collinear regressors)
* `linear_model.LassoLarsCV()` (based on least angle regression; explores more :math:`\alpha` parameters than LassoCV; for few samples faster than LassoCV)
* `linear_model.LassoLarsIC()` (uses AIC or BIC; computationally cheaper; assumes data is really this model; needs proper estimation of degree; breaks down when badly conditioned [more features than samples])
* `linear_model.ElasticNet()` (useful when multiple features which are correlated [unlike Lasso L1 which will pick only one of the features]; inherits stability of Ridge under rotation)
* `linear_model.ElasticNetCV()` (set parameters by CV)
* `linear_model.MultiTaskLasso()` (uses same sparse coefficients for multiple problems)
* `OrthogonalMatchingPursuit()` (L0 norm)
* `Lars` algorithm (numerically efficient if :math:`p\gge n`; full piecewise linear solution path [useful for CV and tuning]; equally correlated variables get similar coefficients; can be sensitive to noise due to iterative refitting of residuals)
* complexity: for a matrix X (:math:`n \times m`) -> :math:`O(nm^2)`
* plot Ridge coefficients (`sklearn <http://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_path.html#example-linear-model-plot-ridge-path-py>`_)
* built-in cross-validation of :math:`\alpha`: `linear_model.RidgeCV()` (like `GridSearchCV` but defaults to an efficient form of leave-one-out)
* Bayesian Regression: weights distributed with precision :math:`\lambda`; :math:`\lambda` and regularization parameter :math:`\alpha` estimated from data (without CV); adapts to data; can be slow
* `BayesianRidge()` (weights are spherical Gaussian; use gamma distribution priors for regularization parameter and weight width [other than standard Gaussian])
* `ARDRegression` (Automatic relevance determination; weight distribution not spherical but axis-parallel; leads to sparser weights)
* `LogisticRegression()` (for L1 you can use `sklearn.svm.l1_min_c` to estimate lower bound for :math:`C` to get non null model)
* `SGDClassifier`, `SGDRegressor`: useful when very many samples
* `Perceptron()`: does not need learning rate; not regularized; updates only on mistakes (therefore slightly faster then SGD and result sparser)
* `PassiveAggressiveClassifier()`, `PassiveAggressiveRegressor()`: for large-scale learning; similar to Perceptron but dont need learning rate; are regularized with parameter :math:`C`

Support Vector Machines
=======================
* high effective in high-dim space (even when more dimensions than samples)
* uses subset of training points -> memory efficient
* but poor performance if much more features than samples
* no direct probability estimates (internally 5-CV used)
* supports numpy.ndarray and scipy.sparse (however same for fit and predict)
* fastest with numpy.ndarray or scipy.sparse.csr_matrix dtype=float64
* `SVC`, `NuSVC`: similar but different parameters
* `LinearSVC`: other implementation
* `clf.n_support_`: number of support vectors
* multiclass:
  * `SVC`: one-vs-one multiclass (therefore :math:`n_c(n_c-1)/2` classifiers)
  * `LinearSCVC`: one-vs-rest (prefered); or `multi_class='crammer_singer'` possible (consistent method but much slower)
* with `probability=True` Platt scaling is used (LogReg on SVM scores fit by CV); very slow; also some theorical issues with Platt; preferably use `decision_function` for confidence scores
* `SVC` only: parameter `class_weight`; sets parameter to :math:`C\cdot\mathrm{weight}`
* all (but LinearSVC) have `sample_weight`
* `SVR`, `NuSVR`: support vector regression
* `OneClassSVM`: find soft (cluster) boundary; for novelty detection
* complexity:
  * :math:`O(n_\mathrm{features}n_\mathrm{samples}^{2\ldots 3})` (use average number of features if sparse)
  * `LinearSVC`: much more efficient, scales almost linearly (millions of samples and/or features)
* tips:
  * check if input data contiguous (inspect flag attributes); otherwise will be copied; however `LinearSVC` will always copy (use `SGDClassifier` to avoid that)
  * for large problems and enough RAM increase `cache_size` to higher value
  * decrease :math:`C` for noisy observations
  * parameter `nu` approximates fraction of training errors and support vectors
  * `LinearSVC` uses some random numbers; decrease `tol` to avoid effect
  * `LinearSVC(loss='l2', penalty='l1', dual=False)` yields sparse solution
  
Stochastic Gradient Descent
===========================
* easily problems with 10,000 features and 10,000 samples
* needs number of iterations
* sensitive to feature scaling
* adviced to use `shuffle=True`
* loss functions:
  * `loss='hinge'`: soft-margine linear SVM (lazy: update only on violation of margin -> sparser model; this one has no `predict_proba`)
  * `loss='modified_huber'`: smoothed hinge loss (lazy); less steep than squared hinge
  * `loss='log'`: logistic regression
* regression with `SGDRegressor`: useful for >10,000 samples (use conventional regression for smaller problems); loss functions:
  * `loss='square_loss'`: ordinary
  * `loss='huber'`: robust regression
  * `loss='epsilon_insensitve'`: linear SVR; need insensitive region width parameter `epsilon`
* sparse implementation gives slightly different results due to a shrunk learnign rate for intercept
* use `scipy.sparse.csr_matrix` for best performance with sparse
* complexity: linear in samples, iterations and features (runtime for given accuracy does not increase with larger training sizes)
* sensitive to features scaling
* recommended: `alpha` by grid search with `10.0**-np.arange(1,7)`; `n_iter = np.ceil(10**6 / n)` since usually convergence after 1,000,000 samples
* if SGD on PCA transform: scale by constant such that average L2 norm is 1

Nearest Neighbors
=================
* `sklearn.neighbors`
* methods:
  * by k-nearest `KNeighborsClassifier`
  * fixed radius `RadiusNeighborsClassifier` (here, if identical distance, then depends on order in training); better if data not uniformly sampled
* non-generalizing ML methods since only remember points
* successful in some image tasks
* numpy arrays (many distance metrics) or scipy.sparse (arbitrary Minkowski distance)
* `NearestNeighbors` (wrapper for `KDTree` and `BallTree`):
  * `algorithm='auto'`: determined (currently `'ball_tree'` if :math:`k<N/2` and `brute` otherwise)
  * `algorithm='ball_tree'`: `BallTree` (better; nested hyperspheres; construction more costly)
  * `algorithm='kd_tree'`: `KDTree` (generalization of 2D quadtrees; good for low-dimensions <20)
  * `algorithm='brute'`: based on `sklearn.metrics.pairwise`
.. highlight::
   nnbrs=NearestNeighbors(...).fit(X)
   distances, indices = nnbrs.kneighbors(X)  # use distances or indices
   nnbrs.kneighbors_graph(X).toarray()       # sparse graph; nearby index are nearby in parameter space -> approx block-diagonal
* for metrics see `DistanceMetric <http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html#sklearn.neighbors.DistanceMetric>`_
* `distance=...` for weighting (uniform and majority vote otherwise)
* regression with mean assigned:
  * `KNeighborsRegressor`
  * `RadiusNeighborsRegressor`
* `leave_size` parameter: when to switch to brute force
* `NearestCentroid` classifier:
  * represents each class by centroid
  * no parameters (good baseline)
  * non-convex
  * assumes equal variances (better: `sklearn.lda.LDA` or `sklearn.qda.QDA`)
  * `shrink_threshold` parameter: normalize feature distances, reduce by parameter and cut below zero -> noisy data won't affect classification
  
Gaussian Processes
==================
* usually regression
* here also post-processing for classification possible
* adv:
  * interpolates observations
  * predictions probabilistic -> compute empirical confidence intervals
  * linear regression models and correlation models can be specified
* disadv:
  * not sparse (uses all samples)
  * bad in high dimensions (when >30; slow and bad prediction)
  * classification only a post-processing addon
* `sklearn.gaussian_process.GaussianProcess()`
* `nugget` parameter to specify noise for points: for regularization (adding to diagonal) [if squared-exponential correlation this is equivalent to fractional variance :math:`(\sigma_i/y_i)^2`]
* with correct `nugget` and `corr` the recovery is robust
* Maths: http://scikit-learn.org/stable/modules/gaussian_process.html#mathematical-formulation
* correlation models:
  * need to know properties of original experiment
  * often matches SVM kernels
  * if infinitely differentiable (smooth): use squared-exponential loss
  * use exponential correlation model otherwise
* implementation based on DACE Matlab toolbox


Naive Bayes
===========
* due to decoupling each distribution can be independently estimated -> helps vs curse of dimensionality
* `predict_proba` is a bad estimator here
* `GaussianNB`: likelihood Gaussian :math:`P(x_i|y)=N(x_i;\mu_y; \sigma_y)`
* `MultinomialNB`: `http://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes`_
* `BernoulliNB`: `http://scikit-learn.org/stable/modules/naive_bayes.html#bernoulli-naive-bayes`_
* suitable for large scale with parameter `partial_fit` that can be used incrementally (use data chunks as large a possible for speed)


Decision Trees
==============
* can be visualized
* little data preparation
* numerical and categorical data
* can do multi-output problems
* white-box: can be translated to boolean logic
* possible to validate model with statistical tests
* can easily overfit
* unstable: small variations in data might result in completely different tree
* heuristics don't always yield best tree
* biased tree if some classes dominate
* `DecisionTreeClassifier`: can be binary [-1, 1] or multiclass [0, ..., K-1]
* `sklearn.tree.export_graphviz(clf, out_file=...)`
* `DecisionTreeRegressor`
* multi-output:
  * several outputs to predict
  * build single model better if correlations
  * compute average reduction across all outputs
* complexity:
  * construction :math:`O(n_\mathrm{samples}\log(n_\mathrm{samples})n_\mathrm{features})`
  * query time :math:`O(\log n_\mathrm{samples})`
  * scikit learn optimizes by presorting
* tips:
  * overfit when many features (need right ratio of samples to features)
  * use dimensionality reduction before
  * visualize trees
  * requires number of samples doubles for each new tree level
  * try `max_depth=3` and ` min_samples_leaf=5` first
  * balance tree before
  * copy made if not `numpy.float32`
* CART used here (unlike C4.5 which translates to if-then rules and prunes additionally)


== Unsupervised learning

http://scikit-learn.org/stable/unsupervised_learning.html

== Model selection and evaluation

== Cross validation

* `cross_validation.train_test_split()`
* use "validation" set to tune parameters
* `cross_validation.cross_val_score()`:
  * different scorings possible
  * can use other strategies by passing cross validation iterator (e.g. `ShuffleSplit`)
  * when `cv` parameter is integer: `KFold` (unsupervised), `StratifiedKFold` (supervised)
* cross validation iterators:
  * http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators
  * boolean masks and indices for split
  * k-fold, stratigied k-fold, leave-one-out, leave-one-label-out, random permutation, bootstrap
  
== Grid search

* `estimator.get_params()` to get parameters
.. highlight :
   param_grid = [
  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},
  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},
  ]
  
* `scoring` parameter to change scoring; otherwise `.score` used which is `metrics.accuracy_score` for classification and `metrics.r2_score` for regression
* `n_jobs=-1` for parallel
* `RandomedSearchCV`:
  * budget can be chosen (no wait on irrelevant parameters)
  * `n_iter` for iterations
  * can use `scipy.stats.expon(scale=...)` etc for sampling (needs `.rvs()` method) [seeded by numpy with `np.random.set_state`
  
== Alternatives to brute force search

* some methods can find parameters as efficient as fitting
* set `alpha` as regularizer strength; computer regularization path
* models: `RidgeCV`, `RidgeClassifier`, `LarsCV`, `LassoLarsCV`, `LassoCV`, `ElasticNetCV`
* information criterion: `LassoLarsIC`
* out-of-bag estimates: `RandomForest*`, `ExtraTrees*`, `GradientBoosting*` (`*`=`Classifier`,`Regressor`)

== Pipeline

* chain estimators (only one `.fit()`)
* grid search of parameters of all estimators at once
* parameter `[('name', estimator) , ...]`
* call `.fit()`, ``.transform()` and pass one to next step
* pipeline has all methods of last estimator
* `FeatureUnion`:
  * independent fits to transformers
  * sample vectors concatenated end-to-end for larger vectors
  
== Model evaluation

* scoring parameter can be any callable function (use `sklearn.metrics.make_scorer` to fix parameters of given scorers); protocol: called with `(estimator, X, y)`, return float (higher is better)
* `sklearn.metric`: `*_score` for maximize, `*_error`/`*_loss` to minimize
* `.confusion_matrix(y_true, y_pred)` computes confusion matrix
* `print(classification_report(y_true, y_pred, target_names=target_names))`
* for multiclass `f1_score`, `fbeta_score`, `precision_recall_fscore_support`, `precision_score` and `recall_score` have a `average` parameter to specify how to combine scores
* `matthews_corrcoef(y_true, y_pred)`
* `fpr, tpr, thresholds = roc_curve(y, scores, pos_label=2)`
* also some dummy estimators to simulate random
* `sklearn.dummy.DummyClassifier(strategy='most_frequent')`: use very simple model

== Dataset transformations

* `DictVectorizer`: `[{...},...]` to numpy array; one-of-K for categorical variables
* `FeatureHasher`:
  * use hash to determine column index (instead of e.g. a hypothetical super dictionary that maps words to columns)
  * input is `[{"feat1":val1, ...}, ...]` or similar; string list input `["word1", "word2", ...]` will be converted to `("word1", 1), ("word2", 1)...`
  * hash sign determines sign of value stored (to cancel effect of possible collisions; use option `non_negative=True` if need positive only)
  * prefer `n_features` a power of 2
  * result is `scipy.sparse`
* text feature extraction `sklearn.feature_extraction.text`:
  * `CountVectorizer`:
    * transform list (documents) of strings to sparse word count matrix (many parameters for word parsing)
    * methods `.toarray()`, `.get_feature_names()`, `.vocabulary_.get("word")`
  * `TfidfTransformer()`:
    * reweight to consider frequent words
    * rows normalized (e.g. L2)
    * weights in `.idf_`
    * `TfidfVectorizer`: combines `CountVectorizer` and `TfidfTransformer`
    * binary values (option) may be less noisy for short texts
  * `HashingVectorizer`:
    * no need for memory heavy translation table
    * default `n_features=2**20`
    * dimensionality does not affect algorithms with CSR matrices (`LinearSVC(dual=True)`, `Perceptron`, `SGDClassifier`, `PassiveAggressive`), but does affect CSC matrices (`LinearSVC(dual=False)`, `Lasso`, ...)
    * can be used for out-of-core with mini batch (<http://scikit-learn.org/stable/auto_examples/applications/plot_out_of_core_classification.html#example-applications-plot-out-of-core-classification-py>_)
* `sklearn.features_extraction.image.extract_patches_2d`:
  * extract 2D arrays (or 3D with color)
  * `image.PatchExtractor`: for pipeline estimator usage or multiple images
* ``img_to_graph` or `grid_to_graph` to encode connectivity between samples

== Preprocessing

* `sklearn.preprocessing`
* `scale`:
  * scale columns to zero mean and unit variance
  * sparse inputs only if `with_mean=False`; will be converted to `sparse.csr_matrix` (use this upstream)
* `StandardScaler`: transformer API to be used in pipeline or to reapply transform
* `MinMaxScaler`:
  * scale to range
  * robust to very small standard deviations
  * preserves zero in entries
* if you need to remove linear correlations: use `decomposition.PCA` or `decomposition.RandomizedPCA` with `white=True`
* `KernelCenterer`: can center kernels
* `normalize`:
  * normalize each row to unit norm
  * for sparse data use `scipy.sparse.csr_matrix` to avoid copies
* `Normalizer`: tranformer API (but no `.fit()` and stateless)
* `Binarizer`:
  * threshold to boolean
  * no `.fit()` method
  * for sparse data use `scipy.sparse.csr_matrix` to avoid copies
* `OneHotEncoder`: one-of-K encoder for categorical features
* `LabelBinarizer`: utility class to create label indicator matrix from a list of multi-class labels
* ``LabelEncoder`:
  * utility class to rank-normalize labels such that they contain only values `0` to `n_classes-1`
  * can be used on any hashable and comparable (e.g. string)
* `Imputer`:
  * simple imputation by mean, median, most frequent of row or column, ...
  
== Kernel Approxmiation

* explicit (instead of implicit like in SVM) feature mapping -> useful for online learning and reduce cost for very large data sets
* -> use approximate kernel map together with linear SVM
* `Nystroem`: low-rank approximation of kernels by subsampling of data (default with `rbf`)
* `RBFSampler`:
  * approximate (Monte Carlo for `.fit()`) mapping for rbf kernel
  * less accurate than `Nystroem` but faster
* ...

== Random projection

* `sklearn.random_projection`
* approximately preserver pairwise distances (good for distance methods)
* based on Johnson-Lindenstrauss lemma (few points in high dim can be projected to subspace)
* `johnson_lindenstrauss_min_dim(samples=..., eps=...)`: conservatively estimate minimal subspace size to guarantee some distortion
* `GaussianRandomProjection`: project to randomly generated matrix where components :math:`N(0,1/n_\mathrm{components})`
* `SparseRandomProjection`: similar but sparse embedding (memory efficient and faster computation)

== Pairwise metrics

* evaluate pairwise distances
* modules with distance metrics and kernels (semi-definite)

== Example data sets

* `sklearn.datasets`
* common API
* also random sample generators
* download data from mldata.org


LogisticRegressionCV: same a GridSearch on LogisticRegression, just faster

== Large scale strategies

http://scikit-learn.org/stable/modules/scaling_strategies.html

=== Extracting features

* sklearn.feature_extraction.FeatureHasher
* sklearn.feature_extraction.text.HashingVectorizer

=== Incremental learning

* `partial_fit()` methods of sklearn.
May have to pass possible classes by `classes=`. May give less importance to later samples as learning rate decreases.

Choosing a good minibatch size:
* SGD
* PassiveAggressive
* discrete NaiveBayes truly online and not affected by batch size
* MiniBatchJMeans convergence affected by batch size (and memory footprints changes a lot)

=== Classification

* sklearn.naive_bayes.MultinomialNB (weak and slow?)
* sklearn.naive_bayes.BernoulliNB
* sklearn.linear_model.Perceptron (sensitive to noise)
* sklearn.linear_model.SGDClassifier
* sklearn.linear_model.PassiveAggressiveClassifier

=== Regression

* sklearn.linear_model.SGDRegressor
* sklearn.linear_model.PassiveAggressiveRegressor

=== Clustering

* sklearn.cluster.MiniBatchKMeans

=== Decomposition / feature Extraction

* sklearn.decomposition.MiniBatchDictionaryLearning
* sklearn.decomposition.IncrementalPCA
* sklearn.cluster.MiniBatchKMeans


|===
| cluster.estimate_bandwidth(X)             | Estimate bandwidth to use with mean-shift http://scikit-learn.org/stable/modules/generated/sklearn.cluster.estimate_bandwidth.html[Ref]
| ensemble.partial_dependence.plot_partial_dependence(gbrt, X, feats)   | http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.partial_dependence.plot_partial_dependence.html[Ref]
|===


[cols="m,d"]
|===
| y_pred = cross_val_predict(clf, X, y)     |
| cv_scores = cross_val_score(clf, X, y)    |
| cross_validate(clf, X, y)                 | Cross-validate multiple metrics (also record fit/score time and train/test score)
|===

== Preprocessing

[cols="m,d"]
|===
| CategoricalEncoder(encoding="onehot")     | Onehot or ordinal http://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.CategoricalEncoder.html[Ref]
|===

== References

* https://github.com/amueller/scipy_2015_sklearn_tutorial/tree/master/notebooks[Scipy 2015 Tutorial]
* http://scikit-learn.org/stable/developers/contributing.html#apis-of-scikit-learn-objects[Writing own estimators]

== Own classifier

* https://scikit-learn.org/stable/developers/contributing.html#rolling-your-own-estimator
* estimator: `.fit(X, y)`
* predictor: `.predict(X)`, `predict_proba(X)`
* transformer: `.transform(X)` (when not on train set),`.fit_transform(X, y)`
* model goodness of fit: `.score(X, y=None)` for score to be usable
* `.decision_function`: ?

=== Estimators

* from `sklearn.base.BaseEstimator`: e.g. for __repr__
* `.set_params(**paramdict)` set all data independent parameters from `__init__`; `return self`
* `.get_params(deep=True)`: returns dict of params; `deep` whether to return parameters of sub-estimators; `get_params` not essential though
* `__init__`:
** must be all keywords arguments with all default values
** all parameters must correspond to an attribute
** no logic, no validation, parameters not changed!
* `.fit`:
** `.fit` throws ValueError when `X.shape[0]!=y.shape[0]`
** `return self`
** if kwargs used, then only for something strictly data dependent (otherwise to init)
** only last `fit` call should matter (otherwise make parameter `warm_start`)
** result parameters end with underscore
** `n_iter` as name when iterative
* `sklearn.utils.estimator_checks.check_estimator` to check (check some of above to make sure it works with GridSearchCV and Pipeline)
* project template: https://github.com/scikit-learn-contrib/project-template/
* `.clone()` for model_selection; just provide `get_params`; otherwise will be deep copied
* `.classes_` if classes deduced (`self.classes_, y = np.unique(y, return_inverse=True)`)
* tags to inspect capabilities: https://scikit-learn.org/stable/developers/contributing.html#estimator-tags

    def set_params(self, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        return self